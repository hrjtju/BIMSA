\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{svg}

% 添加作者格式所需的包
\usepackage{authblk}

% 定义作者格式命令（如果不存在的话）
\providecommand{\fnm}[1]{#1}
\providecommand{\sur}[1]{#1}
\providecommand{\orgdiv}[1]{#1}
\providecommand{\orgname}[1]{#1}
\providecommand{\orgaddress}[1]{#1}
\providecommand{\street}[1]{#1}
\providecommand{\city}[1]{#1}
\providecommand{\postcode}[1]{#1}
\providecommand{\state}[1]{#1}
\providecommand{\country}[1]{#1}
\providecommand{\orgname}[1]{#1}
\providecommand{\email}[1]{#1}
\geometry{margin=1in}

\title{Interpretable Convolutional Kernels for Physical Principles Discovery from Observational Data}




\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a novel approach for learning complex physical principles from observational data using multi-scale group equivariant convolutional neural networks. We demonstrate the effectiveness of our method on two paradigmatic systems: Conway's Game of Life cellular automata. Our neural architecture successfully discovers the underlying dynamical rules governing these systems by learning appropriate mapping functions that capture both local and long-range spatial interactions across multiple scales.

\textbf{Keywords:} Lattice Dynamics, Multi-Scale Group Equivariant Convolutional Neural Networks, Physical Principles, Dynamical Systems, Group representation theory
\end{abstract}

\section{Introduction}
Recent advances in deep learning, specifically convolutional neural networks, have shown remarkable promise in modeling spatial patterns and discovering underlying physical principles from data \citep{lecun2015deep}. Meanwhile, complementary work in symbolic regression has shown remarkable success in discovering explicit mathematical expressions from data, with AI Feynman and AI Feynman 2.0 successfully recovering all equations from the Feynman Lectures on Physics through physics-inspired techniques and Pareto-optimal symbolic regression \citep{udrescu2020ai,udrescu2020ai2}. In molecular dynamics, the Deep Potential Molecular Dynamics (DeePMD) method has demonstrated how deep neural networks can learn many-body interatomic potentials from ab initio data while preserving essential physical symmetries (translation, rotation, and permutation invariance), achieving quantum mechanical accuracy at linear computational cost scaling \citep{zhang2018deep}.

In this work, we extend these ideas by developing a multi-scale convolutional neural network architecture specifically designed for learning complex physical principles from observational data. Our approach leverages the connection between convolution kernels and differential operators to automatically discover the governing equations of dynamical systems without prior knowledge of their mathematical form. We demonstrate the effectiveness of our method on ...

%Cellular automata (CA) represent a fundamental class of discrete dynamical systems that exhibit complex emergent behaviors from simple local rules \citep{wolfram2002new}. Conway's Game of Life, perhaps the most famous cellular automaton, demonstrates how intricate patterns and dynamics can arise from elementary birth-death rules \citep{conway1970game}.

%Learning the underlying rules of cellular automata from observational data presents significant challenges in machine learning, particularly due to the discrete nature of states and the spatial-temporal dependencies inherent in these systems. Recent advances in deep learning, specifically convolutional neural networks, have shown promise in modeling spatial patterns \citep{lecun2015deep}.

\section{Problem Formulation}
\subsection{Lattice Field} 
Consider a discrete field system defined on a $d$-dimensional regular lattice $\Lambda \subset \mathbb{Z}^d$ \citep{kardar2007statistical}. For each lattice site $\mathbf{i} = (i_1, i_2, \ldots, i_d) \in \Lambda$, we define a field variable $\phi_{\mathbf{i}}(t) \in \mathcal{S}$, where $\mathcal{S}$ represents the state space. The system's configuration space can be expressed as:
\begin{equation}
\Phi(t) = \{\phi_{\mathbf{i}}(t) : \mathbf{i} \in \Lambda\} \in \mathcal{S}^{|\Lambda|}
\end{equation}
This formulation naturally extends classical statistical mechanics to discrete lattice systems, where the fundamental principles of equilibrium and non-equilibrium dynamics can be rigorously analyzed \citep{baxter2016exactly}.

The temporal evolution of the system is described by an operator $\mathcal{T}: \mathcal{S}^{|\Lambda|} \rightarrow \mathcal{S}^{|\Lambda|}$:

\begin{equation}
\Phi(t+1) = \mathcal{T}[\Phi(t)]
\end{equation}

where the operator $\mathcal{T}$ must satisfy the following fundamental constraints:

\textbf{Locality Condition}: The evolution of each lattice site depends only on its finite neighborhood:
\begin{equation}
\phi_{\mathbf{i}}(t+1) = f_{\text{local}}(\{\phi_{\mathbf{j}}(t) : \mathbf{j} \in \mathcal{N}(\mathbf{i})\})
\end{equation}
where $\mathcal{N}(\mathbf{i})$ denotes the neighborhood of lattice site $\mathbf{i}$.

\textbf{Causality Condition}: Information propagation has finite speed, i.e., there exists $v_{\max} < \infty$ such that:
\begin{equation}
\text{supp}(\mathcal{T}^n[\delta_{\mathbf{i}}]) \subset B_{\mathbf{i}}(nv_{\max})
\end{equation}
\subsection{Physical Symmetry Principles}

\subsubsection{Time Translation Invariance}

The evolution rules are independent of the choice of absolute time origin. Formally, for any temporal shift $\tau \in \mathbb{Z}$:

\begin{equation}
\mathcal{T}[\Phi(t)] = \Phi(t+1) \Leftrightarrow \mathcal{T}[\Phi(t+\tau)] = \Phi(t+\tau+1)
\end{equation}

This is equivalent to requiring that the parameters of evolution operator $\mathcal{T}$ contain no explicit time dependence.

\subsubsection{Spatial Symmetry Group}

\textbf{Translation Invariance}: For any lattice translation $\mathbf{a} \in \mathbb{Z}^d$, define the translation operator $T_{\mathbf{a}}$:
\begin{equation}
[T_{\mathbf{a}}\Phi]_{\mathbf{i}} = \Phi_{\mathbf{i}+\mathbf{a}}
\end{equation}

The evolution operator commutes with translation operators:
\begin{equation}
\mathcal{T} \circ T_{\mathbf{a}} = T_{\mathbf{a}} \circ \mathcal{T}
\end{equation}

\textbf{Point Group Symmetries}: For square lattices, the system exhibits $C_{4v}$ point group symmetry, including:
\begin{itemize}
\item 90° rotational invariance: $\mathcal{T} \circ R_{\pi/2} = R_{\pi/2} \circ \mathcal{T}$
\item Reflection invariance: $\mathcal{T} \circ \sigma = \sigma \circ \mathcal{T}$
\end{itemize}

\subsubsection{Markovian Property and Information-Theoretic Structure}

The system satisfies the strict Markov condition:
\begin{equation}
P(\Phi(t+1)|\Phi(t), \Phi(t-1), \ldots, \Phi(0)) = P(\Phi(t+1)|\Phi(t))
\end{equation}

This implies that the system's "memory" is completely encoded in the current state, with no hidden historical dependencies.

\subsubsection{Reversibility and Time-Reversal Symmetry}

For conservative systems, the evolution operator is invertible, i.e., there exists an inverse operator $\mathcal{T}^{-1}$:
\begin{equation}
\mathcal{T}^{-1} \circ \mathcal{T} = \mathcal{T} \circ \mathcal{T}^{-1} = \mathcal{I}
\end{equation}
This ensures that no information is lost in the system, and trajectories are deterministic in phase space.


\iffalse
We formulate the problem as learning a mapping function $F$ such that
\begin{equation}
\frac{d}{dt}\mathbf{X}(t) = \mathcal{F}(\mathbf{X}(t); \theta),
\end{equation}
where $\mathbf{X}(t) \in \mathbb{R}^n$ is an $n$-dimensional state vector, $\mathcal{F}: \mathbb{R}^n \times \Theta \rightarrow \mathbb{R}^n$ is a mapping function that governs the system dynamics, and $\theta$ represents the set of trainable parameters that define the specific form of the dynamical system. The corresponding discrete formulation can be expressed as:
\begin{equation}
\mathbf{X}^{(t+1)} = \mathcal{F}(\mathbf{X}^{(t)}; \theta),
\end{equation}
where $\mathbf{X}^{(t)} \in \mathbb{R}^n$ is the state vector at discrete time step $t$, $\mathbf{X}^{(t+1)}$ represents the state at the next discrete time step, $\mathcal{F}$ is the discrete mapping function that updates the system state from one time step to the next. The goal of this paper is to learn $\mathcal{F}$ by automatically discovering dynamical rules from data, where $\mathcal{F}$ can represent differential operators for continuous systems or transition rules for discrete systems.
\fi


\section{Multi-Scale Convolutional Neural Networks}
In this work, we use Multi-Scale Convolutional Neural Networks to discover the underlying physical principles governing dynamical systems by learning the mapping function $\mathcal{F}$ that captures both local and long-range spatial interactions. The multi-scale architecture is essential because physical processes in spatiotemporal systems often involve interactions across multiple spatial scales simultaneously, from immediate neighbor effects to global pattern formation mechanisms.

\textcolor{red}{Based on the aforementioned symmetry principles, our multi-scale CNN architecture should satisfy:
}

\begin{enumerate}
\item \textbf{Parameter Sharing}: Enforces translation invariance
\item \textbf{Equivariant Network Structure}: Preserves rotational and reflection symmetries
\item \textbf{Causal Convolution}: Ensures locality and causality constraints
\item \textbf{Time-Independent Parameterization}: Embodies time translation invariance
\end{enumerate}

This structured inductive bias enables the network to efficiently learn physically reasonable dynamical rules while significantly reducing the required amount of training data.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{fig/Multi-Scale CNN.png}
\caption{Multi-scale convolution operations for capturing spatial interactions at different scales.}
\label{fig:Multi-Scale CNN}
\end{figure}
\section{Numerical Experiments}
\subsection{Conway's Game of Life Rules}
Let $\mathcal{G} \subset \mathbb{Z}^2$ be a finite 2D grid of size $H \times W$, where each cell $(i,j) \in \mathcal{G}$ has a binary state $s_{i,j}^{(t)} \in \{0,1\}$ at discrete time $t \in \mathbb{N}$. The cellular automaton state at time $t$ is defined as:

\begin{equation}
\mathbf{S}^{(t)} = \{s_{i,j}^{(t)} : (i,j) \in \mathcal{G}\} \in \{0,1\}^{H \times W}
\end{equation}

The evolution of the cellular automaton follows a deterministic update rule:

\begin{equation}
\mathbf{S}^{(t+1)} = f(\mathbf{S}^{(t)})
\end{equation}
where $f: \{0,1\}^{H \times W} \rightarrow \{0,1\}^{H \times W}$ is the global transition function.


For Conway's Game of Life, the local transition rule for each cell $(i,j)$ is defined as:

\begin{equation}
s_{i,j}^{(t+1)} = \begin{cases}
1 & \text{if } s_{i,j}^{(t)} = 1 \text{ and } n_{i,j}^{(t)} \in \{2,3\} \\
1 & \text{if } s_{i,j}^{(t)} = 0 \text{ and } n_{i,j}^{(t)} = 3 \\
0 & \text{otherwise}
\end{cases}
\end{equation}
where $n_{i,j}^{(t)} = \sum_{(u,v) \in \mathcal{N}_{i,j}} s_{u,v}^{(t)}$ is the number of living neighbors in the Moore neighborhood:
\begin{equation}
\mathcal{N}_{i,j} = \{(i+\delta_i, j+\delta_j) : \delta_i, \delta_j \in \{-1,0,1\}, (\delta_i, \delta_j) \neq (0,0)\}
\end{equation}
From Fig.\ref{fig:train_sample_01_00900}, , we can observe that the multi-scale CNN architecture successfully learns the Conway's Game of Life dynamics with remarkable accuracy. The model maintains high fidelity even for two-step ahead prediction $t+2$, showing only minimal prediction errors scattered across a few isolated regions. The predicted state at $t+2$ closely matches the true system evolution. From Figure.\ref{fig:Conway_learn}, we can observe that the learned rules closely approximate the true Conway's Game of Life dynamics. In the ``dead to living'' transition, predictions are almost exclusively concentrated at exactly 3 neighbors, precisely matching the birth rule. For ``living to living'' transitions, predictions are predominantly concentrated at 2 and 3 neighbors, perfectly aligning with the survival rule. The ``dead to dead'' and ``living to dead'' distributions demonstrate correct patterns for cell death or stasis under non-survival conditions, with prediction frequencies exhibiting exponential decay at incorrect neighbor counts.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{fig/train_sample_01_00900.png}
\caption{Multi-Scale Convolutional Neural Networks prediction accuracy for cellular automaton dynamics. Top row shows the system evolution from time $t$ to $t+1$, with perfect prediction accuracy. Bottom row demonstrates prediction at time $t+2$. The right panels display prediction errors (top) and state differences between consecutive time steps (bottom).}
 \label{fig:train_sample_01_00900}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/convolution_kernels_visualization_2channels.png}
    \caption{Visualization of learned convolution kernels of 1st layer in dataset with rule B3/S23}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fig/Conway_learn.png}
\caption{Learned cellular automaton rules.}
\label{fig:Conway_learn}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/training_results_0-B3_S23.png}
    \caption{Training curves of different network architectures on dataset with rule B3/S23}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/no_suptitle_filters_B2_S_CNN-small_conv1.png}\\
    \includegraphics[width=\linewidth]{fig/no_suptitle_filters_B13_S012V_CNN-small_conv1.png}
    \caption{Trained convolution kernel weights of the 1st layer on dataset with rule B2/S (Up) and B13/S012V (Down)}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\linewidth]{fig/stats_out-B3_S23-0013.svg}
    \includesvg[width=\linewidth]{fig/stats_out-B36_S23-0043.svg}
    \caption{Counting statistics of network-learned dynamics on whole dataset}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\linewidth]{fig/stats_out-B234_S-0001.svg}
    \includesvg[width=\linewidth]{fig/stats_out-B345_S5-0006.svg}
    \caption{Counting statistics of network-learned dynamics on whole dataset}
    \label{fig:placeholder}
\end{figure}

% The table of different life rules used in this work 
% (in typst code, need to be translated into LaTeX)
%
% #align(center,
%   table(
%     columns: 4,
%     align: (center, center, center, left),
%     stroke: none,
%     table.hline(),
%     table.header([规则名称], [邻域大小], [邻域类型], [特性描述]),
%     table.hline(stroke: 0.5pt),
%     [`B36/S23`],      [3], [Moore],       [和 Conway 的原版生命游戏相似，但有自我复制结构],
%     [`B3678/S34678`], [3], [Moore],       [活细胞群中的死细胞的行为与死细胞群中的活细胞的行为相同],
%     [`B35678/S5678`], [3], [Moore],       [有不可预测行为的菱形斑点],
%     [`B2/S`],         [3], [Moore],       [活细胞每代都会死亡，但该系统常常爆发],
%     [`B234/S`],       [3], [Moore],       [单个的 $2 times 2$ 会演化为一个波斯地毯],
%     [`B345/S5`],      [3], [Moore],       [周期极长的振荡器可以自然地出现],
%     [`B13/S012V`],    [3], [Von Neumann], [],
%     [`B2/S013V`],     [3], [Von Neumann], [],
%     table.hline(),
%   )
% )
%
% With some of them translated into English
% #figure(
%   table(
%     columns: 3, 
%     align: (center, center, left),
%     rows: 7, 
%     row-gutter: 0.5em,
%     stroke: none,
%     table.hline(),
%     table.header([Rule Symbol], [Rule Name], [Description]),
%     table.hline(stroke: 0.5pt),
%     [`B3/S23`], text(size: 16pt)[Life], text(size: 14pt)[John Conway's rule is by far the best known and most explored CA.],
%     [`B36/S23`], text(size: 16pt)[HighLife], text(size: 14pt)[Very similar to Conway's Life but with an interesting replicator.],
%     [`B3678/S34678`], text(size: 16pt)[Day & Night], text(size: 14pt)[Dead cells in a sea of live cells behave the same as live cells in a sea of dead cells.],
%     [`B35678/S5678`], text(size: 16pt)[Diamoeba], text(size: 14pt)[Creates diamond-shaped blobs with unpredictable behavior.],
%     [`B2/S`], text(size: 16pt)[Seeds], text(size: 14pt)[Every living cell dies every generation, but most patterns still explode.],
%     [`B234/S`], text(size: 16pt)[Serviettes or Persian Rug], text(size: 14pt)[A single 2x2 block turns into a set of Persian rugs.],
%     [`B345/S5`], text(size: 16pt)[LongLife], text(size: 14pt)[Oscillators with extremely long periods can occur quite naturally.],
%     table.hline()
%   )
% )

% The table of testing results of different models on different rule data. 
% (in typst code, need to be translated into LaTeX)
%
% #figure(
%   table(
%     columns: 7,
%     align: (horizon+center, right, right, right, right, right, right, right, right),
%     stroke: none,
%     table.hline(),
%     table.header([演化规则 / 模型名], [CNN-tiny], [CNN-small], [MCNN], [P4CNN-tiny], [P4CNN-small], [P4MCNN]),
%     table.hline(stroke: 0.5pt),
%     [`B36/S23`], [0.7002\ 86.79%], [0.0045\ 100.00%], [0.0055\ 99.97%], [0.3631\ 94.74%], [0.0037\ 100%], [*0.0036\ 100.00%*], 
%     [`B36/S23`],  [0.3451\ 90.46%], [*0.0167\ 99.89%*], [0.0262\ 98.94%], [0.4740\ 92.32%], [0.0274\ 99.77%], [0.0148\ 99.86%], 
%     [#highlight[`B3678/S34678`]],  [0.2035\ 92.36%], [0.0133\ 99.96%], [0.0660\ 98.19%], [0.4534\ 93.40%], [0.0159\ 99.96%], [*0.0097\ 99.98%*], 
%     [#highlight[`B35678/S5678`]],  [0.0165\ 99.24%], [0.0216\ 98.73%], [0.0955\ 99.52%], [0.0086\ 99.32%], [0.0058\ 99.65%], [*0.0041\ 99.77%*], 
%     [`B2/S`],  [0.0231\ 99.74%], [*0.0023\ 100.00%*], [0.0024\ 100.00%], [0.6136\ 88.79%], [0.0022\ 100.00%], [0.0034\ 100.00%], 
%     [`B345/S5`],   [0.1710\ 96.25%], [0.0065\ 100.00%], [0.0039\ 100.00%], [*0.0028\ 100.00%*], [*0.0028\ 100.00%*], [0.0119\ 99.92%], 
%     [`B13/S012V`],  [0.2489\ 92.30%], [0.0066\ 100.00%], [0.0045\ 99.99%], [0.1243\ 99.04%], [0.0016\ 100.00%], [*0.0010\ 100.00%*], 
%     [`B2/S013V`], [0.5533\ 77.24%], [0.0046\ 100.00%], [0.0025\ 100.00%], [0.7082\ 84.71%], [0.0091\ 100.00%], [*0.0015\ 100.00%*], 
%     table.hline(),
%   )
% )

% The table of model params and FLOPs
% (in typst code, need to be translated into LaTeX)
%
% #figure(
%   table(
%     columns: 5,
%     align: (horizon+center, right, right, right),
%     stroke: none,
%     table.hline(),
%     table.header([模型名], [FLOPs (M)], [参数量], [估计大小 (MB)], [最低正确率 $(%)$]),
%     table.hline(stroke: 0.5pt),
%     [P4CNN-tiny], [$1.28$], [$94$], [11.21], [84.71],
%     [CNN-tiny], [$5.32$], [$133$], [2.24], [77.24],
%     [MCNN], [$40.48$], [$1,004$], [7.36], [98.19],
%     [*P4MCNN*], [*$6.4$*], [*$1,276$*], [---], [*99.77*],
%     [CNN-small], [$98.5$], [$2,450$], [---], [98.73%],
%     [P4CNN-small], [$10.24$], [$3,202$], [---], [99.65],
%     table.hline(),
%   )
% )

% The table of model params and FLOPs
% (in typst code, need to be translated into LaTeX)
%
% 


\bibliographystyle{unsrt}
\bibliography{ref}% 





\end{document}