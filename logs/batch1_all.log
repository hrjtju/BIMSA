rule: B3/S23 \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_17-14-45_small_2_layer_seq_p4cnn__200-200-B3_S23


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
����R2Conv: 1-1                                      --                        176
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        16
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        2,816
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        16
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        178
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 17:15:08.997632 | Idx:   40/400    | loss: 0.661 | grad_norm: 1.810 | acc: 86.55% |
| 2025-10-16 17:15:18.914351 | Idx:   80/400    | loss: 0.474 | grad_norm: 0.547 | acc: 96.45% |
| 2025-10-16 17:15:30.590534 | Idx:  120/400    | loss: 0.363 | grad_norm: 0.732 | acc: 98.10% |
| 2025-10-16 17:15:40.891364 | Idx:  160/400    | loss: 0.287 | grad_norm: 0.586 | acc: 98.54% |
| 2025-10-16 17:15:51.195911 | Idx:  200/400    | loss: 0.241 | grad_norm: 0.357 | acc: 99.10% |
| 2025-10-16 17:16:02.953019 | Idx:  240/400    | loss: 0.217 | grad_norm: 0.143 | acc: 99.69% |
| 2025-10-16 17:16:13.137970 | Idx:  280/400    | loss: 0.190 | grad_norm: 0.213 | acc: 99.69% |
| 2025-10-16 17:16:24.718418 | Idx:  320/400    | loss: 0.169 | grad_norm: 0.112 | acc: 99.94% |
| 2025-10-16 17:16:34.818192 | Idx:  360/400    | loss: 0.154 | grad_norm: 0.104 | acc: 99.84% |
| 2025-10-16 17:16:43.470704 | Idx:  400/400    | loss: 0.143 | grad_norm: 0.062 | acc: 99.95% |
Train Loss: 0.1426 Acc: 96.46%
Acc: 99.99%


| val_epoch_acc: 99.99% | epoch: 00 | avg_train_loss: 1.0000 | avg_train_acc: 99.8692 | 


Epoch 2/8
----------
| 2025-10-16 17:17:19.290996 | Idx:   40/400    | loss: 0.024 | grad_norm: 0.009 | acc: 99.99% |
| 2025-10-16 17:17:29.624283 | Idx:   80/400    | loss: 0.031 | grad_norm: 0.041 | acc: 99.98% |
| 2025-10-16 17:17:41.147831 | Idx:  120/400    | loss: 0.027 | grad_norm: 0.053 | acc: 100.00% |
| 2025-10-16 17:17:51.313803 | Idx:  160/400    | loss: 0.024 | grad_norm: 0.052 | acc: 99.96% |
| 2025-10-16 17:18:00.225764 | Idx:  200/400    | loss: 0.021 | grad_norm: 0.021 | acc: 99.99% |
| 2025-10-16 17:18:10.361603 | Idx:  240/400    | loss: 0.019 | grad_norm: 0.039 | acc: 100.00% |
| 2025-10-16 17:18:19.869589 | Idx:  280/400    | loss: 0.017 | grad_norm: 0.014 | acc: 99.99% |
| 2025-10-16 17:18:31.426211 | Idx:  320/400    | loss: 0.017 | grad_norm: 0.045 | acc: 99.99% |
| 2025-10-16 17:18:41.524134 | Idx:  360/400    | loss: 0.016 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-16 17:18:50.107943 | Idx:  400/400    | loss: 0.015 | grad_norm: 0.016 | acc: 100.00% |
Train Loss: 0.0151 Acc: 99.82%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 1.0000 | avg_train_acc: 99.9392 | 


Epoch 3/8
----------
| 2025-10-16 17:19:23.137363 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-16 17:19:31.730773 | Idx:   80/400    | loss: 0.005 | grad_norm: 0.025 | acc: 99.99% |
| 2025-10-16 17:19:41.735108 | Idx:  120/400    | loss: 0.005 | grad_norm: 0.016 | acc: 100.00% |
| 2025-10-16 17:19:50.324204 | Idx:  160/400    | loss: 0.006 | grad_norm: 0.014 | acc: 99.99% |
| 2025-10-16 17:19:58.957751 | Idx:  200/400    | loss: 0.005 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 17:20:10.775287 | Idx:  240/400    | loss: 0.023 | grad_norm: 0.126 | acc: 99.71% |
| 2025-10-16 17:20:20.933394 | Idx:  280/400    | loss: 0.024 | grad_norm: 0.095 | acc: 99.97% |
| 2025-10-16 17:20:32.455486 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.226 | acc: 99.68% |
| 2025-10-16 17:20:42.706249 | Idx:  360/400    | loss: 0.031 | grad_norm: 0.112 | acc: 99.84% |
| 2025-10-16 17:20:52.673636 | Idx:  400/400    | loss: 0.030 | grad_norm: 0.124 | acc: 99.87% |
Train Loss: 0.0297 Acc: 99.67%
Acc: 99.15%


| val_epoch_acc: 99.15% | epoch: 02 | avg_train_loss: 1.0000 | avg_train_acc: 99.7242 | 


Epoch 4/8
----------
| 2025-10-16 17:21:25.920228 | Idx:   40/400    | loss: 0.020 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-16 17:21:34.591783 | Idx:   80/400    | loss: 0.019 | grad_norm: 0.027 | acc: 100.00% |
| 2025-10-16 17:21:45.401895 | Idx:  120/400    | loss: 0.019 | grad_norm: 0.093 | acc: 99.88% |
| 2025-10-16 17:21:55.572304 | Idx:  160/400    | loss: 0.017 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 17:22:05.790873 | Idx:  200/400    | loss: 0.015 | grad_norm: 0.018 | acc: 100.00% |
| 2025-10-16 17:22:15.708569 | Idx:  240/400    | loss: 0.013 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 17:22:24.324170 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:22:34.552551 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 17:22:43.130645 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-16 17:22:51.686693 | Idx:  400/400    | loss: 0.010 | grad_norm: 0.099 | acc: 99.97% |
Train Loss: 0.0097 Acc: 99.90%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 1.0000 | avg_train_acc: 99.9929 | 


Epoch 5/8
----------
| 2025-10-16 17:23:24.895377 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:23:33.626779 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 17:23:43.565466 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-16 17:23:52.159136 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.060 | acc: 99.97% |
| 2025-10-16 17:24:00.819871 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 17:24:10.759827 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 17:24:19.334216 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 17:24:29.555119 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-16 17:24:38.171606 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.857 | acc: 98.78% |
| 2025-10-16 17:24:47.146344 | Idx:  400/400    | loss: 0.016 | grad_norm: 0.268 | acc: 99.97% |
Train Loss: 0.0157 Acc: 99.82%
Acc: 99.97%


| val_epoch_acc: 99.97% | epoch: 04 | avg_train_loss: 1.0000 | avg_train_acc: 98.5451 | 


Epoch 6/8
----------
| 2025-10-16 17:25:23.370314 | Idx:   40/400    | loss: 0.044 | grad_norm: 0.200 | acc: 99.75% |
| 2025-10-16 17:25:33.550497 | Idx:   80/400    | loss: 0.031 | grad_norm: 0.021 | acc: 99.99% |
| 2025-10-16 17:25:45.103221 | Idx:  120/400    | loss: 0.023 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-16 17:25:53.677931 | Idx:  160/400    | loss: 0.020 | grad_norm: 1.385 | acc: 97.42% |
| 2025-10-16 17:26:02.301270 | Idx:  200/400    | loss: 0.020 | grad_norm: 2.523 | acc: 99.31% |
| 2025-10-16 17:26:12.257767 | Idx:  240/400    | loss: 0.023 | grad_norm: 0.036 | acc: 100.00% |
| 2025-10-16 17:26:20.887268 | Idx:  280/400    | loss: 0.020 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 17:26:30.877319 | Idx:  320/400    | loss: 0.018 | grad_norm: 0.017 | acc: 100.00% |
| 2025-10-16 17:26:39.515555 | Idx:  360/400    | loss: 0.017 | grad_norm: 0.068 | acc: 99.95% |
| 2025-10-16 17:26:48.140634 | Idx:  400/400    | loss: 0.017 | grad_norm: 0.271 | acc: 99.96% |
Train Loss: 0.0167 Acc: 99.79%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 05 | avg_train_loss: 1.0000 | avg_train_acc: 99.7452 | 


Epoch 7/8
----------
| 2025-10-16 17:27:21.916896 | Idx:   40/400    | loss: 0.006 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 17:27:30.557381 | Idx:   80/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-16 17:27:40.454219 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:27:49.079810 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 17:27:57.751249 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 17:28:07.690621 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 17:28:16.338585 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:28:26.307575 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 17:28:34.991711 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:28:45.107078 | Idx:  400/400    | loss: 0.004 | grad_norm: 0.024 | acc: 99.99% |
Train Loss: 0.0036 Acc: 99.99%
Acc: 99.99%


| val_epoch_acc: 99.99% | epoch: 06 | avg_train_loss: 1.0000 | avg_train_acc: 99.9972 | 


Epoch 8/8
----------
| 2025-10-16 17:29:19.876804 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 17:29:28.604880 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-16 17:29:39.493894 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-16 17:29:49.631583 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 17:29:59.873827 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 17:30:09.851666 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 17:30:18.806377 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.025 | acc: 99.99% |
| 2025-10-16 17:30:28.647996 | Idx:  320/400    | loss: 0.010 | grad_norm: 2.047 | acc: 96.48% |
| 2025-10-16 17:30:36.382424 | Idx:  360/400    | loss: 0.015 | grad_norm: 9.337 | acc: 95.01% |
| 2025-10-16 17:30:45.098169 | Idx:  400/400    | loss: 0.023 | grad_norm: 0.165 | acc: 99.73% |
Train Loss: 0.0234 Acc: 99.75%
Acc: 99.74%


| val_epoch_acc: 99.74% | epoch: 07 | avg_train_loss: 1.0000 | avg_train_acc: 98.7461 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_171445-1uwgizyq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_171445-1uwgizyq\logs[0m
rule: B36/S23 \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_17-31-10_small_2_layer_seq_p4cnn__200-200-B36_S23


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
����R2Conv: 1-1                                      --                        176
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        16
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        2,816
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        16
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        178
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 17:31:33.822051 | Idx:   40/400    | loss: 0.553 | grad_norm: 4.330 | acc: 89.87% |
| 2025-10-16 17:31:41.481144 | Idx:   80/400    | loss: 0.447 | grad_norm: 0.519 | acc: 93.15% |
| 2025-10-16 17:31:50.501357 | Idx:  120/400    | loss: 0.397 | grad_norm: 3.069 | acc: 91.46% |
| 2025-10-16 17:31:58.150644 | Idx:  160/400    | loss: 0.362 | grad_norm: 0.128 | acc: 94.35% |
| 2025-10-16 17:32:05.824911 | Idx:  200/400    | loss: 0.334 | grad_norm: 0.420 | acc: 93.38% |
| 2025-10-16 17:32:15.170603 | Idx:  240/400    | loss: 0.317 | grad_norm: 0.669 | acc: 94.23% |
| 2025-10-16 17:32:22.825216 | Idx:  280/400    | loss: 0.300 | grad_norm: 1.186 | acc: 93.05% |
| 2025-10-16 17:32:32.056248 | Idx:  320/400    | loss: 0.282 | grad_norm: 0.106 | acc: 96.99% |
| 2025-10-16 17:32:41.586046 | Idx:  360/400    | loss: 0.266 | grad_norm: 0.895 | acc: 98.69% |
| 2025-10-16 17:32:51.038713 | Idx:  400/400    | loss: 0.252 | grad_norm: 0.220 | acc: 98.38% |
Train Loss: 0.2518 Acc: 93.45%
Acc: 97.90%


| val_epoch_acc: 97.90% | epoch: 00 | avg_train_loss: 1.0000 | avg_train_acc: 96.9534 | 


Epoch 2/8
----------
| 2025-10-16 17:33:27.256328 | Idx:   40/400    | loss: 0.100 | grad_norm: 0.495 | acc: 97.00% |
| 2025-10-16 17:33:37.495220 | Idx:   80/400    | loss: 0.105 | grad_norm: 0.214 | acc: 98.03% |
| 2025-10-16 17:33:48.971527 | Idx:  120/400    | loss: 0.097 | grad_norm: 0.194 | acc: 98.04% |
| 2025-10-16 17:33:59.135496 | Idx:  160/400    | loss: 0.095 | grad_norm: 0.228 | acc: 98.44% |
| 2025-10-16 17:34:09.341951 | Idx:  200/400    | loss: 0.088 | grad_norm: 0.033 | acc: 99.46% |
| 2025-10-16 17:34:21.139938 | Idx:  240/400    | loss: 0.083 | grad_norm: 0.392 | acc: 98.97% |
| 2025-10-16 17:34:31.343921 | Idx:  280/400    | loss: 0.079 | grad_norm: 0.092 | acc: 99.41% |
| 2025-10-16 17:34:42.975942 | Idx:  320/400    | loss: 0.076 | grad_norm: 0.219 | acc: 99.10% |
| 2025-10-16 17:34:53.157687 | Idx:  360/400    | loss: 0.073 | grad_norm: 0.073 | acc: 99.48% |
| 2025-10-16 17:35:03.310787 | Idx:  400/400    | loss: 0.071 | grad_norm: 0.657 | acc: 99.51% |
Train Loss: 0.0705 Acc: 98.64%
Acc: 99.52%


| val_epoch_acc: 99.52% | epoch: 01 | avg_train_loss: 1.0000 | avg_train_acc: 99.2795 | 


Epoch 3/8
----------
| 2025-10-16 17:35:39.479923 | Idx:   40/400    | loss: 0.039 | grad_norm: 0.330 | acc: 99.65% |
| 2025-10-16 17:35:49.624811 | Idx:   80/400    | loss: 0.035 | grad_norm: 0.092 | acc: 99.65% |
| 2025-10-16 17:36:01.218670 | Idx:  120/400    | loss: 0.051 | grad_norm: 0.121 | acc: 99.40% |
| 2025-10-16 17:36:11.448380 | Idx:  160/400    | loss: 0.080 | grad_norm: 1.933 | acc: 97.95% |
| 2025-10-16 17:36:21.689944 | Idx:  200/400    | loss: 0.073 | grad_norm: 0.273 | acc: 99.33% |
| 2025-10-16 17:36:33.738219 | Idx:  240/400    | loss: 0.072 | grad_norm: 0.523 | acc: 98.57% |
| 2025-10-16 17:36:43.956386 | Idx:  280/400    | loss: 0.066 | grad_norm: 0.133 | acc: 99.45% |
| 2025-10-16 17:36:55.511907 | Idx:  320/400    | loss: 0.062 | grad_norm: 0.063 | acc: 99.66% |
| 2025-10-16 17:37:05.695482 | Idx:  360/400    | loss: 0.060 | grad_norm: 0.120 | acc: 99.65% |
| 2025-10-16 17:37:15.881837 | Idx:  400/400    | loss: 0.057 | grad_norm: 0.067 | acc: 99.74% |
Train Loss: 0.0565 Acc: 99.22%
Acc: 99.74%


| val_epoch_acc: 99.74% | epoch: 02 | avg_train_loss: 1.0000 | avg_train_acc: 99.7200 | 


Epoch 4/8
----------
| 2025-10-16 17:37:52.014792 | Idx:   40/400    | loss: 0.023 | grad_norm: 0.190 | acc: 99.75% |
| 2025-10-16 17:38:02.213981 | Idx:   80/400    | loss: 0.023 | grad_norm: 0.064 | acc: 99.82% |
| 2025-10-16 17:38:13.813103 | Idx:  120/400    | loss: 0.044 | grad_norm: 0.503 | acc: 99.56% |
| 2025-10-16 17:38:24.058864 | Idx:  160/400    | loss: 0.046 | grad_norm: 0.428 | acc: 99.30% |
| 2025-10-16 17:38:34.240460 | Idx:  200/400    | loss: 0.043 | grad_norm: 0.160 | acc: 99.80% |
| 2025-10-16 17:38:45.762177 | Idx:  240/400    | loss: 0.042 | grad_norm: 0.770 | acc: 99.15% |
| 2025-10-16 17:38:56.020119 | Idx:  280/400    | loss: 0.040 | grad_norm: 0.070 | acc: 99.72% |
| 2025-10-16 17:39:07.860622 | Idx:  320/400    | loss: 0.039 | grad_norm: 0.053 | acc: 99.81% |
| 2025-10-16 17:39:18.212334 | Idx:  360/400    | loss: 0.037 | grad_norm: 0.166 | acc: 99.74% |
| 2025-10-16 17:39:28.353761 | Idx:  400/400    | loss: 0.036 | grad_norm: 0.076 | acc: 99.80% |
Train Loss: 0.0356 Acc: 99.51%
Acc: 99.79%


| val_epoch_acc: 99.79% | epoch: 03 | avg_train_loss: 1.0000 | avg_train_acc: 99.7715 | 


Epoch 5/8
----------
| 2025-10-16 17:40:04.388698 | Idx:   40/400    | loss: 0.017 | grad_norm: 0.058 | acc: 99.78% |
| 2025-10-16 17:40:14.618167 | Idx:   80/400    | loss: 0.035 | grad_norm: 0.435 | acc: 98.32% |
| 2025-10-16 17:40:26.209614 | Idx:  120/400    | loss: 0.037 | grad_norm: 0.077 | acc: 99.77% |
| 2025-10-16 17:40:36.417180 | Idx:  160/400    | loss: 0.041 | grad_norm: 0.081 | acc: 99.75% |
| 2025-10-16 17:40:46.514543 | Idx:  200/400    | loss: 0.061 | grad_norm: 0.979 | acc: 97.83% |
| 2025-10-16 17:40:58.069073 | Idx:  240/400    | loss: 0.061 | grad_norm: 3.188 | acc: 96.13% |
| 2025-10-16 17:41:08.272257 | Idx:  280/400    | loss: 0.061 | grad_norm: 0.112 | acc: 99.59% |
| 2025-10-16 17:41:19.897682 | Idx:  320/400    | loss: 0.057 | grad_norm: 0.023 | acc: 99.79% |
| 2025-10-16 17:41:30.180253 | Idx:  360/400    | loss: 0.053 | grad_norm: 0.057 | acc: 99.80% |
| 2025-10-16 17:41:40.420096 | Idx:  400/400    | loss: 0.050 | grad_norm: 0.100 | acc: 99.84% |
Train Loss: 0.0497 Acc: 99.35%
Acc: 99.82%


| val_epoch_acc: 99.82% | epoch: 04 | avg_train_loss: 1.0000 | avg_train_acc: 99.7968 | 


Epoch 6/8
----------
| 2025-10-16 17:42:16.300380 | Idx:   40/400    | loss: 0.022 | grad_norm: 0.095 | acc: 99.69% |
| 2025-10-16 17:42:26.454986 | Idx:   80/400    | loss: 0.020 | grad_norm: 0.089 | acc: 99.86% |
| 2025-10-16 17:42:38.099150 | Idx:  120/400    | loss: 0.018 | grad_norm: 0.112 | acc: 99.74% |
| 2025-10-16 17:42:48.261301 | Idx:  160/400    | loss: 0.017 | grad_norm: 0.104 | acc: 99.84% |
| 2025-10-16 17:42:58.479867 | Idx:  200/400    | loss: 0.017 | grad_norm: 0.092 | acc: 99.83% |
| 2025-10-16 17:43:10.110239 | Idx:  240/400    | loss: 0.017 | grad_norm: 0.067 | acc: 99.86% |
| 2025-10-16 17:43:20.295419 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.098 | acc: 99.76% |
| 2025-10-16 17:43:31.894369 | Idx:  320/400    | loss: 0.016 | grad_norm: 0.120 | acc: 99.88% |
| 2025-10-16 17:43:42.038893 | Idx:  360/400    | loss: 0.015 | grad_norm: 0.047 | acc: 99.92% |
| 2025-10-16 17:43:52.196605 | Idx:  400/400    | loss: 0.024 | grad_norm: 0.550 | acc: 99.11% |
Train Loss: 0.0240 Acc: 99.75%
Acc: 98.82%


| val_epoch_acc: 98.82% | epoch: 05 | avg_train_loss: 1.0000 | avg_train_acc: 98.9441 | 


Epoch 7/8
----------
| 2025-10-16 17:44:27.893732 | Idx:   40/400    | loss: 0.052 | grad_norm: 5.063 | acc: 98.97% |
| 2025-10-16 17:44:38.014495 | Idx:   80/400    | loss: 0.248 | grad_norm: 1.015 | acc: 98.84% |
| 2025-10-16 17:44:49.761200 | Idx:  120/400    | loss: 0.196 | grad_norm: 0.243 | acc: 99.15% |
| 2025-10-16 17:45:00.256993 | Idx:  160/400    | loss: 0.155 | grad_norm: 0.077 | acc: 99.83% |
| 2025-10-16 17:45:10.741985 | Idx:  200/400    | loss: 0.129 | grad_norm: 0.113 | acc: 99.77% |
| 2025-10-16 17:45:22.634638 | Idx:  240/400    | loss: 0.110 | grad_norm: 0.068 | acc: 99.83% |
| 2025-10-16 17:45:33.150918 | Idx:  280/400    | loss: 0.097 | grad_norm: 0.052 | acc: 99.85% |
| 2025-10-16 17:45:45.045477 | Idx:  320/400    | loss: 0.086 | grad_norm: 0.079 | acc: 99.88% |
| 2025-10-16 17:45:54.071814 | Idx:  360/400    | loss: 0.078 | grad_norm: 0.039 | acc: 99.88% |
| 2025-10-16 17:46:01.726573 | Idx:  400/400    | loss: 0.072 | grad_norm: 0.095 | acc: 99.80% |
Train Loss: 0.0719 Acc: 99.23%
Acc: 99.80%


| val_epoch_acc: 99.80% | epoch: 06 | avg_train_loss: 1.0000 | avg_train_acc: 99.8642 | 


Epoch 8/8
----------
| 2025-10-16 17:46:33.641232 | Idx:   40/400    | loss: 0.016 | grad_norm: 0.028 | acc: 99.91% |
| 2025-10-16 17:46:41.317652 | Idx:   80/400    | loss: 0.016 | grad_norm: 0.073 | acc: 99.83% |
| 2025-10-16 17:46:50.309851 | Idx:  120/400    | loss: 0.015 | grad_norm: 0.058 | acc: 99.88% |
| 2025-10-16 17:46:58.000317 | Idx:  160/400    | loss: 0.018 | grad_norm: 0.170 | acc: 99.70% |
| 2025-10-16 17:47:05.653385 | Idx:  200/400    | loss: 0.017 | grad_norm: 0.180 | acc: 99.75% |
| 2025-10-16 17:47:14.638062 | Idx:  240/400    | loss: 0.016 | grad_norm: 0.085 | acc: 99.92% |
| 2025-10-16 17:47:22.281759 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.036 | acc: 99.89% |
| 2025-10-16 17:47:31.310370 | Idx:  320/400    | loss: 0.018 | grad_norm: 2.049 | acc: 98.79% |
| 2025-10-16 17:47:39.015359 | Idx:  360/400    | loss: 0.019 | grad_norm: 0.122 | acc: 99.78% |
| 2025-10-16 17:47:46.682864 | Idx:  400/400    | loss: 0.020 | grad_norm: 0.033 | acc: 99.89% |
Train Loss: 0.0195 Acc: 99.75%
Acc: 99.86%


| val_epoch_acc: 99.86% | epoch: 07 | avg_train_loss: 1.0000 | avg_train_acc: 99.7023 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_173110-oagnv0qc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_173110-oagnv0qc\logs[0m
rule: B3678/S34678 \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_17-48-10_small_2_layer_seq_p4cnn__200-200-B3678_S34678


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
����R2Conv: 1-1                                      --                        176
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        16
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        2,816
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        16
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        178
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 17:48:31.403741 | Idx:   40/400    | loss: 1.218 | grad_norm: 0.348 | acc: 93.62% |
| 2025-10-16 17:48:39.010189 | Idx:   80/400    | loss: 0.784 | grad_norm: 0.096 | acc: 93.52% |
| 2025-10-16 17:48:47.943929 | Idx:  120/400    | loss: 0.630 | grad_norm: 0.128 | acc: 90.38% |
| 2025-10-16 17:48:55.574611 | Idx:  160/400    | loss: 0.548 | grad_norm: 0.429 | acc: 91.65% |
| 2025-10-16 17:49:03.212865 | Idx:  200/400    | loss: 0.493 | grad_norm: 0.171 | acc: 95.88% |
| 2025-10-16 17:49:12.564796 | Idx:  240/400    | loss: 0.457 | grad_norm: 0.576 | acc: 94.90% |
| 2025-10-16 17:49:20.379524 | Idx:  280/400    | loss: 0.421 | grad_norm: 0.909 | acc: 96.38% |
| 2025-10-16 17:49:29.459816 | Idx:  320/400    | loss: 0.393 | grad_norm: 0.129 | acc: 97.95% |
| 2025-10-16 17:49:37.265728 | Idx:  360/400    | loss: 0.367 | grad_norm: 0.227 | acc: 96.76% |
| 2025-10-16 17:49:45.161578 | Idx:  400/400    | loss: 0.342 | grad_norm: 1.407 | acc: 96.61% |
Train Loss: 0.3422 Acc: 92.97%
Acc: 97.32%


| val_epoch_acc: 97.32% | epoch: 00 | avg_train_loss: 1.0000 | avg_train_acc: 98.0674 | 


[0.0843263640999794, 0.08064868301153183, 0.1870139241218567, 0.12655004858970642, 0.09117674827575684, 0.08496624231338501, 0.1359761655330658, 0.07124607264995575, 0.09939083456993103, 0.091221883893013, 0.07461024820804596, 0.08888647705316544, 0.12979649007320404, 0.08733958750963211, 0.09519931674003601, 0.08277188241481781, 0.12581929564476013, 0.08485881984233856, 0.12449826300144196, 0.12853918969631195, 0.10099655389785767, 0.07757682353258133, 0.08989318460226059, 0.1298089325428009, 0.06789504736661911, 0.0842667892575264, 0.15682503581047058, 0.11173240095376968, 0.15746672451496124, 0.14640262722969055]
Epoch 2/8
----------
| 2025-10-16 17:50:17.183232 | Idx:   40/400    | loss: 0.166 | grad_norm: 1.688 | acc: 91.35% |
| 2025-10-16 17:50:24.877104 | Idx:   80/400    | loss: 0.141 | grad_norm: 0.151 | acc: 99.02% |
| 2025-10-16 17:50:33.935104 | Idx:  120/400    | loss: 0.122 | grad_norm: 0.144 | acc: 99.55% |
| 2025-10-16 17:50:41.633684 | Idx:  160/400    | loss: 0.114 | grad_norm: 0.233 | acc: 98.94% |
| 2025-10-16 17:50:49.316843 | Idx:  200/400    | loss: 0.105 | grad_norm: 0.323 | acc: 98.83% |
| 2025-10-16 17:50:58.485164 | Idx:  240/400    | loss: 0.097 | grad_norm: 0.271 | acc: 99.24% |
| 2025-10-16 17:51:06.212882 | Idx:  280/400    | loss: 0.093 | grad_norm: 0.314 | acc: 99.35% |
| 2025-10-16 17:51:15.221692 | Idx:  320/400    | loss: 0.089 | grad_norm: 0.516 | acc: 98.80% |
| 2025-10-16 17:51:22.974790 | Idx:  360/400    | loss: 0.085 | grad_norm: 0.213 | acc: 99.58% |
| 2025-10-16 17:51:30.674349 | Idx:  400/400    | loss: 0.082 | grad_norm: 0.127 | acc: 99.49% |
Train Loss: 0.0820 Acc: 98.58%
Acc: 99.78%


| val_epoch_acc: 99.78% | epoch: 01 | avg_train_loss: 1.0000 | avg_train_acc: 99.0915 | 


[0.0558319017291069, 0.11509158462285995, 0.04295627772808075, 0.043279971927404404, 0.04753810167312622, 0.09312764555215836, 0.08895472437143326, 0.14968761801719666, 0.0993962213397026, 0.055546682327985764, 0.04565601050853729, 0.08221133798360825, 0.08181121200323105, 0.03943099081516266, 0.032339226454496384, 0.040836140513420105, 0.035389065742492676, 0.05016714707016945, 0.030819565057754517, 0.025145795196294785, 0.06428186595439911, 0.06583066284656525, 0.05130159854888916, 0.041539810597896576, 0.026243504136800766, 0.030567511916160583, 0.03952523320913315, 0.029154110699892044, 0.026591654866933823, 0.05074475705623627]
Epoch 3/8
----------
| 2025-10-16 17:52:02.295069 | Idx:   40/400    | loss: 0.064 | grad_norm: 1.795 | acc: 96.96% |
| 2025-10-16 17:52:09.972621 | Idx:   80/400    | loss: 0.056 | grad_norm: 0.583 | acc: 98.87% |
| 2025-10-16 17:52:18.965919 | Idx:  120/400    | loss: 0.057 | grad_norm: 0.335 | acc: 99.52% |
| 2025-10-16 17:52:26.976702 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.168 | acc: 99.78% |
| 2025-10-16 17:52:37.428914 | Idx:  200/400    | loss: 0.048 | grad_norm: 0.110 | acc: 99.92% |
| 2025-10-16 17:52:49.541968 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.094 | acc: 99.78% |
| 2025-10-16 17:53:00.081898 | Idx:  280/400    | loss: 0.046 | grad_norm: 0.333 | acc: 99.71% |
| 2025-10-16 17:53:11.945601 | Idx:  320/400    | loss: 0.044 | grad_norm: 0.105 | acc: 99.90% |
| 2025-10-16 17:53:22.396423 | Idx:  360/400    | loss: 0.041 | grad_norm: 0.075 | acc: 99.92% |
| 2025-10-16 17:53:32.869875 | Idx:  400/400    | loss: 0.050 | grad_norm: 0.694 | acc: 98.05% |
Train Loss: 0.0496 Acc: 99.38%
Acc: 95.48%


| val_epoch_acc: 95.48% | epoch: 02 | avg_train_loss: 1.0000 | avg_train_acc: 98.4358 | 


[0.016542961820960045, 0.01714254543185234, 0.014931297861039639, 0.016647344455122948, 0.019081931561231613, 0.01531730592250824, 0.025786083191633224, 0.015849368646740913, 0.013851591385900974, 0.01512875221669674, 0.019192414358258247, 0.013886496424674988, 0.02989986538887024, 0.015942255035042763, 0.015280311927199364, 0.014783026650547981, 0.018765270709991455, 0.016315000131726265, 0.07114927470684052, 0.34647849202156067, 0.08328015357255936, 0.48329150676727295, 1.4288026094436646, 0.819405734539032, 0.3301508128643036, 0.17294330894947052, 0.2654668092727661, 0.25166481733322144, 0.08598996698856354, 0.11138894408941269]
Epoch 4/8
----------
| 2025-10-16 17:54:09.297149 | Idx:   40/400    | loss: 0.044 | grad_norm: 0.064 | acc: 99.83% |
| 2025-10-16 17:54:19.830855 | Idx:   80/400    | loss: 0.034 | grad_norm: 0.083 | acc: 99.94% |
| 2025-10-16 17:54:31.890481 | Idx:  120/400    | loss: 0.049 | grad_norm: 0.729 | acc: 99.08% |
| 2025-10-16 17:54:42.356903 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.085 | acc: 99.82% |
| 2025-10-16 17:54:52.887989 | Idx:  200/400    | loss: 0.046 | grad_norm: 0.083 | acc: 99.91% |
| 2025-10-16 17:55:04.838920 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.380 | acc: 99.42% |
| 2025-10-16 17:55:15.452893 | Idx:  280/400    | loss: 0.045 | grad_norm: 0.247 | acc: 99.83% |
| 2025-10-16 17:55:28.237369 | Idx:  320/400    | loss: 0.044 | grad_norm: 0.553 | acc: 99.64% |
| 2025-10-16 17:55:39.875072 | Idx:  360/400    | loss: 0.044 | grad_norm: 1.558 | acc: 98.52% |
| 2025-10-16 17:55:50.839969 | Idx:  400/400    | loss: 0.042 | grad_norm: 0.071 | acc: 99.87% |
Train Loss: 0.0419 Acc: 99.43%
Acc: 99.88%


| val_epoch_acc: 99.88% | epoch: 03 | avg_train_loss: 1.0000 | avg_train_acc: 99.7281 | 


[0.02109941467642784, 0.01908072456717491, 0.024633323773741722, 0.027208438143134117, 0.018698224797844887, 0.06580263376235962, 0.01923280954360962, 0.02248343452811241, 0.0526617169380188, 0.024087192490696907, 0.020902622491121292, 0.02134871482849121, 0.036889828741550446, 0.02065759152173996, 0.016381140798330307, 0.01791044883430004, 0.024811431765556335, 0.017161276191473007, 0.03812957555055618, 0.033663876354694366, 0.01835986226797104, 0.025508228689432144, 0.019420281052589417, 0.022310398519039154, 0.02040475606918335, 0.01787605509161949, 0.024001464247703552, 0.022084029391407967, 0.020386947318911552, 0.01622460037469864]
Epoch 5/8
----------
| 2025-10-16 17:56:28.948596 | Idx:   40/400    | loss: 0.020 | grad_norm: 0.036 | acc: 99.96% |
| 2025-10-16 17:56:39.400260 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.400 | acc: 99.53% |
| 2025-10-16 17:56:50.338862 | Idx:  120/400    | loss: 0.037 | grad_norm: 0.108 | acc: 99.91% |
| 2025-10-16 17:56:58.445546 | Idx:  160/400    | loss: 0.037 | grad_norm: 0.176 | acc: 99.82% |
| 2025-10-16 17:57:06.524319 | Idx:  200/400    | loss: 0.034 | grad_norm: 0.028 | acc: 99.95% |
| 2025-10-16 17:57:15.927105 | Idx:  240/400    | loss: 0.031 | grad_norm: 0.115 | acc: 99.90% |
| 2025-10-16 17:57:24.044809 | Idx:  280/400    | loss: 0.041 | grad_norm: 0.240 | acc: 99.63% |
| 2025-10-16 17:57:33.557251 | Idx:  320/400    | loss: 0.044 | grad_norm: 0.384 | acc: 99.55% |
| 2025-10-16 17:57:41.661350 | Idx:  360/400    | loss: 0.043 | grad_norm: 0.078 | acc: 99.93% |
| 2025-10-16 17:57:49.751521 | Idx:  400/400    | loss: 0.041 | grad_norm: 0.125 | acc: 99.93% |
Train Loss: 0.0410 Acc: 99.48%
Acc: 99.97%


| val_epoch_acc: 99.97% | epoch: 04 | avg_train_loss: 1.0000 | avg_train_acc: 99.8624 | 


[0.018274005502462387, 0.024779576808214188, 0.025513071566820145, 0.01631910353899002, 0.01962379738688469, 0.030170440673828125, 0.02460094913840294, 0.017828062176704407, 0.01652262546122074, 0.017337216064333916, 0.01603161171078682, 0.016435913741588593, 0.014444411732256413, 0.014918617904186249, 0.013996429741382599, 0.05189713090658188, 0.021541882306337357, 0.014662101864814758, 0.01385994628071785, 0.01524863950908184, 0.014966771006584167, 0.017102159559726715, 0.01569710299372673, 0.02139357104897499, 0.013463679701089859, 0.02829914167523384, 0.013481862843036652, 0.038079604506492615, 0.01414240337908268, 0.02041003853082657]
Epoch 6/8
----------
| 2025-10-16 17:58:22.329498 | Idx:   40/400    | loss: 0.083 | grad_norm: 1.195 | acc: 97.62% |
| 2025-10-16 17:58:30.760031 | Idx:   80/400    | loss: 0.059 | grad_norm: 0.077 | acc: 99.86% |
| 2025-10-16 17:58:40.320198 | Idx:  120/400    | loss: 0.045 | grad_norm: 0.048 | acc: 99.95% |
| 2025-10-16 17:58:48.451102 | Idx:  160/400    | loss: 0.037 | grad_norm: 0.037 | acc: 99.98% |
| 2025-10-16 17:58:56.609640 | Idx:  200/400    | loss: 0.033 | grad_norm: 0.041 | acc: 99.97% |
| 2025-10-16 17:59:05.992896 | Idx:  240/400    | loss: 0.030 | grad_norm: 0.027 | acc: 99.98% |
| 2025-10-16 17:59:14.095624 | Idx:  280/400    | loss: 0.028 | grad_norm: 0.118 | acc: 99.88% |
| 2025-10-16 17:59:23.447717 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.062 | acc: 99.97% |
| 2025-10-16 17:59:31.784794 | Idx:  360/400    | loss: 0.043 | grad_norm: 0.299 | acc: 99.50% |
| 2025-10-16 17:59:40.251308 | Idx:  400/400    | loss: 0.043 | grad_norm: 0.328 | acc: 99.64% |
Train Loss: 0.0434 Acc: 99.52%
Acc: 99.36%


| val_epoch_acc: 99.36% | epoch: 05 | avg_train_loss: 1.0000 | avg_train_acc: 99.4140 | 


[0.024671874940395355, 0.0353078693151474, 0.046722788363695145, 0.029557064175605774, 0.033142250031232834, 0.027711637318134308, 0.03223654627799988, 0.028519531711935997, 0.02762589417397976, 0.02108105458319187, 0.02042839303612709, 0.026371832937002182, 0.015488022938370705, 0.020282987505197525, 0.021472401916980743, 0.017716553062200546, 0.02340736985206604, 0.02542003244161606, 0.0181235671043396, 0.015941239893436432, 0.40468940138816833, 0.0675886869430542, 0.11930453777313232, 0.05070134997367859, 0.04271725192666054, 0.058838605880737305, 0.0401393286883831, 0.03353588283061981, 0.05598796531558037, 0.02932795137166977]
Epoch 7/8
----------
| 2025-10-16 18:00:13.179420 | Idx:   40/400    | loss: 0.049 | grad_norm: 0.600 | acc: 99.28% |
| 2025-10-16 18:00:21.091906 | Idx:   80/400    | loss: 0.039 | grad_norm: 0.079 | acc: 99.88% |
| 2025-10-16 18:00:30.131746 | Idx:  120/400    | loss: 0.033 | grad_norm: 0.233 | acc: 99.70% |
| 2025-10-16 18:00:37.820669 | Idx:  160/400    | loss: 0.030 | grad_norm: 0.064 | acc: 99.96% |
| 2025-10-16 18:00:45.597487 | Idx:  200/400    | loss: 0.026 | grad_norm: 0.033 | acc: 99.98% |
| 2025-10-16 18:00:55.101004 | Idx:  240/400    | loss: 0.024 | grad_norm: 0.062 | acc: 99.97% |
| 2025-10-16 18:01:03.349302 | Idx:  280/400    | loss: 0.022 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-16 18:01:13.166506 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.484 | acc: 98.94% |
| 2025-10-16 18:01:20.934714 | Idx:  360/400    | loss: 0.044 | grad_norm: 0.073 | acc: 99.85% |
| 2025-10-16 18:01:28.822890 | Idx:  400/400    | loss: 0.041 | grad_norm: 0.063 | acc: 99.95% |
Train Loss: 0.0414 Acc: 99.53%
Acc: 99.94%


| val_epoch_acc: 99.94% | epoch: 06 | avg_train_loss: 1.0000 | avg_train_acc: 99.9202 | 


[0.016132431104779243, 0.016902193427085876, 0.015833303332328796, 0.015145691111683846, 0.015982137992978096, 0.015552125871181488, 0.025065021589398384, 0.030635260045528412, 0.01541733555495739, 0.015085484832525253, 0.016961900517344475, 0.03465800732374191, 0.015217972919344902, 0.015872051939368248, 0.0149375069886446, 0.01765211671590805, 0.012985456734895706, 0.012595550157129765, 0.016340069472789764, 0.013361139222979546, 0.014718826860189438, 0.015848828479647636, 0.013284364715218544, 0.013373445719480515, 0.012642433866858482, 0.02403055876493454, 0.012333353981375694, 0.023060282692313194, 0.018479809165000916, 0.014872903004288673]
Epoch 8/8
----------
| 2025-10-16 18:02:01.230778 | Idx:   40/400    | loss: 0.015 | grad_norm: 0.088 | acc: 99.93% |
| 2025-10-16 18:02:08.925840 | Idx:   80/400    | loss: 0.015 | grad_norm: 0.352 | acc: 99.75% |
| 2025-10-16 18:02:17.913093 | Idx:  120/400    | loss: 0.015 | grad_norm: 0.030 | acc: 99.99% |
| 2025-10-16 18:02:25.596886 | Idx:  160/400    | loss: 0.028 | grad_norm: 0.779 | acc: 98.81% |
| 2025-10-16 18:02:33.365284 | Idx:  200/400    | loss: 0.039 | grad_norm: 0.373 | acc: 98.95% |
| 2025-10-16 18:02:42.626314 | Idx:  240/400    | loss: 0.041 | grad_norm: 0.994 | acc: 99.30% |
| 2025-10-16 18:02:50.401389 | Idx:  280/400    | loss: 0.043 | grad_norm: 0.189 | acc: 99.83% |
| 2025-10-16 18:02:59.810385 | Idx:  320/400    | loss: 0.039 | grad_norm: 0.037 | acc: 99.97% |
| 2025-10-16 18:03:07.514546 | Idx:  360/400    | loss: 0.037 | grad_norm: 0.024 | acc: 99.99% |
| 2025-10-16 18:03:15.385015 | Idx:  400/400    | loss: 0.034 | grad_norm: 0.037 | acc: 99.97% |
Train Loss: 0.0341 Acc: 99.64%
Acc: 99.99%


| val_epoch_acc: 99.99% | epoch: 07 | avg_train_loss: 1.0000 | avg_train_acc: 99.9798 | 


[0.01082049310207367, 0.010676408186554909, 0.01014611218124628, 0.012410471215844154, 0.01703944429755211, 0.013066045939922333, 0.010056639090180397, 0.010128535330295563, 0.010191136971116066, 0.01007868256419897, 0.011121219024062157, 0.010464958846569061, 0.010768763720989227, 0.010121812112629414, 0.010451004840433598, 0.01188158243894577, 0.013120433315634727, 0.01011501345783472, 0.010275537148118019, 0.01246601715683937, 0.010106826201081276, 0.010167213156819344, 0.023154117166996002, 0.010321341454982758, 0.01409967988729477, 0.01043838169425726, 0.010904929600656033, 0.01083314698189497, 0.010203108191490173, 0.010412522591650486]
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_174810-tr3grews[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_174810-tr3grews\logs[0m
rule: B35678/S5678 \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_18-03-39_small_2_layer_seq_p4cnn__200-200-B35678_S5678


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
����R2Conv: 1-1                                      --                        176
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        16
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        2,816
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        16
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        178
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 18:04:00.807845 | Idx:   40/400    | loss: 0.352 | grad_norm: 0.101 | acc: 99.82% |
| 2025-10-16 18:04:10.397882 | Idx:   80/400    | loss: 0.217 | grad_norm: 0.192 | acc: 99.60% |
| 2025-10-16 18:04:20.848389 | Idx:  120/400    | loss: 0.155 | grad_norm: 0.035 | acc: 99.70% |
| 2025-10-16 18:04:30.102760 | Idx:  160/400    | loss: 0.125 | grad_norm: 0.116 | acc: 99.79% |
| 2025-10-16 18:04:39.107371 | Idx:  200/400    | loss: 0.105 | grad_norm: 0.012 | acc: 99.90% |
| 2025-10-16 18:04:49.607296 | Idx:  240/400    | loss: 0.091 | grad_norm: 0.006 | acc: 99.95% |
| 2025-10-16 18:04:58.855280 | Idx:  280/400    | loss: 0.082 | grad_norm: 0.155 | acc: 97.71% |
| 2025-10-16 18:05:09.066532 | Idx:  320/400    | loss: 0.078 | grad_norm: 0.072 | acc: 99.32% |
| 2025-10-16 18:05:16.917231 | Idx:  360/400    | loss: 0.074 | grad_norm: 0.046 | acc: 99.47% |
| 2025-10-16 18:05:25.198784 | Idx:  400/400    | loss: 0.069 | grad_norm: 0.016 | acc: 99.88% |
Train Loss: 0.0694 Acc: 98.17%
Acc: 99.58%


| val_epoch_acc: 99.58% | epoch: 00 | avg_train_loss: 0.0259 | avg_train_acc: 99.5163 | 


Epoch 2/8
----------
| 2025-10-16 18:06:00.965046 | Idx:   40/400    | loss: 0.081 | grad_norm: 0.103 | acc: 98.00% |
| 2025-10-16 18:06:08.899442 | Idx:   80/400    | loss: 0.056 | grad_norm: 0.074 | acc: 99.36% |
| 2025-10-16 18:06:17.993710 | Idx:  120/400    | loss: 0.045 | grad_norm: 0.017 | acc: 99.86% |
| 2025-10-16 18:06:26.098747 | Idx:  160/400    | loss: 0.059 | grad_norm: 0.082 | acc: 99.68% |
| 2025-10-16 18:06:33.973132 | Idx:  200/400    | loss: 0.052 | grad_norm: 0.056 | acc: 98.53% |
| 2025-10-16 18:06:43.213874 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.020 | acc: 99.80% |
| 2025-10-16 18:06:51.123426 | Idx:  280/400    | loss: 0.045 | grad_norm: 0.086 | acc: 99.87% |
| 2025-10-16 18:07:00.268917 | Idx:  320/400    | loss: 0.042 | grad_norm: 0.012 | acc: 99.54% |
| 2025-10-16 18:07:08.063800 | Idx:  360/400    | loss: 0.041 | grad_norm: 0.402 | acc: 97.42% |
| 2025-10-16 18:07:16.138759 | Idx:  400/400    | loss: 0.040 | grad_norm: 0.045 | acc: 99.25% |
Train Loss: 0.0398 Acc: 99.33%
Acc: 99.58%


| val_epoch_acc: 99.58% | epoch: 01 | avg_train_loss: 0.0302 | avg_train_acc: 99.4329 | 


Epoch 3/8
----------
| 2025-10-16 18:07:50.529944 | Idx:   40/400    | loss: 0.028 | grad_norm: 0.045 | acc: 99.87% |
| 2025-10-16 18:07:58.354749 | Idx:   80/400    | loss: 0.031 | grad_norm: 0.028 | acc: 99.70% |
| 2025-10-16 18:08:07.541340 | Idx:  120/400    | loss: 0.026 | grad_norm: 0.012 | acc: 99.78% |
| 2025-10-16 18:08:15.423003 | Idx:  160/400    | loss: 0.026 | grad_norm: 0.006 | acc: 99.93% |
| 2025-10-16 18:08:23.279026 | Idx:  200/400    | loss: 0.026 | grad_norm: 0.070 | acc: 99.86% |
| 2025-10-16 18:08:32.580233 | Idx:  240/400    | loss: 0.027 | grad_norm: 0.320 | acc: 98.69% |
| 2025-10-16 18:08:40.425155 | Idx:  280/400    | loss: 0.026 | grad_norm: 0.015 | acc: 99.94% |
| 2025-10-16 18:08:49.386544 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.074 | acc: 99.15% |
| 2025-10-16 18:08:57.205347 | Idx:  360/400    | loss: 0.025 | grad_norm: 0.013 | acc: 99.67% |
| 2025-10-16 18:09:04.992183 | Idx:  400/400    | loss: 0.025 | grad_norm: 0.032 | acc: 99.71% |
Train Loss: 0.0255 Acc: 99.52%
Acc: 99.64%


| val_epoch_acc: 99.64% | epoch: 02 | avg_train_loss: 0.0250 | avg_train_acc: 99.4879 | 


Epoch 4/8
----------
| 2025-10-16 18:09:37.191872 | Idx:   40/400    | loss: 0.020 | grad_norm: 0.278 | acc: 99.12% |
| 2025-10-16 18:09:44.920780 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.360 | acc: 98.42% |
| 2025-10-16 18:09:53.842975 | Idx:  120/400    | loss: 0.078 | grad_norm: 0.215 | acc: 99.21% |
| 2025-10-16 18:10:01.634888 | Idx:  160/400    | loss: 0.084 | grad_norm: 132.427 | acc: 99.87% |
| 2025-10-16 18:10:09.559940 | Idx:  200/400    | loss: 0.075 | grad_norm: 0.019 | acc: 99.63% |
| 2025-10-16 18:10:18.906405 | Idx:  240/400    | loss: 0.069 | grad_norm: 0.329 | acc: 98.14% |
| 2025-10-16 18:10:26.936215 | Idx:  280/400    | loss: 0.065 | grad_norm: 0.006 | acc: 99.97% |
| 2025-10-16 18:10:36.948605 | Idx:  320/400    | loss: 0.061 | grad_norm: 0.009 | acc: 99.43% |
| 2025-10-16 18:10:44.941343 | Idx:  360/400    | loss: 0.059 | grad_norm: 0.030 | acc: 99.05% |
| 2025-10-16 18:10:52.798671 | Idx:  400/400    | loss: 0.057 | grad_norm: 0.009 | acc: 99.82% |
Train Loss: 0.0565 Acc: 99.08%
Acc: 99.60%


| val_epoch_acc: 99.60% | epoch: 03 | avg_train_loss: 0.0341 | avg_train_acc: 99.3631 | 


Epoch 5/8
----------
| 2025-10-16 18:11:26.319969 | Idx:   40/400    | loss: 0.028 | grad_norm: 0.007 | acc: 99.81% |
| 2025-10-16 18:11:34.690314 | Idx:   80/400    | loss: 0.024 | grad_norm: 0.016 | acc: 99.67% |
| 2025-10-16 18:11:43.757967 | Idx:  120/400    | loss: 0.024 | grad_norm: 0.014 | acc: 99.80% |
| 2025-10-16 18:11:52.088402 | Idx:  160/400    | loss: 0.024 | grad_norm: 0.051 | acc: 99.26% |
| 2025-10-16 18:12:01.779974 | Idx:  200/400    | loss: 0.023 | grad_norm: 0.037 | acc: 99.46% |
| 2025-10-16 18:12:11.378088 | Idx:  240/400    | loss: 0.024 | grad_norm: 0.131 | acc: 99.47% |
| 2025-10-16 18:12:20.610715 | Idx:  280/400    | loss: 0.029 | grad_norm: 0.044 | acc: 99.60% |
| 2025-10-16 18:12:29.988562 | Idx:  320/400    | loss: 0.028 | grad_norm: 0.135 | acc: 97.69% |
| 2025-10-16 18:12:38.584857 | Idx:  360/400    | loss: 0.027 | grad_norm: 0.026 | acc: 99.46% |
| 2025-10-16 18:12:46.929599 | Idx:  400/400    | loss: 0.026 | grad_norm: 0.084 | acc: 99.39% |
Train Loss: 0.0264 Acc: 99.54%
Acc: 99.66%


| val_epoch_acc: 99.66% | epoch: 04 | avg_train_loss: 0.0186 | avg_train_acc: 99.6771 | 


Epoch 6/8
----------
| 2025-10-16 18:13:20.371637 | Idx:   40/400    | loss: 0.032 | grad_norm: 0.036 | acc: 99.96% |
| 2025-10-16 18:13:28.507193 | Idx:   80/400    | loss: 0.035 | grad_norm: 0.336 | acc: 97.55% |
| 2025-10-16 18:13:38.419006 | Idx:  120/400    | loss: 0.033 | grad_norm: 0.001 | acc: 99.99% |
| 2025-10-16 18:13:46.670600 | Idx:  160/400    | loss: 0.031 | grad_norm: 0.005 | acc: 99.77% |
| 2025-10-16 18:13:55.054044 | Idx:  200/400    | loss: 0.029 | grad_norm: 0.007 | acc: 99.93% |
| 2025-10-16 18:14:04.647191 | Idx:  240/400    | loss: 0.029 | grad_norm: 0.016 | acc: 99.78% |
| 2025-10-16 18:14:13.430069 | Idx:  280/400    | loss: 0.028 | grad_norm: 0.025 | acc: 99.91% |
| 2025-10-16 18:14:22.590913 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.026 | acc: 99.89% |
| 2025-10-16 18:14:30.618585 | Idx:  360/400    | loss: 0.027 | grad_norm: 0.053 | acc: 98.33% |
| 2025-10-16 18:14:38.535441 | Idx:  400/400    | loss: 0.030 | grad_norm: 0.039 | acc: 99.83% |
Train Loss: 0.0304 Acc: 99.40%
Acc: 99.28%


| val_epoch_acc: 99.28% | epoch: 05 | avg_train_loss: 0.0722 | avg_train_acc: 98.3332 | 


Epoch 7/8
----------
| 2025-10-16 18:15:10.669868 | Idx:   40/400    | loss: 0.058 | grad_norm: 0.093 | acc: 99.24% |
| 2025-10-16 18:15:18.626229 | Idx:   80/400    | loss: 0.044 | grad_norm: 0.121 | acc: 98.38% |
| 2025-10-16 18:15:27.732024 | Idx:  120/400    | loss: 0.035 | grad_norm: 0.022 | acc: 99.84% |
| 2025-10-16 18:15:35.840732 | Idx:  160/400    | loss: 0.030 | grad_norm: 0.010 | acc: 99.87% |
| 2025-10-16 18:15:43.843421 | Idx:  200/400    | loss: 0.029 | grad_norm: 0.004 | acc: 99.98% |
| 2025-10-16 18:15:53.087018 | Idx:  240/400    | loss: 0.028 | grad_norm: 1.607 | acc: 99.97% |
| 2025-10-16 18:16:01.099755 | Idx:  280/400    | loss: 0.027 | grad_norm: 0.067 | acc: 99.15% |
| 2025-10-16 18:16:10.865491 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.007 | acc: 99.97% |
| 2025-10-16 18:16:18.707046 | Idx:  360/400    | loss: 0.030 | grad_norm: 0.237 | acc: 99.37% |
| 2025-10-16 18:16:26.478428 | Idx:  400/400    | loss: 0.034 | grad_norm: 0.078 | acc: 99.64% |
Train Loss: 0.0337 Acc: 99.54%
Acc: 99.51%


| val_epoch_acc: 99.51% | epoch: 06 | avg_train_loss: 0.0654 | avg_train_acc: 98.9700 | 


Epoch 8/8
----------
| 2025-10-16 18:16:57.916213 | Idx:   40/400    | loss: 0.026 | grad_norm: 0.054 | acc: 99.64% |
| 2025-10-16 18:17:06.025589 | Idx:   80/400    | loss: 0.038 | grad_norm: 0.027 | acc: 99.72% |
| 2025-10-16 18:17:15.432830 | Idx:  120/400    | loss: 0.038 | grad_norm: 0.032 | acc: 97.73% |
| 2025-10-16 18:17:23.440251 | Idx:  160/400    | loss: 0.036 | grad_norm: 0.154 | acc: 98.24% |
| 2025-10-16 18:17:31.402730 | Idx:  200/400    | loss: 0.033 | grad_norm: 0.029 | acc: 98.85% |
| 2025-10-16 18:17:40.398784 | Idx:  240/400    | loss: 0.032 | grad_norm: 0.138 | acc: 98.84% |
| 2025-10-16 18:17:48.158628 | Idx:  280/400    | loss: 0.032 | grad_norm: 0.019 | acc: 99.58% |
| 2025-10-16 18:17:57.258684 | Idx:  320/400    | loss: 0.031 | grad_norm: 0.013 | acc: 99.68% |
| 2025-10-16 18:18:05.054271 | Idx:  360/400    | loss: 0.030 | grad_norm: 0.014 | acc: 99.70% |
| 2025-10-16 18:18:12.857481 | Idx:  400/400    | loss: 0.029 | grad_norm: 0.026 | acc: 99.54% |
Train Loss: 0.0292 Acc: 99.45%
Acc: 99.68%


| val_epoch_acc: 99.68% | epoch: 07 | avg_train_loss: 0.0227 | avg_train_acc: 99.5538 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_180339-ll05b374[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_180339-ll05b374\logs[0m
rule: B3/S23 \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_18-18-36_tiny_2_layer_seq_cnn__200-200-B3_S23


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
����R2Conv: 1-1                                      --                        22
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        2
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        44
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        2
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        24
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 18:18:53.386729 | Idx:   40/400    | loss: 1.020 | grad_norm: 0.621 | acc: 84.38% |
| 2025-10-16 18:18:56.380099 | Idx:   80/400    | loss: 0.731 | grad_norm: 0.176 | acc: 88.80% |
| 2025-10-16 18:19:00.770399 | Idx:  120/400    | loss: 0.612 | grad_norm: 0.540 | acc: 89.35% |
| 2025-10-16 18:19:03.735316 | Idx:  160/400    | loss: 0.550 | grad_norm: 0.460 | acc: 89.04% |
| 2025-10-16 18:19:06.809117 | Idx:  200/400    | loss: 0.503 | grad_norm: 0.120 | acc: 93.25% |
| 2025-10-16 18:19:11.398725 | Idx:  240/400    | loss: 0.468 | grad_norm: 0.613 | acc: 90.08% |
| 2025-10-16 18:19:14.579637 | Idx:  280/400    | loss: 0.437 | grad_norm: 0.197 | acc: 92.93% |
| 2025-10-16 18:19:18.897301 | Idx:  320/400    | loss: 0.416 | grad_norm: 0.251 | acc: 94.05% |
| 2025-10-16 18:19:21.888018 | Idx:  360/400    | loss: 0.394 | grad_norm: 0.161 | acc: 95.53% |
| 2025-10-16 18:19:24.876564 | Idx:  400/400    | loss: 0.374 | grad_norm: 0.155 | acc: 95.45% |
Train Loss: 0.3740 Acc: 91.16%
Acc: 94.59%


| val_epoch_acc: 94.59% | epoch: 00 | avg_train_loss: 0.1947 | avg_train_acc: 94.7974 | 


Epoch 2/8
----------
| 2025-10-16 18:19:51.891416 | Idx:   40/400    | loss: 0.220 | grad_norm: 0.796 | acc: 94.42% |
| 2025-10-16 18:19:54.821343 | Idx:   80/400    | loss: 0.191 | grad_norm: 0.417 | acc: 95.98% |
| 2025-10-16 18:19:59.037846 | Idx:  120/400    | loss: 0.176 | grad_norm: 0.898 | acc: 94.24% |
| 2025-10-16 18:20:01.991335 | Idx:  160/400    | loss: 0.180 | grad_norm: 0.217 | acc: 96.28% |
| 2025-10-16 18:20:04.947360 | Idx:  200/400    | loss: 0.177 | grad_norm: 0.218 | acc: 96.82% |
| 2025-10-16 18:20:09.250520 | Idx:  240/400    | loss: 0.172 | grad_norm: 0.363 | acc: 96.75% |
| 2025-10-16 18:20:12.206821 | Idx:  280/400    | loss: 0.167 | grad_norm: 0.114 | acc: 97.21% |
| 2025-10-16 18:20:16.778500 | Idx:  320/400    | loss: 0.166 | grad_norm: 0.407 | acc: 96.30% |
| 2025-10-16 18:20:19.719186 | Idx:  360/400    | loss: 0.162 | grad_norm: 0.379 | acc: 97.43% |
| 2025-10-16 18:20:22.671468 | Idx:  400/400    | loss: 0.160 | grad_norm: 0.108 | acc: 97.44% |
Train Loss: 0.1602 Acc: 95.90%
Acc: 96.31%


| val_epoch_acc: 96.31% | epoch: 01 | avg_train_loss: 0.1404 | avg_train_acc: 96.3537 | 


Epoch 3/8
----------
| 2025-10-16 18:20:49.826017 | Idx:   40/400    | loss: 0.151 | grad_norm: 0.240 | acc: 97.26% |
| 2025-10-16 18:20:52.776999 | Idx:   80/400    | loss: 0.179 | grad_norm: 0.275 | acc: 96.51% |
| 2025-10-16 18:20:57.114640 | Idx:  120/400    | loss: 0.173 | grad_norm: 0.268 | acc: 96.26% |
| 2025-10-16 18:21:00.105407 | Idx:  160/400    | loss: 0.186 | grad_norm: 16.475 | acc: 83.52% |
| 2025-10-16 18:21:03.084067 | Idx:  200/400    | loss: 0.176 | grad_norm: 0.806 | acc: 94.57% |
| 2025-10-16 18:21:07.357411 | Idx:  240/400    | loss: 0.173 | grad_norm: 0.803 | acc: 97.41% |
| 2025-10-16 18:21:10.311390 | Idx:  280/400    | loss: 0.166 | grad_norm: 0.120 | acc: 97.48% |
| 2025-10-16 18:21:14.748792 | Idx:  320/400    | loss: 0.167 | grad_norm: 0.447 | acc: 96.28% |
| 2025-10-16 18:21:17.727933 | Idx:  360/400    | loss: 0.162 | grad_norm: 0.290 | acc: 96.75% |
| 2025-10-16 18:21:20.767678 | Idx:  400/400    | loss: 0.161 | grad_norm: 0.925 | acc: 96.18% |
Train Loss: 0.1607 Acc: 96.30%
Acc: 97.77%


| val_epoch_acc: 97.77% | epoch: 02 | avg_train_loss: 0.1503 | avg_train_acc: 96.3666 | 


Epoch 4/8
----------
| 2025-10-16 18:21:48.295608 | Idx:   40/400    | loss: 0.113 | grad_norm: 0.396 | acc: 96.72% |
| 2025-10-16 18:21:51.734550 | Idx:   80/400    | loss: 0.136 | grad_norm: 0.489 | acc: 97.47% |
| 2025-10-16 18:21:56.538306 | Idx:  120/400    | loss: 0.155 | grad_norm: 0.412 | acc: 96.56% |
| 2025-10-16 18:22:00.042358 | Idx:  160/400    | loss: 0.150 | grad_norm: 0.414 | acc: 97.05% |
| 2025-10-16 18:22:03.484992 | Idx:  200/400    | loss: 0.148 | grad_norm: 0.132 | acc: 97.43% |
| 2025-10-16 18:22:08.239546 | Idx:  240/400    | loss: 0.145 | grad_norm: 0.158 | acc: 97.48% |
| 2025-10-16 18:22:11.646920 | Idx:  280/400    | loss: 0.141 | grad_norm: 0.212 | acc: 97.45% |
| 2025-10-16 18:22:16.400790 | Idx:  320/400    | loss: 0.137 | grad_norm: 0.541 | acc: 97.57% |
| 2025-10-16 18:22:19.815371 | Idx:  360/400    | loss: 0.144 | grad_norm: 0.217 | acc: 96.67% |
| 2025-10-16 18:22:23.235680 | Idx:  400/400    | loss: 0.143 | grad_norm: 0.484 | acc: 95.84% |
Train Loss: 0.1435 Acc: 96.53%
Acc: 97.50%


| val_epoch_acc: 97.50% | epoch: 03 | avg_train_loss: 0.1384 | avg_train_acc: 96.3149 | 


Epoch 5/8
----------
| 2025-10-16 18:22:51.592731 | Idx:   40/400    | loss: 0.122 | grad_norm: 0.156 | acc: 97.61% |
| 2025-10-16 18:22:55.195912 | Idx:   80/400    | loss: 0.137 | grad_norm: 0.284 | acc: 97.36% |
| 2025-10-16 18:23:00.057547 | Idx:  120/400    | loss: 0.135 | grad_norm: 0.061 | acc: 97.14% |
| 2025-10-16 18:23:03.493583 | Idx:  160/400    | loss: 0.141 | grad_norm: 0.965 | acc: 95.60% |
| 2025-10-16 18:23:06.925884 | Idx:  200/400    | loss: 0.136 | grad_norm: 0.470 | acc: 96.38% |
| 2025-10-16 18:23:11.641583 | Idx:  240/400    | loss: 0.133 | grad_norm: 0.551 | acc: 95.80% |
| 2025-10-16 18:23:15.022713 | Idx:  280/400    | loss: 0.140 | grad_norm: 0.699 | acc: 95.35% |
| 2025-10-16 18:23:19.758532 | Idx:  320/400    | loss: 0.142 | grad_norm: 4.693 | acc: 92.40% |
| 2025-10-16 18:23:23.165215 | Idx:  360/400    | loss: 0.143 | grad_norm: 0.113 | acc: 97.68% |
| 2025-10-16 18:23:26.617230 | Idx:  400/400    | loss: 0.149 | grad_norm: 0.970 | acc: 95.15% |
Train Loss: 0.1490 Acc: 96.56%
Acc: 96.64%


| val_epoch_acc: 96.64% | epoch: 04 | avg_train_loss: 0.2070 | avg_train_acc: 95.9823 | 


Epoch 6/8
----------
| 2025-10-16 18:23:56.124504 | Idx:   40/400    | loss: 0.142 | grad_norm: 0.226 | acc: 97.22% |
| 2025-10-16 18:24:00.019915 | Idx:   80/400    | loss: 0.162 | grad_norm: 0.600 | acc: 94.79% |
| 2025-10-16 18:24:05.254961 | Idx:  120/400    | loss: 0.159 | grad_norm: 0.523 | acc: 96.08% |
| 2025-10-16 18:24:09.146307 | Idx:  160/400    | loss: 0.159 | grad_norm: 0.629 | acc: 95.61% |
| 2025-10-16 18:24:13.026448 | Idx:  200/400    | loss: 0.167 | grad_norm: 0.587 | acc: 95.80% |
| 2025-10-16 18:24:18.294155 | Idx:  240/400    | loss: 0.161 | grad_norm: 0.300 | acc: 96.67% |
| 2025-10-16 18:24:22.213714 | Idx:  280/400    | loss: 0.156 | grad_norm: 0.201 | acc: 97.58% |
| 2025-10-16 18:24:27.223329 | Idx:  320/400    | loss: 0.153 | grad_norm: 0.632 | acc: 96.13% |
| 2025-10-16 18:24:30.274659 | Idx:  360/400    | loss: 0.155 | grad_norm: 0.224 | acc: 97.22% |
| 2025-10-16 18:24:33.323687 | Idx:  400/400    | loss: 0.157 | grad_norm: 0.450 | acc: 96.39% |
Train Loss: 0.1567 Acc: 96.45%
Acc: 96.28%


| val_epoch_acc: 96.28% | epoch: 05 | avg_train_loss: 0.1824 | avg_train_acc: 96.0017 | 


Epoch 7/8
----------
| 2025-10-16 18:25:01.443540 | Idx:   40/400    | loss: 0.144 | grad_norm: 4.448 | acc: 91.47% |
| 2025-10-16 18:25:04.507881 | Idx:   80/400    | loss: 0.137 | grad_norm: 0.255 | acc: 96.42% |
| 2025-10-16 18:25:09.137912 | Idx:  120/400    | loss: 0.144 | grad_norm: 0.059 | acc: 97.68% |
| 2025-10-16 18:25:12.188684 | Idx:  160/400    | loss: 0.138 | grad_norm: 0.443 | acc: 96.44% |
| 2025-10-16 18:25:15.249700 | Idx:  200/400    | loss: 0.139 | grad_norm: 0.200 | acc: 97.24% |
| 2025-10-16 18:25:19.591611 | Idx:  240/400    | loss: 0.141 | grad_norm: 0.326 | acc: 96.76% |
| 2025-10-16 18:25:22.960790 | Idx:  280/400    | loss: 0.137 | grad_norm: 0.231 | acc: 97.23% |
| 2025-10-16 18:25:27.444127 | Idx:  320/400    | loss: 0.134 | grad_norm: 1.037 | acc: 97.51% |
| 2025-10-16 18:25:30.732607 | Idx:  360/400    | loss: 0.133 | grad_norm: 0.061 | acc: 97.83% |
| 2025-10-16 18:25:33.975907 | Idx:  400/400    | loss: 0.135 | grad_norm: 0.342 | acc: 97.36% |
Train Loss: 0.1354 Acc: 96.73%
Acc: 96.88%


| val_epoch_acc: 96.88% | epoch: 06 | avg_train_loss: 0.1445 | avg_train_acc: 96.7049 | 


Epoch 8/8
----------
| 2025-10-16 18:26:02.896917 | Idx:   40/400    | loss: 0.152 | grad_norm: 0.463 | acc: 97.95% |
| 2025-10-16 18:26:06.827716 | Idx:   80/400    | loss: 0.208 | grad_norm: 0.416 | acc: 97.98% |
| 2025-10-16 18:26:12.178036 | Idx:  120/400    | loss: 0.194 | grad_norm: 0.180 | acc: 97.34% |
| 2025-10-16 18:26:16.062138 | Idx:  160/400    | loss: 0.181 | grad_norm: 0.509 | acc: 97.12% |
| 2025-10-16 18:26:19.979203 | Idx:  200/400    | loss: 0.169 | grad_norm: 1.646 | acc: 95.41% |
| 2025-10-16 18:26:25.354868 | Idx:  240/400    | loss: 0.164 | grad_norm: 0.800 | acc: 95.80% |
| 2025-10-16 18:26:29.156101 | Idx:  280/400    | loss: 0.171 | grad_norm: 0.911 | acc: 97.78% |
| 2025-10-16 18:26:34.076119 | Idx:  320/400    | loss: 0.169 | grad_norm: 0.438 | acc: 96.78% |
| 2025-10-16 18:26:37.387722 | Idx:  360/400    | loss: 0.166 | grad_norm: 0.531 | acc: 97.54% |
| 2025-10-16 18:26:40.685583 | Idx:  400/400    | loss: 0.161 | grad_norm: 0.516 | acc: 95.74% |
Train Loss: 0.1614 Acc: 96.45%
Acc: 97.80%


| val_epoch_acc: 97.80% | epoch: 07 | avg_train_loss: 0.1172 | avg_train_acc: 96.8428 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_181836-6jw3zfc8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_181836-6jw3zfc8\logs[0m
rule: B36/S23 \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_18-27-04_tiny_2_layer_seq_cnn__200-200-B36_S23


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
����R2Conv: 1-1                                      --                        22
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        2
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        44
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        2
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        24
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 18:27:20.980983 | Idx:   40/400    | loss: 0.711 | grad_norm: 0.326 | acc: 85.45% |
| 2025-10-16 18:27:24.073724 | Idx:   80/400    | loss: 0.611 | grad_norm: 0.294 | acc: 82.38% |
| 2025-10-16 18:27:28.665086 | Idx:  120/400    | loss: 0.575 | grad_norm: 0.554 | acc: 84.65% |
| 2025-10-16 18:27:31.784070 | Idx:  160/400    | loss: 0.548 | grad_norm: 0.279 | acc: 87.65% |
| 2025-10-16 18:27:34.896957 | Idx:  200/400    | loss: 0.532 | grad_norm: 0.404 | acc: 86.83% |
| 2025-10-16 18:27:39.536836 | Idx:  240/400    | loss: 0.514 | grad_norm: 0.086 | acc: 89.02% |
| 2025-10-16 18:27:42.602497 | Idx:  280/400    | loss: 0.503 | grad_norm: 0.252 | acc: 90.25% |
| 2025-10-16 18:27:47.015431 | Idx:  320/400    | loss: 0.490 | grad_norm: 0.549 | acc: 87.07% |
| 2025-10-16 18:27:50.073805 | Idx:  360/400    | loss: 0.481 | grad_norm: 0.475 | acc: 89.91% |
| 2025-10-16 18:27:53.281225 | Idx:  400/400    | loss: 0.474 | grad_norm: 0.278 | acc: 88.91% |
Train Loss: 0.4743 Acc: 86.53%
Acc: 87.51%


| val_epoch_acc: 87.51% | epoch: 00 | avg_train_loss: 0.4006 | avg_train_acc: 88.7583 | 


Epoch 2/8
----------
| 2025-10-16 18:28:22.322989 | Idx:   40/400    | loss: 0.350 | grad_norm: 0.755 | acc: 88.29% |
| 2025-10-16 18:28:26.091391 | Idx:   80/400    | loss: 0.339 | grad_norm: 0.170 | acc: 92.61% |
| 2025-10-16 18:28:31.022676 | Idx:  120/400    | loss: 0.336 | grad_norm: 0.655 | acc: 90.11% |
| 2025-10-16 18:28:34.236543 | Idx:  160/400    | loss: 0.336 | grad_norm: 0.365 | acc: 93.34% |
| 2025-10-16 18:28:37.482219 | Idx:  200/400    | loss: 0.330 | grad_norm: 0.646 | acc: 90.43% |
| 2025-10-16 18:28:42.100086 | Idx:  240/400    | loss: 0.319 | grad_norm: 0.119 | acc: 92.11% |
| 2025-10-16 18:28:45.325095 | Idx:  280/400    | loss: 0.328 | grad_norm: 0.946 | acc: 87.57% |
| 2025-10-16 18:28:50.298863 | Idx:  320/400    | loss: 0.322 | grad_norm: 0.875 | acc: 89.95% |
| 2025-10-16 18:28:53.519105 | Idx:  360/400    | loss: 0.314 | grad_norm: 0.278 | acc: 92.15% |
| 2025-10-16 18:28:56.827743 | Idx:  400/400    | loss: 0.310 | grad_norm: 0.362 | acc: 94.12% |
Train Loss: 0.3101 Acc: 91.28%
Acc: 92.74%


| val_epoch_acc: 92.74% | epoch: 01 | avg_train_loss: 0.2889 | avg_train_acc: 91.8416 | 


Epoch 3/8
----------
| 2025-10-16 18:29:25.012987 | Idx:   40/400    | loss: 0.247 | grad_norm: 0.079 | acc: 93.05% |
| 2025-10-16 18:29:28.523361 | Idx:   80/400    | loss: 0.246 | grad_norm: 0.287 | acc: 92.80% |
| 2025-10-16 18:29:33.139660 | Idx:  120/400    | loss: 0.258 | grad_norm: 0.047 | acc: 93.18% |
| 2025-10-16 18:29:36.500123 | Idx:  160/400    | loss: 0.272 | grad_norm: 0.385 | acc: 91.60% |
| 2025-10-16 18:29:39.781097 | Idx:  200/400    | loss: 0.270 | grad_norm: 0.254 | acc: 92.18% |
| 2025-10-16 18:29:44.666405 | Idx:  240/400    | loss: 0.269 | grad_norm: 5.115 | acc: 85.65% |
| 2025-10-16 18:29:47.918678 | Idx:  280/400    | loss: 0.267 | grad_norm: 0.136 | acc: 93.50% |
| 2025-10-16 18:29:53.185716 | Idx:  320/400    | loss: 0.265 | grad_norm: 1.250 | acc: 91.52% |
| 2025-10-16 18:29:56.518270 | Idx:  360/400    | loss: 0.264 | grad_norm: 0.214 | acc: 93.21% |
| 2025-10-16 18:29:59.775631 | Idx:  400/400    | loss: 0.262 | grad_norm: 0.193 | acc: 92.84% |
Train Loss: 0.2617 Acc: 92.17%
Acc: 93.41%


| val_epoch_acc: 93.41% | epoch: 02 | avg_train_loss: 0.2407 | avg_train_acc: 92.5309 | 


Epoch 4/8
----------
| 2025-10-16 18:30:27.616136 | Idx:   40/400    | loss: 0.239 | grad_norm: 1.563 | acc: 88.61% |
| 2025-10-16 18:30:30.876771 | Idx:   80/400    | loss: 0.241 | grad_norm: 0.276 | acc: 92.30% |
| 2025-10-16 18:30:35.509206 | Idx:  120/400    | loss: 0.243 | grad_norm: 0.318 | acc: 93.97% |
| 2025-10-16 18:30:38.771488 | Idx:  160/400    | loss: 0.243 | grad_norm: 0.073 | acc: 93.02% |
| 2025-10-16 18:30:42.049127 | Idx:  200/400    | loss: 0.249 | grad_norm: 0.092 | acc: 93.08% |
| 2025-10-16 18:30:46.745014 | Idx:  240/400    | loss: 0.247 | grad_norm: 0.047 | acc: 93.79% |
| 2025-10-16 18:30:50.043126 | Idx:  280/400    | loss: 0.253 | grad_norm: 0.185 | acc: 92.64% |
| 2025-10-16 18:30:55.197945 | Idx:  320/400    | loss: 0.250 | grad_norm: 0.308 | acc: 91.67% |
| 2025-10-16 18:30:59.022503 | Idx:  360/400    | loss: 0.249 | grad_norm: 0.126 | acc: 93.98% |
| 2025-10-16 18:31:02.830346 | Idx:  400/400    | loss: 0.262 | grad_norm: 2.163 | acc: 90.24% |
Train Loss: 0.2623 Acc: 92.47%
Acc: 92.08%


| val_epoch_acc: 92.08% | epoch: 03 | avg_train_loss: 0.4180 | avg_train_acc: 91.9845 | 


Epoch 5/8
----------
| 2025-10-16 18:31:31.835350 | Idx:   40/400    | loss: 0.238 | grad_norm: 0.314 | acc: 91.55% |
| 2025-10-16 18:31:35.120774 | Idx:   80/400    | loss: 0.280 | grad_norm: 0.283 | acc: 91.54% |
| 2025-10-16 18:31:39.848692 | Idx:  120/400    | loss: 0.267 | grad_norm: 0.146 | acc: 93.44% |
| 2025-10-16 18:31:43.148012 | Idx:  160/400    | loss: 0.272 | grad_norm: 0.395 | acc: 91.95% |
| 2025-10-16 18:31:46.459064 | Idx:  200/400    | loss: 0.269 | grad_norm: 0.318 | acc: 92.52% |
| 2025-10-16 18:31:51.232768 | Idx:  240/400    | loss: 0.264 | grad_norm: 0.397 | acc: 91.75% |
| 2025-10-16 18:31:54.528731 | Idx:  280/400    | loss: 0.260 | grad_norm: 0.120 | acc: 94.05% |
| 2025-10-16 18:31:59.154957 | Idx:  320/400    | loss: 0.262 | grad_norm: 0.056 | acc: 93.85% |
| 2025-10-16 18:32:02.436942 | Idx:  360/400    | loss: 0.265 | grad_norm: 0.294 | acc: 94.32% |
| 2025-10-16 18:32:05.726736 | Idx:  400/400    | loss: 0.262 | grad_norm: 0.073 | acc: 92.79% |
Train Loss: 0.2618 Acc: 92.60%
Acc: 93.10%


| val_epoch_acc: 93.10% | epoch: 04 | avg_train_loss: 0.2191 | avg_train_acc: 93.1405 | 


Epoch 6/8
----------
| 2025-10-16 18:32:33.525967 | Idx:   40/400    | loss: 0.223 | grad_norm: 0.340 | acc: 92.64% |
| 2025-10-16 18:32:36.819898 | Idx:   80/400    | loss: 0.258 | grad_norm: 0.463 | acc: 93.60% |
| 2025-10-16 18:32:41.471683 | Idx:  120/400    | loss: 0.250 | grad_norm: 0.287 | acc: 94.31% |
| 2025-10-16 18:32:44.738751 | Idx:  160/400    | loss: 0.253 | grad_norm: 0.256 | acc: 91.41% |
| 2025-10-16 18:32:48.034499 | Idx:  200/400    | loss: 0.256 | grad_norm: 0.073 | acc: 91.92% |
| 2025-10-16 18:32:52.676359 | Idx:  240/400    | loss: 0.254 | grad_norm: 0.132 | acc: 92.54% |
| 2025-10-16 18:32:56.016917 | Idx:  280/400    | loss: 0.265 | grad_norm: 0.499 | acc: 90.91% |
| 2025-10-16 18:33:00.732117 | Idx:  320/400    | loss: 0.261 | grad_norm: 0.111 | acc: 94.32% |
| 2025-10-16 18:33:04.031383 | Idx:  360/400    | loss: 0.258 | grad_norm: 0.196 | acc: 92.89% |
| 2025-10-16 18:33:07.390236 | Idx:  400/400    | loss: 0.269 | grad_norm: 0.360 | acc: 91.45% |
Train Loss: 0.2690 Acc: 92.41%
Acc: 93.06%


| val_epoch_acc: 93.06% | epoch: 05 | avg_train_loss: 0.3958 | avg_train_acc: 91.6609 | 


Epoch 7/8
----------
| 2025-10-16 18:33:36.854240 | Idx:   40/400    | loss: 0.270 | grad_norm: 0.303 | acc: 92.29% |
| 2025-10-16 18:33:40.652981 | Idx:   80/400    | loss: 0.251 | grad_norm: 0.558 | acc: 91.24% |
| 2025-10-16 18:33:45.884581 | Idx:  120/400    | loss: 0.273 | grad_norm: 0.090 | acc: 92.97% |
| 2025-10-16 18:33:49.723079 | Idx:  160/400    | loss: 0.277 | grad_norm: 11.673 | acc: 80.96% |
| 2025-10-16 18:33:53.550570 | Idx:  200/400    | loss: 0.287 | grad_norm: 0.297 | acc: 93.77% |
| 2025-10-16 18:33:58.565499 | Idx:  240/400    | loss: 0.290 | grad_norm: 0.605 | acc: 92.32% |
| 2025-10-16 18:34:01.855682 | Idx:  280/400    | loss: 0.292 | grad_norm: 0.842 | acc: 91.18% |
| 2025-10-16 18:34:06.632844 | Idx:  320/400    | loss: 0.290 | grad_norm: 0.429 | acc: 91.61% |
| 2025-10-16 18:34:09.940063 | Idx:  360/400    | loss: 0.288 | grad_norm: 0.234 | acc: 94.03% |
| 2025-10-16 18:34:13.222350 | Idx:  400/400    | loss: 0.285 | grad_norm: 0.422 | acc: 94.42% |
Train Loss: 0.2855 Acc: 92.13%
Acc: 93.57%


| val_epoch_acc: 93.57% | epoch: 06 | avg_train_loss: 0.2621 | avg_train_acc: 92.6844 | 


Epoch 8/8
----------
| 2025-10-16 18:34:41.365148 | Idx:   40/400    | loss: 0.287 | grad_norm: 0.236 | acc: 92.57% |
| 2025-10-16 18:34:44.657029 | Idx:   80/400    | loss: 0.276 | grad_norm: 1.061 | acc: 92.82% |
| 2025-10-16 18:34:49.313425 | Idx:  120/400    | loss: 0.286 | grad_norm: 0.363 | acc: 91.45% |
| 2025-10-16 18:34:52.590652 | Idx:  160/400    | loss: 0.285 | grad_norm: 0.106 | acc: 93.61% |
| 2025-10-16 18:34:55.860759 | Idx:  200/400    | loss: 0.286 | grad_norm: 1.181 | acc: 95.56% |
| 2025-10-16 18:35:00.488892 | Idx:  240/400    | loss: 0.278 | grad_norm: 0.131 | acc: 94.24% |
| 2025-10-16 18:35:03.777788 | Idx:  280/400    | loss: 0.279 | grad_norm: 0.133 | acc: 93.68% |
| 2025-10-16 18:35:08.411781 | Idx:  320/400    | loss: 0.278 | grad_norm: 2.051 | acc: 86.66% |
| 2025-10-16 18:35:11.693488 | Idx:  360/400    | loss: 0.274 | grad_norm: 0.166 | acc: 93.60% |
| 2025-10-16 18:35:14.976097 | Idx:  400/400    | loss: 0.270 | grad_norm: 0.961 | acc: 91.04% |
Train Loss: 0.2696 Acc: 92.44%
Acc: 94.03%


| val_epoch_acc: 94.03% | epoch: 07 | avg_train_loss: 0.2402 | avg_train_acc: 92.6809 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_182704-c596z4sf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_182704-c596z4sf\logs[0m
rule: B3678/S34678 \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_18-35-39_tiny_2_layer_seq_cnn__200-200-B3678_S34678


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
����R2Conv: 1-1                                      --                        22
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        2
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        44
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        2
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        24
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 18:35:56.863185 | Idx:   40/400    | loss: 0.860 | grad_norm: 0.283 | acc: 91.28% |
| 2025-10-16 18:36:00.684858 | Idx:   80/400    | loss: 0.632 | grad_norm: 0.120 | acc: 92.42% |
| 2025-10-16 18:36:05.882881 | Idx:  120/400    | loss: 0.557 | grad_norm: 0.123 | acc: 94.89% |
| 2025-10-16 18:36:09.711284 | Idx:  160/400    | loss: 0.519 | grad_norm: 0.105 | acc: 91.24% |
| 2025-10-16 18:36:13.518556 | Idx:  200/400    | loss: 0.488 | grad_norm: 0.152 | acc: 89.65% |
| 2025-10-16 18:36:18.292568 | Idx:  240/400    | loss: 0.469 | grad_norm: 0.587 | acc: 94.09% |
| 2025-10-16 18:36:21.610059 | Idx:  280/400    | loss: 0.456 | grad_norm: 1.883 | acc: 79.98% |
| 2025-10-16 18:36:26.241257 | Idx:  320/400    | loss: 0.443 | grad_norm: 0.535 | acc: 94.95% |
| 2025-10-16 18:36:29.312770 | Idx:  360/400    | loss: 0.432 | grad_norm: 0.145 | acc: 94.08% |
| 2025-10-16 18:36:32.367362 | Idx:  400/400    | loss: 0.425 | grad_norm: 0.304 | acc: 84.65% |
Train Loss: 0.4254 Acc: 89.30%
Acc: 91.30%


| val_epoch_acc: 91.30% | epoch: 00 | avg_train_loss: 0.3595 | avg_train_acc: 90.5411 | 


Epoch 2/8
----------
| 2025-10-16 18:37:00.289813 | Idx:   40/400    | loss: 0.352 | grad_norm: 1.274 | acc: 85.96% |
| 2025-10-16 18:37:03.671985 | Idx:   80/400    | loss: 0.352 | grad_norm: 0.257 | acc: 88.03% |
| 2025-10-16 18:37:08.461188 | Idx:  120/400    | loss: 0.342 | grad_norm: 1.247 | acc: 88.27% |
| 2025-10-16 18:37:11.763892 | Idx:  160/400    | loss: 0.343 | grad_norm: 0.394 | acc: 93.50% |
| 2025-10-16 18:37:15.100186 | Idx:  200/400    | loss: 0.350 | grad_norm: 0.110 | acc: 88.88% |
| 2025-10-16 18:37:19.813166 | Idx:  240/400    | loss: 0.354 | grad_norm: 0.846 | acc: 92.25% |
| 2025-10-16 18:37:23.177102 | Idx:  280/400    | loss: 0.350 | grad_norm: 0.150 | acc: 91.09% |
| 2025-10-16 18:37:28.225090 | Idx:  320/400    | loss: 0.352 | grad_norm: 0.596 | acc: 90.15% |
| 2025-10-16 18:37:31.588682 | Idx:  360/400    | loss: 0.354 | grad_norm: 0.345 | acc: 92.22% |
| 2025-10-16 18:37:34.993016 | Idx:  400/400    | loss: 0.357 | grad_norm: 0.931 | acc: 90.41% |
Train Loss: 0.3566 Acc: 90.51%
Acc: 91.32%


| val_epoch_acc: 91.32% | epoch: 01 | avg_train_loss: 0.3812 | avg_train_acc: 89.9793 | 


Epoch 3/8
----------
| 2025-10-16 18:38:03.368488 | Idx:   40/400    | loss: 0.351 | grad_norm: 0.046 | acc: 90.80% |
| 2025-10-16 18:38:06.669589 | Idx:   80/400    | loss: 0.341 | grad_norm: 0.379 | acc: 90.00% |
| 2025-10-16 18:38:11.243873 | Idx:  120/400    | loss: 0.341 | grad_norm: 0.381 | acc: 93.39% |
| 2025-10-16 18:38:14.535540 | Idx:  160/400    | loss: 0.343 | grad_norm: 0.714 | acc: 91.25% |
| 2025-10-16 18:38:17.837326 | Idx:  200/400    | loss: 0.345 | grad_norm: 0.112 | acc: 93.69% |
| 2025-10-16 18:38:22.434085 | Idx:  240/400    | loss: 0.350 | grad_norm: 0.161 | acc: 90.39% |
| 2025-10-16 18:38:25.719354 | Idx:  280/400    | loss: 0.347 | grad_norm: 0.221 | acc: 90.38% |
| 2025-10-16 18:38:30.842681 | Idx:  320/400    | loss: 0.344 | grad_norm: 0.414 | acc: 94.83% |
| 2025-10-16 18:38:34.340605 | Idx:  360/400    | loss: 0.345 | grad_norm: 0.891 | acc: 86.78% |
| 2025-10-16 18:38:38.134027 | Idx:  400/400    | loss: 0.343 | grad_norm: 0.303 | acc: 89.21% |
Train Loss: 0.3429 Acc: 90.74%
Acc: 91.75%


| val_epoch_acc: 91.75% | epoch: 02 | avg_train_loss: 0.3259 | avg_train_acc: 91.1461 | 


Epoch 4/8
----------
| 2025-10-16 18:39:06.945792 | Idx:   40/400    | loss: 0.361 | grad_norm: 0.273 | acc: 89.90% |
| 2025-10-16 18:39:10.292818 | Idx:   80/400    | loss: 0.331 | grad_norm: 0.379 | acc: 93.34% |
| 2025-10-16 18:39:14.843175 | Idx:  120/400    | loss: 0.334 | grad_norm: 0.652 | acc: 93.48% |
| 2025-10-16 18:39:18.205064 | Idx:  160/400    | loss: 0.335 | grad_norm: 0.408 | acc: 88.75% |
| 2025-10-16 18:39:21.513105 | Idx:  200/400    | loss: 0.340 | grad_norm: 1.964 | acc: 80.82% |
| 2025-10-16 18:39:26.395532 | Idx:  240/400    | loss: 0.347 | grad_norm: 0.122 | acc: 89.49% |
| 2025-10-16 18:39:30.589896 | Idx:  280/400    | loss: 0.342 | grad_norm: 0.809 | acc: 90.36% |
| 2025-10-16 18:39:36.088631 | Idx:  320/400    | loss: 0.342 | grad_norm: 0.243 | acc: 88.70% |
| 2025-10-16 18:39:39.349191 | Idx:  360/400    | loss: 0.343 | grad_norm: 0.768 | acc: 87.80% |
| 2025-10-16 18:39:42.775292 | Idx:  400/400    | loss: 0.342 | grad_norm: 0.283 | acc: 96.50% |
Train Loss: 0.3420 Acc: 90.71%
Acc: 91.64%


| val_epoch_acc: 91.64% | epoch: 03 | avg_train_loss: 0.3303 | avg_train_acc: 90.8760 | 


Epoch 5/8
----------
| 2025-10-16 18:40:12.702250 | Idx:   40/400    | loss: 0.348 | grad_norm: 0.051 | acc: 87.67% |
| 2025-10-16 18:40:16.654572 | Idx:   80/400    | loss: 0.347 | grad_norm: 0.567 | acc: 90.18% |
| 2025-10-16 18:40:21.941620 | Idx:  120/400    | loss: 0.339 | grad_norm: 0.326 | acc: 93.45% |
| 2025-10-16 18:40:25.845647 | Idx:  160/400    | loss: 0.343 | grad_norm: 0.500 | acc: 93.56% |
| 2025-10-16 18:40:29.334107 | Idx:  200/400    | loss: 0.345 | grad_norm: 0.215 | acc: 92.95% |
| 2025-10-16 18:40:33.848027 | Idx:  240/400    | loss: 0.348 | grad_norm: 1.538 | acc: 84.86% |
| 2025-10-16 18:40:36.920347 | Idx:  280/400    | loss: 0.345 | grad_norm: 0.504 | acc: 87.16% |
| 2025-10-16 18:40:41.434795 | Idx:  320/400    | loss: 0.344 | grad_norm: 0.047 | acc: 90.98% |
| 2025-10-16 18:40:44.489390 | Idx:  360/400    | loss: 0.343 | grad_norm: 0.305 | acc: 95.61% |
| 2025-10-16 18:40:47.708524 | Idx:  400/400    | loss: 0.343 | grad_norm: 1.083 | acc: 90.05% |
Train Loss: 0.3428 Acc: 90.60%
Acc: 91.51%


| val_epoch_acc: 91.51% | epoch: 04 | avg_train_loss: 0.3398 | avg_train_acc: 90.6478 | 


Epoch 6/8
----------
| 2025-10-16 18:41:17.487952 | Idx:   40/400    | loss: 0.360 | grad_norm: 0.643 | acc: 89.92% |
| 2025-10-16 18:41:21.344091 | Idx:   80/400    | loss: 0.349 | grad_norm: 1.083 | acc: 91.11% |
| 2025-10-16 18:41:26.619965 | Idx:  120/400    | loss: 0.340 | grad_norm: 0.991 | acc: 87.84% |
| 2025-10-16 18:41:30.521941 | Idx:  160/400    | loss: 0.330 | grad_norm: 0.054 | acc: 87.17% |
| 2025-10-16 18:41:34.107701 | Idx:  200/400    | loss: 0.334 | grad_norm: 2.042 | acc: 86.23% |
| 2025-10-16 18:41:38.801534 | Idx:  240/400    | loss: 0.336 | grad_norm: 0.569 | acc: 93.47% |
| 2025-10-16 18:41:42.022699 | Idx:  280/400    | loss: 0.336 | grad_norm: 0.133 | acc: 91.13% |
| 2025-10-16 18:41:46.598833 | Idx:  320/400    | loss: 0.334 | grad_norm: 0.085 | acc: 92.70% |
| 2025-10-16 18:41:49.805939 | Idx:  360/400    | loss: 0.339 | grad_norm: 0.361 | acc: 94.14% |
| 2025-10-16 18:41:53.028169 | Idx:  400/400    | loss: 0.341 | grad_norm: 0.123 | acc: 92.63% |
Train Loss: 0.3414 Acc: 90.74%
Acc: 92.13%


| val_epoch_acc: 92.13% | epoch: 05 | avg_train_loss: 0.3563 | avg_train_acc: 90.3089 | 


Epoch 7/8
----------
| 2025-10-16 18:42:20.687112 | Idx:   40/400    | loss: 0.374 | grad_norm: 1.400 | acc: 89.38% |
| 2025-10-16 18:42:23.893387 | Idx:   80/400    | loss: 0.361 | grad_norm: 0.207 | acc: 90.27% |
| 2025-10-16 18:42:28.563831 | Idx:  120/400    | loss: 0.338 | grad_norm: 0.113 | acc: 91.26% |
| 2025-10-16 18:42:31.770509 | Idx:  160/400    | loss: 0.335 | grad_norm: 0.585 | acc: 85.47% |
| 2025-10-16 18:42:34.999170 | Idx:  200/400    | loss: 0.336 | grad_norm: 0.527 | acc: 87.11% |
| 2025-10-16 18:42:40.146091 | Idx:  240/400    | loss: 0.338 | grad_norm: 0.793 | acc: 90.96% |
| 2025-10-16 18:42:43.876866 | Idx:  280/400    | loss: 0.341 | grad_norm: 0.166 | acc: 92.43% |
| 2025-10-16 18:42:49.053211 | Idx:  320/400    | loss: 0.345 | grad_norm: 0.941 | acc: 91.87% |
| 2025-10-16 18:42:52.787199 | Idx:  360/400    | loss: 0.342 | grad_norm: 0.224 | acc: 93.54% |
| 2025-10-16 18:42:56.519038 | Idx:  400/400    | loss: 0.344 | grad_norm: 0.567 | acc: 91.86% |
Train Loss: 0.3442 Acc: 90.58%
Acc: 91.66%


| val_epoch_acc: 91.66% | epoch: 06 | avg_train_loss: 0.3657 | avg_train_acc: 89.9268 | 


Epoch 8/8
----------
| 2025-10-16 18:43:25.124197 | Idx:   40/400    | loss: 0.381 | grad_norm: 0.528 | acc: 90.30% |
| 2025-10-16 18:43:28.452015 | Idx:   80/400    | loss: 0.367 | grad_norm: 0.303 | acc: 93.89% |
| 2025-10-16 18:43:33.378877 | Idx:  120/400    | loss: 0.357 | grad_norm: 0.307 | acc: 93.81% |
| 2025-10-16 18:43:37.166407 | Idx:  160/400    | loss: 0.355 | grad_norm: 0.556 | acc: 89.29% |
| 2025-10-16 18:43:40.515215 | Idx:  200/400    | loss: 0.352 | grad_norm: 1.143 | acc: 87.21% |
| 2025-10-16 18:43:45.499372 | Idx:  240/400    | loss: 0.351 | grad_norm: 0.516 | acc: 95.64% |
| 2025-10-16 18:43:49.254867 | Idx:  280/400    | loss: 0.348 | grad_norm: 0.677 | acc: 93.16% |
| 2025-10-16 18:43:54.069163 | Idx:  320/400    | loss: 0.344 | grad_norm: 0.057 | acc: 92.03% |
| 2025-10-16 18:43:57.407928 | Idx:  360/400    | loss: 0.344 | grad_norm: 0.097 | acc: 90.38% |
| 2025-10-16 18:44:00.831450 | Idx:  400/400    | loss: 0.343 | grad_norm: 0.090 | acc: 90.81% |
Train Loss: 0.3433 Acc: 90.70%
Acc: 91.93%


| val_epoch_acc: 91.93% | epoch: 07 | avg_train_loss: 0.3041 | avg_train_acc: 91.8055 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_183539-qfhmdnyj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_183539-qfhmdnyj\logs[0m
rule: B35678/S5678 \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_18-44-26_tiny_2_layer_seq_cnn__200-200-B35678_S5678


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
����R2Conv: 1-1                                      --                        22
��    ����BlocksBasisExpansion: 2-1                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-1         --                        --
����InnerBatchNorm: 1-2                              --                        --
��    ����BatchNorm3d: 2-2                            --                        2
����ReLU: 1-3                                        --                        --
����R2Conv: 1-4                                      --                        44
��    ����BlocksBasisExpansion: 2-3                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-2         --                        --
����InnerBatchNorm: 1-5                              --                        --
��    ����BatchNorm3d: 2-4                            --                        2
����ReLU: 1-6                                        --                        --
����R2Conv: 1-7                                      --                        24
��    ����BlocksBasisExpansion: 2-5                   --                        --
��    ��    ����SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-16 18:44:43.921877 | Idx:   40/400    | loss: 2.854 | grad_norm: 0.305 | acc: 99.61% |
| 2025-10-16 18:44:47.826042 | Idx:   80/400    | loss: 1.497 | grad_norm: 0.103 | acc: 99.59% |
| 2025-10-16 18:44:52.928424 | Idx:  120/400    | loss: 1.026 | grad_norm: 0.050 | acc: 99.54% |
| 2025-10-16 18:44:56.509336 | Idx:  160/400    | loss: 0.787 | grad_norm: 0.038 | acc: 99.70% |
| 2025-10-16 18:45:00.391002 | Idx:  200/400    | loss: 0.644 | grad_norm: 0.060 | acc: 99.61% |
| 2025-10-16 18:45:05.689128 | Idx:  240/400    | loss: 0.558 | grad_norm: 0.214 | acc: 99.38% |
| 2025-10-16 18:45:09.602772 | Idx:  280/400    | loss: 0.487 | grad_norm: 0.020 | acc: 99.95% |
| 2025-10-16 18:45:14.706580 | Idx:  320/400    | loss: 0.432 | grad_norm: 0.088 | acc: 99.52% |
| 2025-10-16 18:45:18.630487 | Idx:  360/400    | loss: 0.390 | grad_norm: 0.015 | acc: 99.95% |
| 2025-10-16 18:45:22.581139 | Idx:  400/400    | loss: 0.355 | grad_norm: 0.024 | acc: 99.95% |
Train Loss: 0.3545 Acc: 94.27%
Acc: 99.39%


| val_epoch_acc: 99.39% | epoch: 00 | avg_train_loss: 0.0386 | avg_train_acc: 99.3122 | 


Epoch 2/8
----------
| 2025-10-16 18:45:51.573220 | Idx:   40/400    | loss: 0.062 | grad_norm: 0.436 | acc: 98.69% |
| 2025-10-16 18:45:55.475553 | Idx:   80/400    | loss: 0.054 | grad_norm: 0.042 | acc: 99.82% |
| 2025-10-16 18:46:00.455150 | Idx:  120/400    | loss: 0.051 | grad_norm: 0.017 | acc: 99.83% |
| 2025-10-16 18:46:04.359898 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.262 | acc: 99.00% |
| 2025-10-16 18:46:08.250696 | Idx:  200/400    | loss: 0.050 | grad_norm: 0.019 | acc: 99.16% |
| 2025-10-16 18:46:13.513137 | Idx:  240/400    | loss: 0.050 | grad_norm: 0.330 | acc: 97.69% |
| 2025-10-16 18:46:17.513834 | Idx:  280/400    | loss: 0.049 | grad_norm: 0.344 | acc: 99.52% |
| 2025-10-16 18:46:22.913403 | Idx:  320/400    | loss: 0.047 | grad_norm: 0.020 | acc: 99.38% |
| 2025-10-16 18:46:26.908001 | Idx:  360/400    | loss: 0.047 | grad_norm: 0.092 | acc: 99.18% |
| 2025-10-16 18:46:30.359324 | Idx:  400/400    | loss: 0.046 | grad_norm: 0.141 | acc: 98.97% |
Train Loss: 0.0463 Acc: 99.19%
Acc: 99.48%


| val_epoch_acc: 99.48% | epoch: 01 | avg_train_loss: 0.0420 | avg_train_acc: 99.1736 | 


Epoch 3/8
----------
| 2025-10-16 18:46:59.716888 | Idx:   40/400    | loss: 0.043 | grad_norm: 0.054 | acc: 99.80% |
| 2025-10-16 18:47:03.202598 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.398 | acc: 98.66% |
| 2025-10-16 18:47:08.132283 | Idx:  120/400    | loss: 0.054 | grad_norm: 0.407 | acc: 97.47% |
| 2025-10-16 18:47:11.896148 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.053 | acc: 99.65% |
| 2025-10-16 18:47:15.671358 | Idx:  200/400    | loss: 0.047 | grad_norm: 0.040 | acc: 99.70% |
| 2025-10-16 18:47:20.682484 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.267 | acc: 98.93% |
| 2025-10-16 18:47:24.507020 | Idx:  280/400    | loss: 0.048 | grad_norm: 0.030 | acc: 99.54% |
| 2025-10-16 18:47:29.764596 | Idx:  320/400    | loss: 0.051 | grad_norm: 0.009 | acc: 99.84% |
| 2025-10-16 18:47:33.561207 | Idx:  360/400    | loss: 0.049 | grad_norm: 0.078 | acc: 99.70% |
| 2025-10-16 18:47:37.369052 | Idx:  400/400    | loss: 0.047 | grad_norm: 0.071 | acc: 99.76% |
Train Loss: 0.0473 Acc: 99.12%
Acc: 99.45%


| val_epoch_acc: 99.45% | epoch: 02 | avg_train_loss: 0.0358 | avg_train_acc: 99.2813 | 


Epoch 4/8
----------
| 2025-10-16 18:48:06.461841 | Idx:   40/400    | loss: 0.029 | grad_norm: 0.629 | acc: 96.65% |
| 2025-10-16 18:48:10.253509 | Idx:   80/400    | loss: 0.033 | grad_norm: 0.058 | acc: 99.55% |
| 2025-10-16 18:48:15.432802 | Idx:  120/400    | loss: 0.061 | grad_norm: 0.092 | acc: 99.57% |
| 2025-10-16 18:48:19.260221 | Idx:  160/400    | loss: 0.054 | grad_norm: 0.061 | acc: 99.85% |
| 2025-10-16 18:48:23.054961 | Idx:  200/400    | loss: 0.050 | grad_norm: 0.357 | acc: 98.67% |
| 2025-10-16 18:48:27.957344 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.059 | acc: 99.81% |
| 2025-10-16 18:48:31.516229 | Idx:  280/400    | loss: 0.046 | grad_norm: 0.448 | acc: 97.20% |
| 2025-10-16 18:48:36.215641 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.005 | acc: 99.88% |
| 2025-10-16 18:48:39.880123 | Idx:  360/400    | loss: 0.043 | grad_norm: 0.016 | acc: 99.81% |
| 2025-10-16 18:48:43.843691 | Idx:  400/400    | loss: 0.043 | grad_norm: 0.331 | acc: 97.94% |
Train Loss: 0.0427 Acc: 99.14%
Acc: 99.34%


| val_epoch_acc: 99.34% | epoch: 03 | avg_train_loss: 0.0383 | avg_train_acc: 99.2755 | 


Epoch 5/8
----------
| 2025-10-16 18:49:13.275359 | Idx:   40/400    | loss: 0.032 | grad_norm: 0.038 | acc: 99.83% |
| 2025-10-16 18:49:17.143678 | Idx:   80/400    | loss: 0.033 | grad_norm: 0.197 | acc: 99.07% |
| 2025-10-16 18:49:22.304435 | Idx:  120/400    | loss: 0.036 | grad_norm: 2.083 | acc: 96.24% |
| 2025-10-16 18:49:26.241029 | Idx:  160/400    | loss: 0.036 | grad_norm: 0.169 | acc: 99.40% |
| 2025-10-16 18:49:30.158236 | Idx:  200/400    | loss: 0.035 | grad_norm: 0.005 | acc: 99.87% |
| 2025-10-16 18:49:35.410110 | Idx:  240/400    | loss: 0.038 | grad_norm: 0.017 | acc: 99.91% |
| 2025-10-16 18:49:39.312490 | Idx:  280/400    | loss: 0.047 | grad_norm: 0.026 | acc: 99.75% |
| 2025-10-16 18:49:44.672521 | Idx:  320/400    | loss: 0.046 | grad_norm: 0.030 | acc: 99.40% |
| 2025-10-16 18:49:48.347315 | Idx:  360/400    | loss: 0.045 | grad_norm: 0.168 | acc: 99.33% |
| 2025-10-16 18:49:51.812232 | Idx:  400/400    | loss: 0.045 | grad_norm: 0.009 | acc: 99.63% |
Train Loss: 0.0454 Acc: 99.12%
Acc: 99.42%


| val_epoch_acc: 99.42% | epoch: 04 | avg_train_loss: 0.0481 | avg_train_acc: 99.2829 | 


Epoch 6/8
----------
| 2025-10-16 18:50:19.882911 | Idx:   40/400    | loss: 0.030 | grad_norm: 0.008 | acc: 99.78% |
| 2025-10-16 18:50:23.443921 | Idx:   80/400    | loss: 0.035 | grad_norm: 0.052 | acc: 99.78% |
| 2025-10-16 18:50:28.542733 | Idx:  120/400    | loss: 0.032 | grad_norm: 0.005 | acc: 99.92% |
| 2025-10-16 18:50:32.417827 | Idx:  160/400    | loss: 0.030 | grad_norm: 0.038 | acc: 99.71% |
| 2025-10-16 18:50:36.343131 | Idx:  200/400    | loss: 0.030 | grad_norm: 0.010 | acc: 99.94% |
| 2025-10-16 18:50:41.568483 | Idx:  240/400    | loss: 0.035 | grad_norm: 0.056 | acc: 99.81% |
| 2025-10-16 18:50:45.424612 | Idx:  280/400    | loss: 0.037 | grad_norm: 0.029 | acc: 98.36% |
| 2025-10-16 18:50:50.549449 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.013 | acc: 99.89% |
| 2025-10-16 18:50:54.012573 | Idx:  360/400    | loss: 0.036 | grad_norm: 0.079 | acc: 99.52% |
| 2025-10-16 18:50:57.449584 | Idx:  400/400    | loss: 0.035 | grad_norm: 0.327 | acc: 98.88% |
Train Loss: 0.0351 Acc: 99.23%
Acc: 99.48%


| val_epoch_acc: 99.48% | epoch: 05 | avg_train_loss: 0.0308 | avg_train_acc: 99.4840 | 


Epoch 7/8
----------
| 2025-10-16 18:51:25.351650 | Idx:   40/400    | loss: 0.030 | grad_norm: 0.112 | acc: 97.97% |
| 2025-10-16 18:51:28.803676 | Idx:   80/400    | loss: 0.039 | grad_norm: 0.100 | acc: 99.64% |
| 2025-10-16 18:51:33.661598 | Idx:  120/400    | loss: 0.044 | grad_norm: 0.009 | acc: 99.97% |
| 2025-10-16 18:51:37.098559 | Idx:  160/400    | loss: 0.042 | grad_norm: 0.118 | acc: 99.60% |
| 2025-10-16 18:51:40.537437 | Idx:  200/400    | loss: 0.041 | grad_norm: 0.023 | acc: 99.70% |
| 2025-10-16 18:51:45.286429 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.037 | acc: 99.83% |
| 2025-10-16 18:51:48.733226 | Idx:  280/400    | loss: 0.039 | grad_norm: 0.092 | acc: 99.51% |
| 2025-10-16 18:51:53.374354 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.085 | acc: 99.45% |
| 2025-10-16 18:51:56.821369 | Idx:  360/400    | loss: 0.037 | grad_norm: 0.371 | acc: 97.94% |
| 2025-10-16 18:52:00.257930 | Idx:  400/400    | loss: 0.038 | grad_norm: 0.212 | acc: 98.88% |
Train Loss: 0.0377 Acc: 99.35%
Acc: 99.59%


| val_epoch_acc: 99.59% | epoch: 06 | avg_train_loss: 0.0430 | avg_train_acc: 99.4259 | 


Epoch 8/8
----------
| 2025-10-16 18:52:28.951634 | Idx:   40/400    | loss: 0.034 | grad_norm: 0.014 | acc: 99.97% |
| 2025-10-16 18:52:32.712641 | Idx:   80/400    | loss: 0.029 | grad_norm: 0.052 | acc: 99.77% |
| 2025-10-16 18:52:37.739618 | Idx:  120/400    | loss: 0.028 | grad_norm: 0.008 | acc: 99.73% |
| 2025-10-16 18:52:41.492454 | Idx:  160/400    | loss: 0.036 | grad_norm: 0.105 | acc: 99.67% |
| 2025-10-16 18:52:45.274319 | Idx:  200/400    | loss: 0.035 | grad_norm: 0.068 | acc: 99.65% |
| 2025-10-16 18:52:50.283768 | Idx:  240/400    | loss: 0.034 | grad_norm: 0.018 | acc: 99.83% |
| 2025-10-16 18:52:54.094087 | Idx:  280/400    | loss: 0.031 | grad_norm: 0.072 | acc: 99.79% |
| 2025-10-16 18:52:59.099651 | Idx:  320/400    | loss: 0.032 | grad_norm: 0.030 | acc: 99.86% |
| 2025-10-16 18:53:02.849803 | Idx:  360/400    | loss: 0.032 | grad_norm: 0.046 | acc: 99.72% |
| 2025-10-16 18:53:06.647785 | Idx:  400/400    | loss: 0.032 | grad_norm: 0.148 | acc: 99.36% |
Train Loss: 0.0320 Acc: 99.25%
Acc: 99.55%


| val_epoch_acc: 99.55% | epoch: 07 | avg_train_loss: 0.0276 | avg_train_acc: 99.4878 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_184426-t11rl7lm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_184426-t11rl7lm\logs[0m
rule: B3/S23 \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_18-53-31_tiny_2_layer_seq_cnn__200-200-B3_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 8, 200, 200]          408
����BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
����ReLU: 1-3                              [1, 8, 200, 200]          --
����Conv2d: 1-4                            [1, 8, 200, 200]          1,608
����BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
����ReLU: 1-6                              [1, 8, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 18:53:48.406793 | Idx:   40/400    | loss: 0.631 | grad_norm: 0.210 | acc: 87.32% |
| 2025-10-16 18:53:51.628716 | Idx:   80/400    | loss: 0.502 | grad_norm: 0.123 | acc: 92.75% |
| 2025-10-16 18:53:56.202298 | Idx:  120/400    | loss: 0.440 | grad_norm: 0.157 | acc: 92.19% |
| 2025-10-16 18:53:59.401976 | Idx:  160/400    | loss: 0.387 | grad_norm: 0.476 | acc: 93.63% |
| 2025-10-16 18:54:02.637648 | Idx:  200/400    | loss: 0.344 | grad_norm: 0.173 | acc: 98.64% |
| 2025-10-16 18:54:07.674691 | Idx:  240/400    | loss: 0.321 | grad_norm: 0.854 | acc: 92.74% |
| 2025-10-16 18:54:10.864796 | Idx:  280/400    | loss: 0.296 | grad_norm: 0.036 | acc: 99.79% |
| 2025-10-16 18:54:15.456850 | Idx:  320/400    | loss: 0.263 | grad_norm: 0.118 | acc: 99.75% |
| 2025-10-16 18:54:18.654976 | Idx:  360/400    | loss: 0.237 | grad_norm: 0.119 | acc: 99.76% |
| 2025-10-16 18:54:21.863600 | Idx:  400/400    | loss: 0.220 | grad_norm: 2.389 | acc: 97.90% |
Train Loss: 0.2196 Acc: 94.53%
Acc: 95.63%


| val_epoch_acc: 95.63% | epoch: 00 | avg_train_loss: 0.0710 | avg_train_acc: 99.1295 | 


Epoch 2/8
----------
| 2025-10-16 18:54:50.063031 | Idx:   40/400    | loss: 0.081 | grad_norm: 0.188 | acc: 99.56% |
| 2025-10-16 18:54:53.272765 | Idx:   80/400    | loss: 0.098 | grad_norm: 0.193 | acc: 99.91% |
| 2025-10-16 18:54:57.888356 | Idx:  120/400    | loss: 0.087 | grad_norm: 0.189 | acc: 99.62% |
| 2025-10-16 18:55:01.064706 | Idx:  160/400    | loss: 0.075 | grad_norm: 0.033 | acc: 99.95% |
| 2025-10-16 18:55:04.246752 | Idx:  200/400    | loss: 0.071 | grad_norm: 1.908 | acc: 99.19% |
| 2025-10-16 18:55:08.812658 | Idx:  240/400    | loss: 0.064 | grad_norm: 4.691 | acc: 97.33% |
| 2025-10-16 18:55:11.996910 | Idx:  280/400    | loss: 0.067 | grad_norm: 0.028 | acc: 99.97% |
| 2025-10-16 18:55:16.843703 | Idx:  320/400    | loss: 0.065 | grad_norm: 0.271 | acc: 99.27% |
| 2025-10-16 18:55:19.996982 | Idx:  360/400    | loss: 0.064 | grad_norm: 0.041 | acc: 99.98% |
| 2025-10-16 18:55:23.198694 | Idx:  400/400    | loss: 0.067 | grad_norm: 0.220 | acc: 99.55% |
Train Loss: 0.0669 Acc: 98.93%
Acc: 99.89%


| val_epoch_acc: 99.89% | epoch: 01 | avg_train_loss: 0.1128 | avg_train_acc: 98.6184 | 


Epoch 3/8
----------
| 2025-10-16 18:55:51.200972 | Idx:   40/400    | loss: 0.116 | grad_norm: 0.277 | acc: 99.19% |
| 2025-10-16 18:55:54.374376 | Idx:   80/400    | loss: 0.099 | grad_norm: 2.425 | acc: 96.79% |
| 2025-10-16 18:55:59.060388 | Idx:  120/400    | loss: 0.085 | grad_norm: 0.046 | acc: 99.94% |
| 2025-10-16 18:56:02.186499 | Idx:  160/400    | loss: 0.076 | grad_norm: 0.162 | acc: 99.60% |
| 2025-10-16 18:56:05.355307 | Idx:  200/400    | loss: 0.074 | grad_norm: 0.053 | acc: 99.97% |
| 2025-10-16 18:56:09.892237 | Idx:  240/400    | loss: 0.070 | grad_norm: 0.030 | acc: 99.97% |
| 2025-10-16 18:56:13.055705 | Idx:  280/400    | loss: 0.069 | grad_norm: 0.381 | acc: 99.21% |
| 2025-10-16 18:56:17.894004 | Idx:  320/400    | loss: 0.063 | grad_norm: 0.058 | acc: 99.99% |
| 2025-10-16 18:56:21.046047 | Idx:  360/400    | loss: 0.060 | grad_norm: 0.032 | acc: 100.00% |
| 2025-10-16 18:56:24.202504 | Idx:  400/400    | loss: 0.063 | grad_norm: 9.695 | acc: 94.71% |
Train Loss: 0.0629 Acc: 99.16%
Acc: 99.76%


| val_epoch_acc: 99.76% | epoch: 02 | avg_train_loss: 0.1117 | avg_train_acc: 99.1148 | 


Epoch 4/8
----------
| 2025-10-16 18:56:52.652271 | Idx:   40/400    | loss: 0.010 | grad_norm: 0.018 | acc: 100.00% |
| 2025-10-16 18:56:55.812875 | Idx:   80/400    | loss: 0.012 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 18:57:00.435026 | Idx:  120/400    | loss: 0.033 | grad_norm: 1.610 | acc: 99.28% |
| 2025-10-16 18:57:03.652025 | Idx:  160/400    | loss: 0.032 | grad_norm: 0.361 | acc: 99.23% |
| 2025-10-16 18:57:06.881248 | Idx:  200/400    | loss: 0.037 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-16 18:57:10.699274 | Idx:  240/400    | loss: 0.041 | grad_norm: 0.012 | acc: 99.99% |
| 2025-10-16 18:57:13.132635 | Idx:  280/400    | loss: 0.037 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-16 18:57:16.902698 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.016 | acc: 100.00% |
| 2025-10-16 18:57:19.321014 | Idx:  360/400    | loss: 0.042 | grad_norm: 0.108 | acc: 99.95% |
| 2025-10-16 18:57:21.714469 | Idx:  400/400    | loss: 0.040 | grad_norm: 0.008 | acc: 99.99% |
Train Loss: 0.0405 Acc: 99.55%
Acc: 99.84%


| val_epoch_acc: 99.84% | epoch: 03 | avg_train_loss: 0.0294 | avg_train_acc: 99.4600 | 


Epoch 5/8
----------
| 2025-10-16 18:57:48.861281 | Idx:   40/400    | loss: 0.015 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-16 18:57:51.514573 | Idx:   80/400    | loss: 0.085 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 18:57:55.561176 | Idx:  120/400    | loss: 0.061 | grad_norm: 0.062 | acc: 99.96% |
| 2025-10-16 18:57:58.557732 | Idx:  160/400    | loss: 0.066 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 18:58:01.581244 | Idx:  200/400    | loss: 0.056 | grad_norm: 4.249 | acc: 98.57% |
| 2025-10-16 18:58:05.949555 | Idx:  240/400    | loss: 0.052 | grad_norm: 0.105 | acc: 99.84% |
| 2025-10-16 18:58:08.924690 | Idx:  280/400    | loss: 0.054 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-16 18:58:13.313725 | Idx:  320/400    | loss: 0.057 | grad_norm: 0.167 | acc: 98.96% |
| 2025-10-16 18:58:16.335078 | Idx:  360/400    | loss: 0.052 | grad_norm: 0.027 | acc: 99.98% |
| 2025-10-16 18:58:19.379219 | Idx:  400/400    | loss: 0.051 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0511 Acc: 99.53%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 04 | avg_train_loss: 0.0392 | avg_train_acc: 99.4162 | 


Epoch 6/8
----------
| 2025-10-16 18:58:47.678433 | Idx:   40/400    | loss: 0.015 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 18:58:50.687629 | Idx:   80/400    | loss: 0.010 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 18:58:55.049609 | Idx:  120/400    | loss: 0.008 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 18:58:57.731866 | Idx:  160/400    | loss: 0.018 | grad_norm: 0.107 | acc: 100.00% |
| 2025-10-16 18:59:00.104854 | Idx:  200/400    | loss: 0.018 | grad_norm: 0.594 | acc: 99.71% |
| 2025-10-16 18:59:03.852624 | Idx:  240/400    | loss: 0.050 | grad_norm: 0.036 | acc: 100.00% |
| 2025-10-16 18:59:06.215344 | Idx:  280/400    | loss: 0.070 | grad_norm: 0.048 | acc: 100.00% |
| 2025-10-16 18:59:10.242704 | Idx:  320/400    | loss: 0.063 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-16 18:59:12.786350 | Idx:  360/400    | loss: 0.062 | grad_norm: 0.024 | acc: 99.98% |
| 2025-10-16 18:59:15.134090 | Idx:  400/400    | loss: 0.056 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0563 Acc: 99.54%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_185331-1m00qzyh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_185331-1m00qzyh\logs[0m
rule: B36/S23 \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_18-59-38_tiny_2_layer_seq_cnn__200-200-B36_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 8, 200, 200]          408
����BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
����ReLU: 1-3                              [1, 8, 200, 200]          --
����Conv2d: 1-4                            [1, 8, 200, 200]          1,608
����BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
����ReLU: 1-6                              [1, 8, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 18:59:53.700380 | Idx:   40/400    | loss: 0.714 | grad_norm: 1.464 | acc: 86.20% |
| 2025-10-16 18:59:56.083422 | Idx:   80/400    | loss: 0.571 | grad_norm: 0.134 | acc: 89.70% |
| 2025-10-16 18:59:59.957907 | Idx:  120/400    | loss: 0.525 | grad_norm: 0.310 | acc: 87.70% |
| 2025-10-16 19:00:02.630394 | Idx:  160/400    | loss: 0.487 | grad_norm: 0.301 | acc: 93.33% |
| 2025-10-16 19:00:05.056400 | Idx:  200/400    | loss: 0.453 | grad_norm: 0.176 | acc: 91.92% |
| 2025-10-16 19:00:09.151840 | Idx:  240/400    | loss: 0.418 | grad_norm: 0.663 | acc: 93.48% |
| 2025-10-16 19:00:11.839865 | Idx:  280/400    | loss: 0.391 | grad_norm: 0.216 | acc: 96.58% |
| 2025-10-16 19:00:15.602715 | Idx:  320/400    | loss: 0.364 | grad_norm: 0.357 | acc: 98.71% |
| 2025-10-16 19:00:18.083012 | Idx:  360/400    | loss: 0.349 | grad_norm: 6.963 | acc: 86.48% |
| 2025-10-16 19:00:20.525351 | Idx:  400/400    | loss: 0.337 | grad_norm: 0.693 | acc: 94.89% |
Train Loss: 0.3373 Acc: 90.95%
Acc: 97.32%


| val_epoch_acc: 97.32% | epoch: 00 | avg_train_loss: 0.2674 | avg_train_acc: 94.5963 | 


Epoch 2/8
----------
| 2025-10-16 19:00:47.777474 | Idx:   40/400    | loss: 0.125 | grad_norm: 0.279 | acc: 99.22% |
| 2025-10-16 19:00:50.421056 | Idx:   80/400    | loss: 0.194 | grad_norm: 0.114 | acc: 97.82% |
| 2025-10-16 19:00:54.399374 | Idx:  120/400    | loss: 0.173 | grad_norm: 0.966 | acc: 99.50% |
| 2025-10-16 19:00:57.015356 | Idx:  160/400    | loss: 0.157 | grad_norm: 0.139 | acc: 98.63% |
| 2025-10-16 19:00:59.710159 | Idx:  200/400    | loss: 0.172 | grad_norm: 0.192 | acc: 99.43% |
| 2025-10-16 19:01:03.887315 | Idx:  240/400    | loss: 0.162 | grad_norm: 0.156 | acc: 99.47% |
| 2025-10-16 19:01:06.363039 | Idx:  280/400    | loss: 0.155 | grad_norm: 0.089 | acc: 99.50% |
| 2025-10-16 19:01:10.935582 | Idx:  320/400    | loss: 0.149 | grad_norm: 1.201 | acc: 99.30% |
| 2025-10-16 19:01:13.491430 | Idx:  360/400    | loss: 0.151 | grad_norm: 0.234 | acc: 99.16% |
| 2025-10-16 19:01:15.944364 | Idx:  400/400    | loss: 0.150 | grad_norm: 0.905 | acc: 98.25% |
Train Loss: 0.1503 Acc: 97.65%
Acc: 99.30%


| val_epoch_acc: 99.30% | epoch: 01 | avg_train_loss: 0.1118 | avg_train_acc: 98.2015 | 


Epoch 3/8
----------
| 2025-10-16 19:01:43.341297 | Idx:   40/400    | loss: 0.145 | grad_norm: 0.068 | acc: 99.35% |
| 2025-10-16 19:01:45.998879 | Idx:   80/400    | loss: 0.099 | grad_norm: 0.781 | acc: 99.01% |
| 2025-10-16 19:01:50.255831 | Idx:  120/400    | loss: 0.097 | grad_norm: 0.067 | acc: 99.45% |
| 2025-10-16 19:01:52.997975 | Idx:  160/400    | loss: 0.104 | grad_norm: 0.094 | acc: 99.54% |
| 2025-10-16 19:01:55.638867 | Idx:  200/400    | loss: 0.101 | grad_norm: 0.064 | acc: 99.73% |
| 2025-10-16 19:01:59.634619 | Idx:  240/400    | loss: 0.094 | grad_norm: 0.057 | acc: 99.62% |
| 2025-10-16 19:02:02.190396 | Idx:  280/400    | loss: 0.095 | grad_norm: 3.528 | acc: 93.66% |
| 2025-10-16 19:02:06.283270 | Idx:  320/400    | loss: 0.101 | grad_norm: 0.142 | acc: 99.71% |
| 2025-10-16 19:02:08.739384 | Idx:  360/400    | loss: 0.093 | grad_norm: 0.098 | acc: 99.68% |
| 2025-10-16 19:02:11.191886 | Idx:  400/400    | loss: 0.089 | grad_norm: 0.126 | acc: 99.82% |
Train Loss: 0.0890 Acc: 98.73%
Acc: 99.80%


| val_epoch_acc: 99.80% | epoch: 02 | avg_train_loss: 0.0522 | avg_train_acc: 99.0701 | 


Epoch 4/8
----------
| 2025-10-16 19:02:38.235663 | Idx:   40/400    | loss: 0.079 | grad_norm: 0.073 | acc: 99.75% |
| 2025-10-16 19:02:40.730226 | Idx:   80/400    | loss: 0.063 | grad_norm: 0.041 | acc: 99.83% |
| 2025-10-16 19:02:44.814116 | Idx:  120/400    | loss: 0.084 | grad_norm: 0.382 | acc: 99.58% |
| 2025-10-16 19:02:47.524325 | Idx:  160/400    | loss: 0.099 | grad_norm: 1.966 | acc: 96.39% |
| 2025-10-16 19:02:50.093582 | Idx:  200/400    | loss: 0.107 | grad_norm: 0.076 | acc: 99.68% |
| 2025-10-16 19:02:53.885062 | Idx:  240/400    | loss: 0.103 | grad_norm: 0.074 | acc: 99.86% |
| 2025-10-16 19:02:56.367308 | Idx:  280/400    | loss: 0.099 | grad_norm: 0.068 | acc: 99.85% |
| 2025-10-16 19:03:00.273185 | Idx:  320/400    | loss: 0.100 | grad_norm: 0.026 | acc: 99.85% |
| 2025-10-16 19:03:02.808912 | Idx:  360/400    | loss: 0.103 | grad_norm: 0.292 | acc: 99.28% |
| 2025-10-16 19:03:05.265263 | Idx:  400/400    | loss: 0.099 | grad_norm: 0.663 | acc: 98.91% |
Train Loss: 0.0989 Acc: 98.68%
Acc: 99.84%


| val_epoch_acc: 99.84% | epoch: 03 | avg_train_loss: 0.0596 | avg_train_acc: 99.0742 | 


Epoch 5/8
----------
| 2025-10-16 19:03:31.517565 | Idx:   40/400    | loss: 0.094 | grad_norm: 0.104 | acc: 99.61% |
| 2025-10-16 19:03:33.973531 | Idx:   80/400    | loss: 0.146 | grad_norm: 0.138 | acc: 99.69% |
| 2025-10-16 19:03:37.747505 | Idx:  120/400    | loss: 0.117 | grad_norm: 0.260 | acc: 99.67% |
| 2025-10-16 19:03:40.249701 | Idx:  160/400    | loss: 0.104 | grad_norm: 0.800 | acc: 98.16% |
| 2025-10-16 19:03:43.024331 | Idx:  200/400    | loss: 0.121 | grad_norm: 1.508 | acc: 96.15% |
| 2025-10-16 19:03:46.973286 | Idx:  240/400    | loss: 0.113 | grad_norm: 0.037 | acc: 99.89% |
| 2025-10-16 19:03:49.407077 | Idx:  280/400    | loss: 0.113 | grad_norm: 0.052 | acc: 99.88% |
| 2025-10-16 19:03:53.246023 | Idx:  320/400    | loss: 0.132 | grad_norm: 0.137 | acc: 99.82% |
| 2025-10-16 19:03:55.718806 | Idx:  360/400    | loss: 0.131 | grad_norm: 0.071 | acc: 99.86% |
| 2025-10-16 19:03:58.105675 | Idx:  400/400    | loss: 0.132 | grad_norm: 0.095 | acc: 99.88% |
Train Loss: 0.1318 Acc: 98.35%
Acc: 99.89%


| val_epoch_acc: 99.89% | epoch: 04 | avg_train_loss: 0.1364 | avg_train_acc: 98.1060 | 


Epoch 6/8
----------
| 2025-10-16 19:04:24.864327 | Idx:   40/400    | loss: 0.111 | grad_norm: 0.339 | acc: 99.67% |
| 2025-10-16 19:04:27.544993 | Idx:   80/400    | loss: 0.084 | grad_norm: 0.187 | acc: 99.53% |
| 2025-10-16 19:04:31.299426 | Idx:  120/400    | loss: 0.075 | grad_norm: 0.036 | acc: 99.88% |
| 2025-10-16 19:04:33.668552 | Idx:  160/400    | loss: 0.066 | grad_norm: 0.631 | acc: 99.25% |
| 2025-10-16 19:04:36.026686 | Idx:  200/400    | loss: 0.070 | grad_norm: 0.104 | acc: 99.81% |
| 2025-10-16 19:04:39.677066 | Idx:  240/400    | loss: 0.072 | grad_norm: 0.649 | acc: 98.95% |
| 2025-10-16 19:04:42.035567 | Idx:  280/400    | loss: 0.067 | grad_norm: 0.114 | acc: 99.76% |
| 2025-10-16 19:04:45.749105 | Idx:  320/400    | loss: 0.065 | grad_norm: 0.247 | acc: 99.89% |
| 2025-10-16 19:04:48.107012 | Idx:  360/400    | loss: 0.070 | grad_norm: 0.352 | acc: 99.48% |
| 2025-10-16 19:04:50.458719 | Idx:  400/400    | loss: 0.066 | grad_norm: 0.146 | acc: 99.85% |
Train Loss: 0.0657 Acc: 99.04%
Acc: 99.90%


| val_epoch_acc: 99.90% | epoch: 05 | avg_train_loss: 0.0247 | avg_train_acc: 99.7094 | 


Epoch 7/8
----------
| 2025-10-16 19:05:16.179866 | Idx:   40/400    | loss: 0.028 | grad_norm: 0.088 | acc: 99.91% |
| 2025-10-16 19:05:18.533364 | Idx:   80/400    | loss: 0.045 | grad_norm: 0.073 | acc: 99.83% |
| 2025-10-16 19:05:22.223681 | Idx:  120/400    | loss: 0.058 | grad_norm: 0.095 | acc: 99.83% |
| 2025-10-16 19:05:24.569978 | Idx:  160/400    | loss: 0.054 | grad_norm: 1.145 | acc: 99.79% |
| 2025-10-16 19:05:26.914704 | Idx:  200/400    | loss: 0.052 | grad_norm: 0.417 | acc: 99.27% |
| 2025-10-16 19:05:30.599210 | Idx:  240/400    | loss: 0.053 | grad_norm: 0.068 | acc: 99.80% |
| 2025-10-16 19:05:32.952810 | Idx:  280/400    | loss: 0.054 | grad_norm: 0.713 | acc: 99.17% |
| 2025-10-16 19:05:36.586110 | Idx:  320/400    | loss: 0.051 | grad_norm: 0.058 | acc: 99.91% |
| 2025-10-16 19:05:38.931361 | Idx:  360/400    | loss: 0.052 | grad_norm: 0.180 | acc: 99.71% |
| 2025-10-16 19:05:41.265326 | Idx:  400/400    | loss: 0.050 | grad_norm: 0.047 | acc: 99.90% |
Train Loss: 0.0499 Acc: 99.29%
Acc: 99.90%


| val_epoch_acc: 99.90% | epoch: 06 | avg_train_loss: 0.0259 | avg_train_acc: 99.6951 | 


Epoch 8/8
----------
| 2025-10-16 19:06:06.948522 | Idx:   40/400    | loss: 0.064 | grad_norm: 0.035 | acc: 99.90% |
| 2025-10-16 19:06:09.287976 | Idx:   80/400    | loss: 0.044 | grad_norm: 0.099 | acc: 99.84% |
| 2025-10-16 19:06:12.982330 | Idx:  120/400    | loss: 0.039 | grad_norm: 0.031 | acc: 99.91% |
| 2025-10-16 19:06:15.332999 | Idx:  160/400    | loss: 0.041 | grad_norm: 0.180 | acc: 99.82% |
| 2025-10-16 19:06:17.684754 | Idx:  200/400    | loss: 0.040 | grad_norm: 0.962 | acc: 98.96% |
| 2025-10-16 19:06:21.373516 | Idx:  240/400    | loss: 0.040 | grad_norm: 0.415 | acc: 99.78% |
| 2025-10-16 19:06:23.720269 | Idx:  280/400    | loss: 0.039 | grad_norm: 0.432 | acc: 99.61% |
| 2025-10-16 19:06:27.515658 | Idx:  320/400    | loss: 0.041 | grad_norm: 3.140 | acc: 99.06% |
| 2025-10-16 19:06:29.868695 | Idx:  360/400    | loss: 0.040 | grad_norm: 0.880 | acc: 99.59% |
| 2025-10-16 19:06:32.203478 | Idx:  400/400    | loss: 0.038 | grad_norm: 0.038 | acc: 99.90% |
Train Loss: 0.0384 Acc: 99.44%
Acc: 99.91%


| val_epoch_acc: 99.91% | epoch: 07 | avg_train_loss: 0.0214 | avg_train_acc: 99.7300 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_185938-x8b8ye10[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_185938-x8b8ye10\logs[0m
rule: B3678/S34678 \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_19-06-55_tiny_2_layer_seq_cnn__200-200-B3678_S34678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 8, 200, 200]          408
����BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
����ReLU: 1-3                              [1, 8, 200, 200]          --
����Conv2d: 1-4                            [1, 8, 200, 200]          1,608
����BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
����ReLU: 1-6                              [1, 8, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:07:11.116184 | Idx:   40/400    | loss: 0.503 | grad_norm: 0.557 | acc: 92.60% |
| 2025-10-16 19:07:13.455332 | Idx:   80/400    | loss: 0.416 | grad_norm: 0.080 | acc: 95.00% |
| 2025-10-16 19:07:17.063462 | Idx:  120/400    | loss: 0.402 | grad_norm: 0.370 | acc: 90.28% |
| 2025-10-16 19:07:19.420918 | Idx:  160/400    | loss: 0.384 | grad_norm: 0.435 | acc: 93.69% |
| 2025-10-16 19:07:21.763586 | Idx:  200/400    | loss: 0.372 | grad_norm: 0.052 | acc: 97.90% |
| 2025-10-16 19:07:25.666389 | Idx:  240/400    | loss: 0.361 | grad_norm: 0.070 | acc: 92.52% |
| 2025-10-16 19:07:28.009181 | Idx:  280/400    | loss: 0.357 | grad_norm: 0.210 | acc: 95.28% |
| 2025-10-16 19:07:31.701105 | Idx:  320/400    | loss: 0.354 | grad_norm: 0.142 | acc: 94.58% |
| 2025-10-16 19:07:34.055403 | Idx:  360/400    | loss: 0.352 | grad_norm: 0.216 | acc: 90.31% |
| 2025-10-16 19:07:36.391651 | Idx:  400/400    | loss: 0.348 | grad_norm: 1.011 | acc: 84.65% |
Train Loss: 0.3478 Acc: 90.51%
Acc: 92.97%


| val_epoch_acc: 92.97% | epoch: 00 | avg_train_loss: 0.3121 | avg_train_acc: 91.4506 | 


Epoch 2/8
----------
| 2025-10-16 19:08:01.716281 | Idx:   40/400    | loss: 0.290 | grad_norm: 0.357 | acc: 93.00% |
| 2025-10-16 19:08:04.065406 | Idx:   80/400    | loss: 0.296 | grad_norm: 0.186 | acc: 90.35% |
| 2025-10-16 19:08:07.874941 | Idx:  120/400    | loss: 0.284 | grad_norm: 0.154 | acc: 97.39% |
| 2025-10-16 19:08:10.224955 | Idx:  160/400    | loss: 0.280 | grad_norm: 0.105 | acc: 94.86% |
| 2025-10-16 19:08:12.563404 | Idx:  200/400    | loss: 0.278 | grad_norm: 0.364 | acc: 96.02% |
| 2025-10-16 19:08:16.408056 | Idx:  240/400    | loss: 0.275 | grad_norm: 0.689 | acc: 93.94% |
| 2025-10-16 19:08:18.742255 | Idx:  280/400    | loss: 0.267 | grad_norm: 0.335 | acc: 95.67% |
| 2025-10-16 19:08:22.588259 | Idx:  320/400    | loss: 0.261 | grad_norm: 0.494 | acc: 94.10% |
| 2025-10-16 19:08:24.940039 | Idx:  360/400    | loss: 0.256 | grad_norm: 0.742 | acc: 96.48% |
| 2025-10-16 19:08:27.277590 | Idx:  400/400    | loss: 0.253 | grad_norm: 0.151 | acc: 98.50% |
Train Loss: 0.2526 Acc: 93.63%
Acc: 96.60%


| val_epoch_acc: 96.60% | epoch: 01 | avg_train_loss: 0.2176 | avg_train_acc: 95.2393 | 


Epoch 3/8
----------
| 2025-10-16 19:08:52.869870 | Idx:   40/400    | loss: 0.153 | grad_norm: 0.577 | acc: 94.38% |
| 2025-10-16 19:08:55.215688 | Idx:   80/400    | loss: 0.155 | grad_norm: 0.345 | acc: 96.96% |
| 2025-10-16 19:08:58.864546 | Idx:  120/400    | loss: 0.173 | grad_norm: 1.445 | acc: 97.24% |
| 2025-10-16 19:09:01.218603 | Idx:  160/400    | loss: 0.175 | grad_norm: 1.052 | acc: 97.44% |
| 2025-10-16 19:09:03.563392 | Idx:  200/400    | loss: 0.174 | grad_norm: 1.516 | acc: 92.07% |
| 2025-10-16 19:09:07.233930 | Idx:  240/400    | loss: 0.179 | grad_norm: 0.348 | acc: 98.93% |
| 2025-10-16 19:09:09.582106 | Idx:  280/400    | loss: 0.179 | grad_norm: 0.392 | acc: 99.28% |
| 2025-10-16 19:09:13.505922 | Idx:  320/400    | loss: 0.179 | grad_norm: 0.891 | acc: 96.53% |
| 2025-10-16 19:09:15.852518 | Idx:  360/400    | loss: 0.179 | grad_norm: 0.463 | acc: 97.78% |
| 2025-10-16 19:09:18.190893 | Idx:  400/400    | loss: 0.177 | grad_norm: 0.751 | acc: 98.69% |
Train Loss: 0.1768 Acc: 96.33%
Acc: 98.61%


| val_epoch_acc: 98.61% | epoch: 02 | avg_train_loss: 0.1629 | avg_train_acc: 96.8109 | 


Epoch 4/8
----------
| 2025-10-16 19:09:43.683597 | Idx:   40/400    | loss: 0.131 | grad_norm: 0.603 | acc: 95.37% |
| 2025-10-16 19:09:46.034249 | Idx:   80/400    | loss: 0.131 | grad_norm: 1.050 | acc: 94.99% |
| 2025-10-16 19:09:49.703093 | Idx:  120/400    | loss: 0.135 | grad_norm: 0.887 | acc: 98.66% |
| 2025-10-16 19:09:52.045819 | Idx:  160/400    | loss: 0.124 | grad_norm: 0.233 | acc: 99.89% |
| 2025-10-16 19:09:54.385124 | Idx:  200/400    | loss: 0.146 | grad_norm: 4.471 | acc: 92.76% |
| 2025-10-16 19:09:58.195817 | Idx:  240/400    | loss: 0.146 | grad_norm: 0.643 | acc: 97.75% |
| 2025-10-16 19:10:00.534609 | Idx:  280/400    | loss: 0.153 | grad_norm: 7.735 | acc: 89.00% |
| 2025-10-16 19:10:04.183255 | Idx:  320/400    | loss: 0.155 | grad_norm: 0.395 | acc: 97.08% |
| 2025-10-16 19:10:06.566972 | Idx:  360/400    | loss: 0.163 | grad_norm: 6.050 | acc: 86.55% |
| 2025-10-16 19:10:08.900509 | Idx:  400/400    | loss: 0.158 | grad_norm: 0.819 | acc: 99.30% |
Train Loss: 0.1582 Acc: 97.09%
Acc: 99.47%


| val_epoch_acc: 99.47% | epoch: 03 | avg_train_loss: 0.1039 | avg_train_acc: 98.2060 | 


Epoch 5/8
----------
| 2025-10-16 19:10:34.830056 | Idx:   40/400    | loss: 0.168 | grad_norm: 0.535 | acc: 98.49% |
| 2025-10-16 19:10:37.166320 | Idx:   80/400    | loss: 0.141 | grad_norm: 0.215 | acc: 98.79% |
| 2025-10-16 19:10:40.952031 | Idx:  120/400    | loss: 0.121 | grad_norm: 0.747 | acc: 96.88% |
| 2025-10-16 19:10:43.295250 | Idx:  160/400    | loss: 0.145 | grad_norm: 2.481 | acc: 97.84% |
| 2025-10-16 19:10:45.638291 | Idx:  200/400    | loss: 0.129 | grad_norm: 1.734 | acc: 97.78% |
| 2025-10-16 19:10:49.310680 | Idx:  240/400    | loss: 0.121 | grad_norm: 1.438 | acc: 93.20% |
| 2025-10-16 19:10:51.647896 | Idx:  280/400    | loss: 0.127 | grad_norm: 1.732 | acc: 99.11% |
| 2025-10-16 19:10:55.274602 | Idx:  320/400    | loss: 0.120 | grad_norm: 0.265 | acc: 99.58% |
| 2025-10-16 19:10:57.613962 | Idx:  360/400    | loss: 0.120 | grad_norm: 2.377 | acc: 95.03% |
| 2025-10-16 19:10:59.944308 | Idx:  400/400    | loss: 0.134 | grad_norm: 2.765 | acc: 97.94% |
Train Loss: 0.1339 Acc: 97.77%
Acc: 99.53%


| val_epoch_acc: 99.53% | epoch: 04 | avg_train_loss: 0.1855 | avg_train_acc: 96.8648 | 


Epoch 6/8
----------
| 2025-10-16 19:11:25.827879 | Idx:   40/400    | loss: 0.151 | grad_norm: 1.191 | acc: 99.18% |
| 2025-10-16 19:11:28.698402 | Idx:   80/400    | loss: 0.126 | grad_norm: 3.891 | acc: 92.56% |
| 2025-10-16 19:11:32.657179 | Idx:  120/400    | loss: 0.150 | grad_norm: 1.391 | acc: 97.37% |
| 2025-10-16 19:11:35.065277 | Idx:  160/400    | loss: 0.130 | grad_norm: 0.355 | acc: 99.62% |
| 2025-10-16 19:11:37.451833 | Idx:  200/400    | loss: 0.118 | grad_norm: 2.861 | acc: 96.89% |
| 2025-10-16 19:11:41.190767 | Idx:  240/400    | loss: 0.115 | grad_norm: 0.221 | acc: 99.97% |
| 2025-10-16 19:11:43.562870 | Idx:  280/400    | loss: 0.120 | grad_norm: 0.847 | acc: 99.34% |
| 2025-10-16 19:11:47.320790 | Idx:  320/400    | loss: 0.119 | grad_norm: 0.219 | acc: 99.97% |
| 2025-10-16 19:11:49.700953 | Idx:  360/400    | loss: 0.120 | grad_norm: 0.101 | acc: 99.78% |
| 2025-10-16 19:11:52.070471 | Idx:  400/400    | loss: 0.117 | grad_norm: 0.189 | acc: 100.00% |
Train Loss: 0.1169 Acc: 98.11%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 05 | avg_train_loss: 0.0891 | avg_train_acc: 98.5085 | 


Epoch 7/8
----------
| 2025-10-16 19:12:19.023723 | Idx:   40/400    | loss: 0.072 | grad_norm: 0.697 | acc: 99.82% |
| 2025-10-16 19:12:21.744368 | Idx:   80/400    | loss: 0.069 | grad_norm: 0.081 | acc: 100.00% |
| 2025-10-16 19:12:26.017530 | Idx:  120/400    | loss: 0.073 | grad_norm: 2.171 | acc: 99.18% |
| 2025-10-16 19:12:29.021336 | Idx:  160/400    | loss: 0.075 | grad_norm: 1.775 | acc: 98.99% |
| 2025-10-16 19:12:31.795859 | Idx:  200/400    | loss: 0.075 | grad_norm: 0.283 | acc: 99.99% |
| 2025-10-16 19:12:36.148963 | Idx:  240/400    | loss: 0.080 | grad_norm: 0.936 | acc: 99.11% |
| 2025-10-16 19:12:39.192746 | Idx:  280/400    | loss: 0.079 | grad_norm: 2.431 | acc: 96.66% |
| 2025-10-16 19:12:43.616191 | Idx:  320/400    | loss: 0.077 | grad_norm: 0.138 | acc: 99.97% |
| 2025-10-16 19:12:46.613644 | Idx:  360/400    | loss: 0.082 | grad_norm: 3.576 | acc: 95.71% |
| 2025-10-16 19:12:49.602257 | Idx:  400/400    | loss: 0.083 | grad_norm: 0.261 | acc: 100.00% |
Train Loss: 0.0832 Acc: 98.72%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 06 | avg_train_loss: 0.0625 | avg_train_acc: 99.1500 | 


Epoch 8/8
----------
| 2025-10-16 19:13:18.271640 | Idx:   40/400    | loss: 0.143 | grad_norm: 7.522 | acc: 81.84% |
| 2025-10-16 19:13:21.303748 | Idx:   80/400    | loss: 0.123 | grad_norm: 0.153 | acc: 99.85% |
| 2025-10-16 19:13:25.751712 | Idx:  120/400    | loss: 0.101 | grad_norm: 0.765 | acc: 99.74% |
| 2025-10-16 19:13:28.906934 | Idx:  160/400    | loss: 0.090 | grad_norm: 0.388 | acc: 99.97% |
| 2025-10-16 19:13:32.045860 | Idx:  200/400    | loss: 0.101 | grad_norm: 1.109 | acc: 98.24% |
| 2025-10-16 19:13:36.770456 | Idx:  240/400    | loss: 0.101 | grad_norm: 0.278 | acc: 99.83% |
| 2025-10-16 19:13:39.810864 | Idx:  280/400    | loss: 0.110 | grad_norm: 0.557 | acc: 99.99% |
| 2025-10-16 19:13:44.440677 | Idx:  320/400    | loss: 0.103 | grad_norm: 1.189 | acc: 99.66% |
| 2025-10-16 19:13:47.516735 | Idx:  360/400    | loss: 0.102 | grad_norm: 1.130 | acc: 98.87% |
| 2025-10-16 19:13:50.588680 | Idx:  400/400    | loss: 0.100 | grad_norm: 1.189 | acc: 99.96% |
Train Loss: 0.0995 Acc: 98.46%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 07 | avg_train_loss: 0.0736 | avg_train_acc: 98.6675 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_190655-js6pdmwz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_190655-js6pdmwz\logs[0m
rule: B35678/S5678 \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_19-14-16_tiny_2_layer_seq_cnn__200-200-B35678_S5678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 8, 200, 200]          408
����BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
����ReLU: 1-3                              [1, 8, 200, 200]          --
����Conv2d: 1-4                            [1, 8, 200, 200]          1,608
����BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
����ReLU: 1-6                              [1, 8, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:14:33.300955 | Idx:   40/400    | loss: 0.231 | grad_norm: 0.342 | acc: 97.67% |
| 2025-10-16 19:14:36.463913 | Idx:   80/400    | loss: 0.214 | grad_norm: 0.135 | acc: 98.20% |
| 2025-10-16 19:14:41.070744 | Idx:  120/400    | loss: 0.168 | grad_norm: 0.337 | acc: 98.77% |
| 2025-10-16 19:14:44.240706 | Idx:  160/400    | loss: 0.138 | grad_norm: 0.028 | acc: 99.82% |
| 2025-10-16 19:14:47.344293 | Idx:  200/400    | loss: 0.122 | grad_norm: 0.052 | acc: 99.29% |
| 2025-10-16 19:14:52.078578 | Idx:  240/400    | loss: 0.116 | grad_norm: 0.323 | acc: 98.29% |
| 2025-10-16 19:14:55.171319 | Idx:  280/400    | loss: 0.108 | grad_norm: 0.016 | acc: 99.58% |
| 2025-10-16 19:14:59.588393 | Idx:  320/400    | loss: 0.100 | grad_norm: 0.124 | acc: 98.42% |
| 2025-10-16 19:15:02.732500 | Idx:  360/400    | loss: 0.095 | grad_norm: 0.513 | acc: 99.79% |
| 2025-10-16 19:15:05.859883 | Idx:  400/400    | loss: 0.091 | grad_norm: 0.017 | acc: 99.72% |
Train Loss: 0.0906 Acc: 98.50%
Acc: 99.13%


| val_epoch_acc: 99.13% | epoch: 00 | avg_train_loss: 0.0483 | avg_train_acc: 99.0372 | 


Epoch 2/8
----------
| 2025-10-16 19:15:34.308030 | Idx:   40/400    | loss: 0.184 | grad_norm: 0.014 | acc: 99.71% |
| 2025-10-16 19:15:37.464851 | Idx:   80/400    | loss: 0.338 | grad_norm: 0.008 | acc: 99.90% |
| 2025-10-16 19:15:41.905505 | Idx:  120/400    | loss: 0.249 | grad_norm: 0.790 | acc: 97.53% |
| 2025-10-16 19:15:45.026830 | Idx:  160/400    | loss: 0.204 | grad_norm: 0.064 | acc: 98.70% |
| 2025-10-16 19:15:48.108532 | Idx:  200/400    | loss: 0.179 | grad_norm: 0.350 | acc: 97.28% |
| 2025-10-16 19:15:52.674852 | Idx:  240/400    | loss: 0.161 | grad_norm: 0.252 | acc: 98.98% |
| 2025-10-16 19:15:55.747242 | Idx:  280/400    | loss: 0.145 | grad_norm: 0.137 | acc: 97.50% |
| 2025-10-16 19:16:00.508693 | Idx:  320/400    | loss: 0.132 | grad_norm: 0.014 | acc: 99.55% |
| 2025-10-16 19:16:03.576531 | Idx:  360/400    | loss: 0.124 | grad_norm: 0.047 | acc: 98.57% |
| 2025-10-16 19:16:06.633924 | Idx:  400/400    | loss: 0.117 | grad_norm: 0.014 | acc: 99.57% |
Train Loss: 0.1167 Acc: 98.73%
Acc: 99.07%


| val_epoch_acc: 99.07% | epoch: 01 | avg_train_loss: 0.0520 | avg_train_acc: 98.9268 | 


Epoch 3/8
----------
| 2025-10-16 19:16:35.002972 | Idx:   40/400    | loss: 0.037 | grad_norm: 0.019 | acc: 99.46% |
| 2025-10-16 19:16:38.025759 | Idx:   80/400    | loss: 0.043 | grad_norm: 0.017 | acc: 99.31% |
| 2025-10-16 19:16:42.331681 | Idx:  120/400    | loss: 0.046 | grad_norm: 0.058 | acc: 98.33% |
| 2025-10-16 19:16:45.358122 | Idx:  160/400    | loss: 0.044 | grad_norm: 0.089 | acc: 97.71% |
| 2025-10-16 19:16:48.369752 | Idx:  200/400    | loss: 0.045 | grad_norm: 0.102 | acc: 98.22% |
| 2025-10-16 19:16:52.702243 | Idx:  240/400    | loss: 0.043 | grad_norm: 0.018 | acc: 99.96% |
| 2025-10-16 19:16:55.736442 | Idx:  280/400    | loss: 0.045 | grad_norm: 0.013 | acc: 99.59% |
| 2025-10-16 19:17:00.399424 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.264 | acc: 97.36% |
| 2025-10-16 19:17:03.416810 | Idx:  360/400    | loss: 0.046 | grad_norm: 0.108 | acc: 99.37% |
| 2025-10-16 19:17:06.418705 | Idx:  400/400    | loss: 0.046 | grad_norm: 0.011 | acc: 99.69% |
Train Loss: 0.0462 Acc: 99.14%
Acc: 98.80%


| val_epoch_acc: 98.80% | epoch: 02 | avg_train_loss: 0.0559 | avg_train_acc: 99.0434 | 


Epoch 4/8
----------
| 2025-10-16 19:17:34.301583 | Idx:   40/400    | loss: 0.070 | grad_norm: 0.395 | acc: 97.14% |
| 2025-10-16 19:17:37.313932 | Idx:   80/400    | loss: 0.065 | grad_norm: 0.007 | acc: 99.88% |
| 2025-10-16 19:17:41.565488 | Idx:  120/400    | loss: 0.066 | grad_norm: 0.012 | acc: 99.68% |
| 2025-10-16 19:17:44.613877 | Idx:  160/400    | loss: 0.068 | grad_norm: 0.149 | acc: 94.91% |
| 2025-10-16 19:17:47.702568 | Idx:  200/400    | loss: 0.064 | grad_norm: 0.022 | acc: 99.77% |
| 2025-10-16 19:17:52.196148 | Idx:  240/400    | loss: 0.064 | grad_norm: 0.067 | acc: 98.12% |
| 2025-10-16 19:17:55.244041 | Idx:  280/400    | loss: 0.060 | grad_norm: 0.031 | acc: 99.91% |
| 2025-10-16 19:17:59.586625 | Idx:  320/400    | loss: 0.056 | grad_norm: 0.006 | acc: 99.01% |
| 2025-10-16 19:18:02.613284 | Idx:  360/400    | loss: 0.052 | grad_norm: 0.006 | acc: 99.58% |
| 2025-10-16 19:18:05.652906 | Idx:  400/400    | loss: 0.051 | grad_norm: 0.011 | acc: 99.73% |
Train Loss: 0.0512 Acc: 99.09%
Acc: 99.32%


| val_epoch_acc: 99.32% | epoch: 03 | avg_train_loss: 0.0468 | avg_train_acc: 99.1455 | 


Epoch 5/8
----------
| 2025-10-16 19:18:34.093773 | Idx:   40/400    | loss: 0.093 | grad_norm: 0.051 | acc: 99.72% |
| 2025-10-16 19:18:37.105447 | Idx:   80/400    | loss: 0.070 | grad_norm: 0.008 | acc: 99.80% |
| 2025-10-16 19:18:41.391678 | Idx:  120/400    | loss: 0.060 | grad_norm: 0.005 | acc: 99.60% |
| 2025-10-16 19:18:44.410385 | Idx:  160/400    | loss: 0.056 | grad_norm: 0.010 | acc: 99.92% |
| 2025-10-16 19:18:47.443835 | Idx:  200/400    | loss: 0.052 | grad_norm: 0.012 | acc: 99.94% |
| 2025-10-16 19:18:51.788900 | Idx:  240/400    | loss: 0.049 | grad_norm: 0.014 | acc: 99.38% |
| 2025-10-16 19:18:54.777243 | Idx:  280/400    | loss: 0.047 | grad_norm: 0.007 | acc: 99.91% |
| 2025-10-16 19:18:59.036300 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.008 | acc: 99.66% |
| 2025-10-16 19:19:02.077784 | Idx:  360/400    | loss: 0.045 | grad_norm: 0.319 | acc: 97.63% |
| 2025-10-16 19:19:05.071327 | Idx:  400/400    | loss: 0.044 | grad_norm: 0.015 | acc: 99.96% |
Train Loss: 0.0435 Acc: 99.11%
Acc: 99.39%


| val_epoch_acc: 99.39% | epoch: 04 | avg_train_loss: 0.0334 | avg_train_acc: 99.3589 | 


Epoch 6/8
----------
| 2025-10-16 19:19:33.452536 | Idx:   40/400    | loss: 0.041 | grad_norm: 0.135 | acc: 98.72% |
| 2025-10-16 19:19:36.456107 | Idx:   80/400    | loss: 0.040 | grad_norm: 0.068 | acc: 99.60% |
| 2025-10-16 19:19:40.737943 | Idx:  120/400    | loss: 0.780 | grad_norm: 0.347 | acc: 99.46% |
| 2025-10-16 19:19:43.799589 | Idx:  160/400    | loss: 0.595 | grad_norm: 0.020 | acc: 99.98% |
| 2025-10-16 19:19:46.813348 | Idx:  200/400    | loss: 0.483 | grad_norm: 0.048 | acc: 99.49% |
| 2025-10-16 19:19:51.141652 | Idx:  240/400    | loss: 0.407 | grad_norm: 0.015 | acc: 99.99% |
| 2025-10-16 19:19:54.172362 | Idx:  280/400    | loss: 0.354 | grad_norm: 0.003 | acc: 99.96% |
| 2025-10-16 19:19:58.490477 | Idx:  320/400    | loss: 0.313 | grad_norm: 0.010 | acc: 99.78% |
| 2025-10-16 19:20:01.482151 | Idx:  360/400    | loss: 0.281 | grad_norm: 0.007 | acc: 99.89% |
| 2025-10-16 19:20:04.540173 | Idx:  400/400    | loss: 0.256 | grad_norm: 0.093 | acc: 95.86% |
Train Loss: 0.2564 Acc: 99.26%
Acc: 99.50%


| val_epoch_acc: 99.50% | epoch: 05 | avg_train_loss: 0.0332 | avg_train_acc: 99.2895 | 


Epoch 7/8
----------
| 2025-10-16 19:20:33.047346 | Idx:   40/400    | loss: 0.026 | grad_norm: 0.079 | acc: 98.35% |
| 2025-10-16 19:20:36.075605 | Idx:   80/400    | loss: 0.022 | grad_norm: 0.006 | acc: 99.87% |
| 2025-10-16 19:20:40.319882 | Idx:  120/400    | loss: 0.034 | grad_norm: 0.081 | acc: 99.64% |
| 2025-10-16 19:20:43.353600 | Idx:  160/400    | loss: 0.033 | grad_norm: 0.201 | acc: 97.95% |
| 2025-10-16 19:20:46.417586 | Idx:  200/400    | loss: 0.033 | grad_norm: 0.060 | acc: 99.09% |
| 2025-10-16 19:20:50.760702 | Idx:  240/400    | loss: 0.032 | grad_norm: 0.094 | acc: 99.38% |
| 2025-10-16 19:20:53.839214 | Idx:  280/400    | loss: 0.032 | grad_norm: 0.008 | acc: 99.90% |
| 2025-10-16 19:20:58.309319 | Idx:  320/400    | loss: 0.033 | grad_norm: 0.530 | acc: 98.29% |
| 2025-10-16 19:21:01.353288 | Idx:  360/400    | loss: 0.034 | grad_norm: 0.209 | acc: 99.15% |
| 2025-10-16 19:21:04.467891 | Idx:  400/400    | loss: 0.034 | grad_norm: 0.010 | acc: 99.92% |
Train Loss: 0.0343 Acc: 99.34%
Acc: 99.48%


| val_epoch_acc: 99.48% | epoch: 06 | avg_train_loss: 0.0371 | avg_train_acc: 99.2301 | 


Epoch 8/8
----------
| 2025-10-16 19:21:33.175600 | Idx:   40/400    | loss: 0.058 | grad_norm: 0.199 | acc: 98.34% |
| 2025-10-16 19:21:36.210484 | Idx:   80/400    | loss: 0.074 | grad_norm: 0.075 | acc: 99.16% |
| 2025-10-16 19:21:40.629940 | Idx:  120/400    | loss: 0.062 | grad_norm: 0.023 | acc: 99.28% |
| 2025-10-16 19:21:43.744352 | Idx:  160/400    | loss: 0.054 | grad_norm: 0.027 | acc: 99.84% |
| 2025-10-16 19:21:46.773755 | Idx:  200/400    | loss: 0.048 | grad_norm: 0.014 | acc: 99.85% |
| 2025-10-16 19:21:51.071194 | Idx:  240/400    | loss: 0.044 | grad_norm: 0.015 | acc: 99.75% |
| 2025-10-16 19:21:54.094471 | Idx:  280/400    | loss: 0.042 | grad_norm: 0.022 | acc: 99.93% |
| 2025-10-16 19:21:58.414700 | Idx:  320/400    | loss: 0.039 | grad_norm: 0.005 | acc: 99.98% |
| 2025-10-16 19:22:01.468470 | Idx:  360/400    | loss: 0.041 | grad_norm: 0.024 | acc: 99.65% |
| 2025-10-16 19:22:04.515952 | Idx:  400/400    | loss: 0.040 | grad_norm: 0.009 | acc: 99.94% |
Train Loss: 0.0400 Acc: 99.18%
Acc: 99.69%


| val_epoch_acc: 99.69% | epoch: 07 | avg_train_loss: 0.0351 | avg_train_acc: 99.3009 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_191416-wx2c6gqj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_191416-wx2c6gqj\logs[0m
rule: B3/S23 \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_19-22-29_tiny_2_layer_seq_cnn__200-200-B3_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 1, 200, 200]          51
����BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
����ReLU: 1-3                              [1, 1, 200, 200]          --
����Conv2d: 1-4                            [1, 1, 200, 200]          26
����BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
����ReLU: 1-6                              [1, 1, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:22:44.878281 | Idx:   40/400    | loss: 1.001 | grad_norm: 0.293 | acc: 85.01% |
| 2025-10-16 19:22:46.282280 | Idx:   80/400    | loss: 0.768 | grad_norm: 0.781 | acc: 90.27% |
| 2025-10-16 19:22:49.082316 | Idx:  120/400    | loss: 0.657 | grad_norm: 0.689 | acc: 91.46% |
| 2025-10-16 19:22:50.451815 | Idx:  160/400    | loss: 0.583 | grad_norm: 0.628 | acc: 92.42% |
| 2025-10-16 19:22:51.843325 | Idx:  200/400    | loss: 0.538 | grad_norm: 0.920 | acc: 89.96% |
| 2025-10-16 19:22:54.965444 | Idx:  240/400    | loss: 0.509 | grad_norm: 0.854 | acc: 90.43% |
| 2025-10-16 19:22:56.332304 | Idx:  280/400    | loss: 0.484 | grad_norm: 1.421 | acc: 86.09% |
| 2025-10-16 19:22:59.131705 | Idx:  320/400    | loss: 0.468 | grad_norm: 0.651 | acc: 90.33% |
| 2025-10-16 19:23:00.480975 | Idx:  360/400    | loss: 0.448 | grad_norm: 0.702 | acc: 91.70% |
| 2025-10-16 19:23:01.817412 | Idx:  400/400    | loss: 0.431 | grad_norm: 0.920 | acc: 95.19% |
Train Loss: 0.4310 Acc: 87.97%
Acc: 87.94%


| val_epoch_acc: 87.94% | epoch: 00 | avg_train_loss: 0.2770 | avg_train_acc: 92.9549 | 


Epoch 2/8
----------
| 2025-10-16 19:23:29.047876 | Idx:   40/400    | loss: 0.257 | grad_norm: 2.598 | acc: 93.45% |
| 2025-10-16 19:23:30.433957 | Idx:   80/400    | loss: 0.265 | grad_norm: 1.440 | acc: 89.04% |
| 2025-10-16 19:23:33.301462 | Idx:  120/400    | loss: 0.265 | grad_norm: 0.429 | acc: 94.50% |
| 2025-10-16 19:23:34.667307 | Idx:  160/400    | loss: 0.265 | grad_norm: 0.189 | acc: 94.87% |
| 2025-10-16 19:23:36.105937 | Idx:  200/400    | loss: 0.264 | grad_norm: 3.271 | acc: 92.13% |
| 2025-10-16 19:23:39.110180 | Idx:  240/400    | loss: 0.255 | grad_norm: 0.409 | acc: 95.23% |
| 2025-10-16 19:23:40.749889 | Idx:  280/400    | loss: 0.262 | grad_norm: 0.328 | acc: 94.71% |
| 2025-10-16 19:23:44.192640 | Idx:  320/400    | loss: 0.260 | grad_norm: 0.352 | acc: 95.14% |
| 2025-10-16 19:23:45.514516 | Idx:  360/400    | loss: 0.261 | grad_norm: 0.316 | acc: 94.56% |
| 2025-10-16 19:23:46.780337 | Idx:  400/400    | loss: 0.262 | grad_norm: 4.753 | acc: 91.38% |
Train Loss: 0.2617 Acc: 93.34%
Acc: 95.08%


| val_epoch_acc: 95.08% | epoch: 01 | avg_train_loss: 0.2518 | avg_train_acc: 93.5129 | 


Epoch 3/8
----------
| 2025-10-16 19:24:13.055360 | Idx:   40/400    | loss: 0.272 | grad_norm: 0.530 | acc: 93.27% |
| 2025-10-16 19:24:14.435292 | Idx:   80/400    | loss: 0.254 | grad_norm: 0.459 | acc: 93.99% |
| 2025-10-16 19:24:17.201124 | Idx:  120/400    | loss: 0.249 | grad_norm: 0.230 | acc: 94.31% |
| 2025-10-16 19:24:18.567327 | Idx:  160/400    | loss: 0.248 | grad_norm: 0.399 | acc: 94.58% |
| 2025-10-16 19:24:19.940971 | Idx:  200/400    | loss: 0.244 | grad_norm: 0.668 | acc: 94.64% |
| 2025-10-16 19:24:22.749684 | Idx:  240/400    | loss: 0.240 | grad_norm: 2.983 | acc: 93.49% |
| 2025-10-16 19:24:24.089717 | Idx:  280/400    | loss: 0.237 | grad_norm: 0.165 | acc: 94.84% |
| 2025-10-16 19:24:27.120471 | Idx:  320/400    | loss: 0.234 | grad_norm: 0.391 | acc: 94.78% |
| 2025-10-16 19:24:28.476118 | Idx:  360/400    | loss: 0.240 | grad_norm: 0.178 | acc: 94.37% |
| 2025-10-16 19:24:29.810639 | Idx:  400/400    | loss: 0.245 | grad_norm: 2.134 | acc: 94.75% |
Train Loss: 0.2449 Acc: 93.83%
Acc: 96.33%


| val_epoch_acc: 96.33% | epoch: 02 | avg_train_loss: 0.2932 | avg_train_acc: 93.3362 | 


Epoch 4/8
----------
| 2025-10-16 19:24:56.264319 | Idx:   40/400    | loss: 0.274 | grad_norm: 1.090 | acc: 93.43% |
| 2025-10-16 19:24:57.626211 | Idx:   80/400    | loss: 0.253 | grad_norm: 0.353 | acc: 95.60% |
| 2025-10-16 19:25:00.382140 | Idx:  120/400    | loss: 0.246 | grad_norm: 0.415 | acc: 94.58% |
| 2025-10-16 19:25:01.719947 | Idx:  160/400    | loss: 0.233 | grad_norm: 0.457 | acc: 94.92% |
| 2025-10-16 19:25:03.060188 | Idx:  200/400    | loss: 0.234 | grad_norm: 7.145 | acc: 89.26% |
| 2025-10-16 19:25:05.793923 | Idx:  240/400    | loss: 0.253 | grad_norm: 5.925 | acc: 88.92% |
| 2025-10-16 19:25:07.213842 | Idx:  280/400    | loss: 0.251 | grad_norm: 1.890 | acc: 87.89% |
| 2025-10-16 19:25:09.896596 | Idx:  320/400    | loss: 0.245 | grad_norm: 0.174 | acc: 95.49% |
| 2025-10-16 19:25:11.170451 | Idx:  360/400    | loss: 0.242 | grad_norm: 1.041 | acc: 95.07% |
| 2025-10-16 19:25:12.490171 | Idx:  400/400    | loss: 0.238 | grad_norm: 0.263 | acc: 96.16% |
Train Loss: 0.2385 Acc: 94.09%
Acc: 95.67%


| val_epoch_acc: 95.67% | epoch: 03 | avg_train_loss: 0.1998 | avg_train_acc: 94.8421 | 


Epoch 5/8
----------
| 2025-10-16 19:25:39.062295 | Idx:   40/400    | loss: 0.242 | grad_norm: 0.433 | acc: 96.17% |
| 2025-10-16 19:25:40.418857 | Idx:   80/400    | loss: 0.246 | grad_norm: 0.333 | acc: 94.74% |
| 2025-10-16 19:25:43.152079 | Idx:  120/400    | loss: 0.238 | grad_norm: 0.592 | acc: 94.00% |
| 2025-10-16 19:25:44.538071 | Idx:  160/400    | loss: 0.236 | grad_norm: 0.315 | acc: 95.10% |
| 2025-10-16 19:25:45.888031 | Idx:  200/400    | loss: 0.232 | grad_norm: 0.219 | acc: 95.65% |
| 2025-10-16 19:25:48.649313 | Idx:  240/400    | loss: 0.235 | grad_norm: 0.731 | acc: 94.77% |
| 2025-10-16 19:25:50.012579 | Idx:  280/400    | loss: 0.230 | grad_norm: 0.175 | acc: 96.02% |
| 2025-10-16 19:25:52.772310 | Idx:  320/400    | loss: 0.238 | grad_norm: 0.443 | acc: 95.91% |
| 2025-10-16 19:25:54.123789 | Idx:  360/400    | loss: 0.234 | grad_norm: 0.505 | acc: 94.93% |
| 2025-10-16 19:25:55.456927 | Idx:  400/400    | loss: 0.232 | grad_norm: 0.099 | acc: 95.40% |
Train Loss: 0.2315 Acc: 94.24%
Acc: 95.51%


| val_epoch_acc: 95.51% | epoch: 04 | avg_train_loss: 0.2061 | avg_train_acc: 94.4792 | 


Epoch 6/8
----------
| 2025-10-16 19:26:21.839916 | Idx:   40/400    | loss: 0.268 | grad_norm: 0.103 | acc: 94.95% |
| 2025-10-16 19:26:23.238367 | Idx:   80/400    | loss: 0.249 | grad_norm: 1.214 | acc: 92.97% |
| 2025-10-16 19:26:26.029366 | Idx:  120/400    | loss: 0.264 | grad_norm: 0.565 | acc: 95.12% |
| 2025-10-16 19:26:27.411210 | Idx:  160/400    | loss: 0.250 | grad_norm: 0.280 | acc: 94.81% |
| 2025-10-16 19:26:28.772849 | Idx:  200/400    | loss: 0.262 | grad_norm: 0.706 | acc: 94.10% |
| 2025-10-16 19:26:31.720306 | Idx:  240/400    | loss: 0.258 | grad_norm: 0.850 | acc: 94.56% |
| 2025-10-16 19:26:33.118519 | Idx:  280/400    | loss: 0.257 | grad_norm: 0.462 | acc: 93.87% |
| 2025-10-16 19:26:36.012902 | Idx:  320/400    | loss: 0.251 | grad_norm: 0.365 | acc: 96.15% |
| 2025-10-16 19:26:37.367955 | Idx:  360/400    | loss: 0.253 | grad_norm: 0.589 | acc: 94.88% |
| 2025-10-16 19:26:38.715596 | Idx:  400/400    | loss: 0.252 | grad_norm: 0.420 | acc: 95.56% |
Train Loss: 0.2519 Acc: 94.26%
Acc: 95.37%


| val_epoch_acc: 95.37% | epoch: 05 | avg_train_loss: 0.1938 | avg_train_acc: 95.0106 | 


Epoch 7/8
----------
| 2025-10-16 19:27:05.234512 | Idx:   40/400    | loss: 0.183 | grad_norm: 0.227 | acc: 95.61% |
| 2025-10-16 19:27:06.629633 | Idx:   80/400    | loss: 0.229 | grad_norm: 0.226 | acc: 94.46% |
| 2025-10-16 19:27:09.396720 | Idx:  120/400    | loss: 0.234 | grad_norm: 1.012 | acc: 93.21% |
| 2025-10-16 19:27:10.797131 | Idx:  160/400    | loss: 0.235 | grad_norm: 0.205 | acc: 94.92% |
| 2025-10-16 19:27:12.078941 | Idx:  200/400    | loss: 0.232 | grad_norm: 1.356 | acc: 90.88% |
| 2025-10-16 19:27:14.847057 | Idx:  240/400    | loss: 0.233 | grad_norm: 0.170 | acc: 95.27% |
| 2025-10-16 19:27:16.201125 | Idx:  280/400    | loss: 0.230 | grad_norm: 2.170 | acc: 94.73% |
| 2025-10-16 19:27:18.997498 | Idx:  320/400    | loss: 0.224 | grad_norm: 0.476 | acc: 96.17% |
| 2025-10-16 19:27:20.337118 | Idx:  360/400    | loss: 0.225 | grad_norm: 0.082 | acc: 95.54% |
| 2025-10-16 19:27:21.683496 | Idx:  400/400    | loss: 0.222 | grad_norm: 0.600 | acc: 95.09% |
Train Loss: 0.2218 Acc: 94.53%
Acc: 95.25%


| val_epoch_acc: 95.25% | epoch: 06 | avg_train_loss: 0.1965 | avg_train_acc: 94.8245 | 


Epoch 8/8
----------
| 2025-10-16 19:27:48.384758 | Idx:   40/400    | loss: 0.237 | grad_norm: 0.469 | acc: 95.00% |
| 2025-10-16 19:27:49.750901 | Idx:   80/400    | loss: 0.231 | grad_norm: 0.934 | acc: 96.78% |
| 2025-10-16 19:27:52.485544 | Idx:  120/400    | loss: 0.225 | grad_norm: 0.192 | acc: 95.73% |
| 2025-10-16 19:27:53.819986 | Idx:  160/400    | loss: 0.215 | grad_norm: 0.267 | acc: 95.45% |
| 2025-10-16 19:27:55.150877 | Idx:  200/400    | loss: 0.222 | grad_norm: 0.369 | acc: 94.53% |
| 2025-10-16 19:27:58.028605 | Idx:  240/400    | loss: 0.231 | grad_norm: 1.289 | acc: 95.68% |
| 2025-10-16 19:27:59.386292 | Idx:  280/400    | loss: 0.235 | grad_norm: 0.743 | acc: 94.15% |
| 2025-10-16 19:28:02.152879 | Idx:  320/400    | loss: 0.233 | grad_norm: 0.061 | acc: 95.83% |
| 2025-10-16 19:28:03.483076 | Idx:  360/400    | loss: 0.230 | grad_norm: 0.292 | acc: 94.45% |
| 2025-10-16 19:28:04.803666 | Idx:  400/400    | loss: 0.234 | grad_norm: 0.546 | acc: 95.32% |
Train Loss: 0.2341 Acc: 94.28%
Acc: 95.86%


| val_epoch_acc: 95.86% | epoch: 07 | avg_train_loss: 0.2300 | avg_train_acc: 93.7673 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_192229-kzz424hj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_192229-kzz424hj\logs[0m
rule: B36/S23 \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_19-28-29_tiny_2_layer_seq_cnn__200-200-B36_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 1, 200, 200]          51
����BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
����ReLU: 1-3                              [1, 1, 200, 200]          --
����Conv2d: 1-4                            [1, 1, 200, 200]          26
����BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
����ReLU: 1-6                              [1, 1, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:28:45.050287 | Idx:   40/400    | loss: 1.027 | grad_norm: 0.840 | acc: 84.52% |
| 2025-10-16 19:28:46.388409 | Idx:   80/400    | loss: 0.824 | grad_norm: 0.988 | acc: 83.69% |
| 2025-10-16 19:28:49.120970 | Idx:  120/400    | loss: 0.737 | grad_norm: 0.392 | acc: 85.21% |
| 2025-10-16 19:28:50.451416 | Idx:  160/400    | loss: 0.679 | grad_norm: 0.609 | acc: 86.05% |
| 2025-10-16 19:28:51.785099 | Idx:  200/400    | loss: 0.642 | grad_norm: 0.491 | acc: 88.69% |
| 2025-10-16 19:28:54.737391 | Idx:  240/400    | loss: 0.614 | grad_norm: 0.262 | acc: 89.47% |
| 2025-10-16 19:28:56.093489 | Idx:  280/400    | loss: 0.591 | grad_norm: 1.233 | acc: 89.71% |
| 2025-10-16 19:28:58.908163 | Idx:  320/400    | loss: 0.570 | grad_norm: 1.902 | acc: 88.60% |
| 2025-10-16 19:29:00.290034 | Idx:  360/400    | loss: 0.559 | grad_norm: 1.302 | acc: 92.36% |
| 2025-10-16 19:29:01.631325 | Idx:  400/400    | loss: 0.545 | grad_norm: 0.291 | acc: 92.19% |
Train Loss: 0.5453 Acc: 85.58%
Acc: 91.40%


| val_epoch_acc: 91.40% | epoch: 00 | avg_train_loss: 0.4297 | avg_train_acc: 88.6692 | 


Epoch 2/8
----------
| 2025-10-16 19:29:27.550565 | Idx:   40/400    | loss: 0.424 | grad_norm: 0.744 | acc: 86.00% |
| 2025-10-16 19:29:28.907820 | Idx:   80/400    | loss: 0.404 | grad_norm: 0.571 | acc: 92.09% |
| 2025-10-16 19:29:31.670761 | Idx:  120/400    | loss: 0.388 | grad_norm: 0.224 | acc: 91.91% |
| 2025-10-16 19:29:33.018291 | Idx:  160/400    | loss: 0.381 | grad_norm: 0.679 | acc: 89.98% |
| 2025-10-16 19:29:34.351287 | Idx:  200/400    | loss: 0.370 | grad_norm: 0.869 | acc: 88.89% |
| 2025-10-16 19:29:37.043235 | Idx:  240/400    | loss: 0.372 | grad_norm: 0.348 | acc: 92.92% |
| 2025-10-16 19:29:38.385337 | Idx:  280/400    | loss: 0.376 | grad_norm: 0.380 | acc: 91.37% |
| 2025-10-16 19:29:41.391385 | Idx:  320/400    | loss: 0.376 | grad_norm: 0.721 | acc: 89.71% |
| 2025-10-16 19:29:42.751387 | Idx:  360/400    | loss: 0.371 | grad_norm: 1.168 | acc: 87.78% |
| 2025-10-16 19:29:44.150665 | Idx:  400/400    | loss: 0.373 | grad_norm: 1.846 | acc: 93.23% |
Train Loss: 0.3729 Acc: 90.25%
Acc: 92.03%


| val_epoch_acc: 92.03% | epoch: 01 | avg_train_loss: 0.3656 | avg_train_acc: 90.5027 | 


Epoch 3/8
----------
| 2025-10-16 19:30:10.180072 | Idx:   40/400    | loss: 0.416 | grad_norm: 1.169 | acc: 91.64% |
| 2025-10-16 19:30:11.520573 | Idx:   80/400    | loss: 0.386 | grad_norm: 0.893 | acc: 90.35% |
| 2025-10-16 19:30:14.240438 | Idx:  120/400    | loss: 0.377 | grad_norm: 0.849 | acc: 91.90% |
| 2025-10-16 19:30:15.587493 | Idx:  160/400    | loss: 0.375 | grad_norm: 0.726 | acc: 91.41% |
| 2025-10-16 19:30:16.940418 | Idx:  200/400    | loss: 0.367 | grad_norm: 1.283 | acc: 87.19% |
| 2025-10-16 19:30:19.684642 | Idx:  240/400    | loss: 0.363 | grad_norm: 1.639 | acc: 90.58% |
| 2025-10-16 19:30:21.019705 | Idx:  280/400    | loss: 0.364 | grad_norm: 1.144 | acc: 94.18% |
| 2025-10-16 19:30:24.019118 | Idx:  320/400    | loss: 0.359 | grad_norm: 0.371 | acc: 92.49% |
| 2025-10-16 19:30:25.352372 | Idx:  360/400    | loss: 0.364 | grad_norm: 0.519 | acc: 91.75% |
| 2025-10-16 19:30:26.672163 | Idx:  400/400    | loss: 0.369 | grad_norm: 2.579 | acc: 88.53% |
Train Loss: 0.3695 Acc: 90.35%
Acc: 92.17%


| val_epoch_acc: 92.17% | epoch: 02 | avg_train_loss: 0.4491 | avg_train_acc: 89.5551 | 


Epoch 4/8
----------
| 2025-10-16 19:30:52.868627 | Idx:   40/400    | loss: 0.364 | grad_norm: 0.760 | acc: 91.88% |
| 2025-10-16 19:30:54.222572 | Idx:   80/400    | loss: 0.356 | grad_norm: 0.205 | acc: 91.49% |
| 2025-10-16 19:30:57.040650 | Idx:  120/400    | loss: 0.348 | grad_norm: 0.225 | acc: 92.74% |
| 2025-10-16 19:30:58.409758 | Idx:  160/400    | loss: 0.361 | grad_norm: 4.026 | acc: 86.29% |
| 2025-10-16 19:30:59.754634 | Idx:  200/400    | loss: 0.361 | grad_norm: 0.457 | acc: 91.57% |
| 2025-10-16 19:31:02.535985 | Idx:  240/400    | loss: 0.370 | grad_norm: 0.776 | acc: 91.74% |
| 2025-10-16 19:31:03.887317 | Idx:  280/400    | loss: 0.362 | grad_norm: 0.448 | acc: 92.49% |
| 2025-10-16 19:31:06.659693 | Idx:  320/400    | loss: 0.360 | grad_norm: 0.146 | acc: 92.93% |
| 2025-10-16 19:31:07.989764 | Idx:  360/400    | loss: 0.354 | grad_norm: 1.091 | acc: 93.51% |
| 2025-10-16 19:31:09.330546 | Idx:  400/400    | loss: 0.352 | grad_norm: 3.223 | acc: 87.58% |
Train Loss: 0.3522 Acc: 90.81%
Acc: 90.84%


| val_epoch_acc: 90.84% | epoch: 03 | avg_train_loss: 0.3370 | avg_train_acc: 91.1088 | 


Epoch 5/8
----------
| 2025-10-16 19:31:35.757206 | Idx:   40/400    | loss: 0.303 | grad_norm: 0.662 | acc: 92.01% |
| 2025-10-16 19:31:37.106169 | Idx:   80/400    | loss: 0.319 | grad_norm: 0.986 | acc: 92.11% |
| 2025-10-16 19:31:39.836616 | Idx:  120/400    | loss: 0.328 | grad_norm: 0.714 | acc: 90.60% |
| 2025-10-16 19:31:41.174753 | Idx:  160/400    | loss: 0.335 | grad_norm: 5.072 | acc: 81.60% |
| 2025-10-16 19:31:42.502352 | Idx:  200/400    | loss: 0.332 | grad_norm: 0.412 | acc: 93.03% |
| 2025-10-16 19:31:45.239100 | Idx:  240/400    | loss: 0.336 | grad_norm: 0.817 | acc: 93.41% |
| 2025-10-16 19:31:46.569335 | Idx:  280/400    | loss: 0.333 | grad_norm: 0.533 | acc: 93.24% |
| 2025-10-16 19:31:49.307174 | Idx:  320/400    | loss: 0.334 | grad_norm: 0.331 | acc: 92.97% |
| 2025-10-16 19:31:50.635391 | Idx:  360/400    | loss: 0.336 | grad_norm: 1.640 | acc: 86.34% |
| 2025-10-16 19:31:51.964720 | Idx:  400/400    | loss: 0.333 | grad_norm: 0.210 | acc: 92.38% |
Train Loss: 0.3326 Acc: 90.94%
Acc: 92.35%


| val_epoch_acc: 92.35% | epoch: 04 | avg_train_loss: 0.2967 | avg_train_acc: 91.5253 | 


Epoch 6/8
----------
| 2025-10-16 19:32:18.677121 | Idx:   40/400    | loss: 0.345 | grad_norm: 0.438 | acc: 92.57% |
| 2025-10-16 19:32:20.024393 | Idx:   80/400    | loss: 0.326 | grad_norm: 0.836 | acc: 90.02% |
| 2025-10-16 19:32:22.872041 | Idx:  120/400    | loss: 0.356 | grad_norm: 0.946 | acc: 88.64% |
| 2025-10-16 19:32:24.208868 | Idx:  160/400    | loss: 0.360 | grad_norm: 0.360 | acc: 93.27% |
| 2025-10-16 19:32:25.539332 | Idx:  200/400    | loss: 0.348 | grad_norm: 0.542 | acc: 92.36% |
| 2025-10-16 19:32:28.258395 | Idx:  240/400    | loss: 0.350 | grad_norm: 1.134 | acc: 88.13% |
| 2025-10-16 19:32:29.603071 | Idx:  280/400    | loss: 0.343 | grad_norm: 1.193 | acc: 91.19% |
| 2025-10-16 19:32:32.357492 | Idx:  320/400    | loss: 0.349 | grad_norm: 0.241 | acc: 93.60% |
| 2025-10-16 19:32:33.690796 | Idx:  360/400    | loss: 0.353 | grad_norm: 0.654 | acc: 92.86% |
| 2025-10-16 19:32:35.009878 | Idx:  400/400    | loss: 0.349 | grad_norm: 0.124 | acc: 91.36% |
Train Loss: 0.3487 Acc: 90.99%
Acc: 92.03%


| val_epoch_acc: 92.03% | epoch: 05 | avg_train_loss: 0.3322 | avg_train_acc: 91.4387 | 


Epoch 7/8
----------
| 2025-10-16 19:33:01.504734 | Idx:   40/400    | loss: 0.328 | grad_norm: 0.459 | acc: 92.87% |
| 2025-10-16 19:33:02.843372 | Idx:   80/400    | loss: 0.339 | grad_norm: 7.878 | acc: 75.64% |
| 2025-10-16 19:33:05.570931 | Idx:  120/400    | loss: 0.347 | grad_norm: 1.683 | acc: 91.44% |
| 2025-10-16 19:33:06.909479 | Idx:  160/400    | loss: 0.349 | grad_norm: 0.429 | acc: 92.59% |
| 2025-10-16 19:33:08.274014 | Idx:  200/400    | loss: 0.340 | grad_norm: 1.744 | acc: 85.87% |
| 2025-10-16 19:33:11.037928 | Idx:  240/400    | loss: 0.340 | grad_norm: 0.921 | acc: 89.30% |
| 2025-10-16 19:33:12.376799 | Idx:  280/400    | loss: 0.338 | grad_norm: 0.837 | acc: 92.74% |
| 2025-10-16 19:33:15.106027 | Idx:  320/400    | loss: 0.338 | grad_norm: 0.647 | acc: 92.29% |
| 2025-10-16 19:33:16.455884 | Idx:  360/400    | loss: 0.336 | grad_norm: 0.554 | acc: 91.92% |
| 2025-10-16 19:33:17.783827 | Idx:  400/400    | loss: 0.338 | grad_norm: 2.076 | acc: 89.70% |
Train Loss: 0.3375 Acc: 90.97%
Acc: 86.05%


| val_epoch_acc: 86.05% | epoch: 06 | avg_train_loss: 0.3395 | avg_train_acc: 91.3513 | 


Epoch 8/8
----------
| 2025-10-16 19:33:44.257696 | Idx:   40/400    | loss: 0.362 | grad_norm: 2.894 | acc: 88.42% |
| 2025-10-16 19:33:45.587264 | Idx:   80/400    | loss: 0.364 | grad_norm: 0.337 | acc: 93.17% |
| 2025-10-16 19:33:48.304574 | Idx:  120/400    | loss: 0.334 | grad_norm: 0.174 | acc: 92.22% |
| 2025-10-16 19:33:49.538140 | Idx:  160/400    | loss: 0.328 | grad_norm: 0.301 | acc: 93.76% |
| 2025-10-16 19:33:50.767641 | Idx:  200/400    | loss: 0.333 | grad_norm: 0.828 | acc: 92.19% |
| 2025-10-16 19:33:53.395169 | Idx:  240/400    | loss: 0.328 | grad_norm: 0.991 | acc: 90.66% |
| 2025-10-16 19:33:54.617974 | Idx:  280/400    | loss: 0.329 | grad_norm: 0.088 | acc: 92.08% |
| 2025-10-16 19:33:57.207616 | Idx:  320/400    | loss: 0.328 | grad_norm: 2.959 | acc: 90.81% |
| 2025-10-16 19:33:58.419246 | Idx:  360/400    | loss: 0.330 | grad_norm: 0.735 | acc: 93.12% |
| 2025-10-16 19:33:59.618597 | Idx:  400/400    | loss: 0.338 | grad_norm: 0.886 | acc: 92.92% |
Train Loss: 0.3379 Acc: 91.24%
Acc: 92.20%


| val_epoch_acc: 92.20% | epoch: 07 | avg_train_loss: 0.4008 | avg_train_acc: 90.6846 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_192830-lehyga3h[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_192830-lehyga3h\logs[0m
rule: B3678/S34678 \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_19-34-23_tiny_2_layer_seq_cnn__200-200-B3678_S34678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 1, 200, 200]          51
����BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
����ReLU: 1-3                              [1, 1, 200, 200]          --
����Conv2d: 1-4                            [1, 1, 200, 200]          26
����BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
����ReLU: 1-6                              [1, 1, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:34:38.387386 | Idx:   40/400    | loss: 0.895 | grad_norm: 0.198 | acc: 89.50% |
| 2025-10-16 19:34:39.722076 | Idx:   80/400    | loss: 0.688 | grad_norm: 0.630 | acc: 82.39% |
| 2025-10-16 19:34:42.556259 | Idx:  120/400    | loss: 0.598 | grad_norm: 0.164 | acc: 89.52% |
| 2025-10-16 19:34:43.927418 | Idx:  160/400    | loss: 0.556 | grad_norm: 0.357 | acc: 94.07% |
| 2025-10-16 19:34:45.254923 | Idx:  200/400    | loss: 0.526 | grad_norm: 0.458 | acc: 90.87% |
| 2025-10-16 19:34:48.173453 | Idx:  240/400    | loss: 0.504 | grad_norm: 0.198 | acc: 93.92% |
| 2025-10-16 19:34:49.506580 | Idx:  280/400    | loss: 0.491 | grad_norm: 0.500 | acc: 86.80% |
| 2025-10-16 19:34:52.162218 | Idx:  320/400    | loss: 0.476 | grad_norm: 0.770 | acc: 89.85% |
| 2025-10-16 19:34:53.499356 | Idx:  360/400    | loss: 0.465 | grad_norm: 0.370 | acc: 88.69% |
| 2025-10-16 19:34:54.828369 | Idx:  400/400    | loss: 0.458 | grad_norm: 0.850 | acc: 86.39% |
Train Loss: 0.4582 Acc: 89.86%
Acc: 89.04%


| val_epoch_acc: 89.04% | epoch: 00 | avg_train_loss: 0.3931 | avg_train_acc: 90.5702 | 


Epoch 2/8
----------
| 2025-10-16 19:35:20.509105 | Idx:   40/400    | loss: 0.374 | grad_norm: 0.096 | acc: 96.71% |
| 2025-10-16 19:35:21.842896 | Idx:   80/400    | loss: 0.383 | grad_norm: 0.377 | acc: 86.41% |
| 2025-10-16 19:35:24.541049 | Idx:  120/400    | loss: 0.383 | grad_norm: 0.444 | acc: 95.57% |
| 2025-10-16 19:35:25.874768 | Idx:  160/400    | loss: 0.384 | grad_norm: 0.432 | acc: 91.30% |
| 2025-10-16 19:35:27.209087 | Idx:  200/400    | loss: 0.383 | grad_norm: 0.387 | acc: 93.33% |
| 2025-10-16 19:35:29.967667 | Idx:  240/400    | loss: 0.386 | grad_norm: 1.789 | acc: 77.45% |
| 2025-10-16 19:35:31.310881 | Idx:  280/400    | loss: 0.388 | grad_norm: 0.147 | acc: 93.13% |
| 2025-10-16 19:35:34.315123 | Idx:  320/400    | loss: 0.387 | grad_norm: 0.442 | acc: 91.96% |
| 2025-10-16 19:35:35.654971 | Idx:  360/400    | loss: 0.387 | grad_norm: 0.389 | acc: 95.22% |
| 2025-10-16 19:35:36.984250 | Idx:  400/400    | loss: 0.387 | grad_norm: 0.188 | acc: 87.16% |
Train Loss: 0.3873 Acc: 90.59%
Acc: 92.08%


| val_epoch_acc: 92.08% | epoch: 01 | avg_train_loss: 0.3938 | avg_train_acc: 90.2839 | 


Epoch 3/8
----------
| 2025-10-16 19:36:03.189291 | Idx:   40/400    | loss: 0.422 | grad_norm: 0.873 | acc: 85.12% |
| 2025-10-16 19:36:04.555980 | Idx:   80/400    | loss: 0.396 | grad_norm: 0.800 | acc: 93.33% |
| 2025-10-16 19:36:07.324237 | Idx:  120/400    | loss: 0.385 | grad_norm: 0.525 | acc: 90.66% |
| 2025-10-16 19:36:08.655667 | Idx:  160/400    | loss: 0.388 | grad_norm: 0.062 | acc: 93.20% |
| 2025-10-16 19:36:09.992046 | Idx:  200/400    | loss: 0.383 | grad_norm: 0.888 | acc: 90.80% |
| 2025-10-16 19:36:12.744134 | Idx:  240/400    | loss: 0.385 | grad_norm: 0.260 | acc: 90.60% |
| 2025-10-16 19:36:14.074991 | Idx:  280/400    | loss: 0.381 | grad_norm: 0.179 | acc: 84.90% |
| 2025-10-16 19:36:17.327521 | Idx:  320/400    | loss: 0.380 | grad_norm: 0.475 | acc: 92.29% |
| 2025-10-16 19:36:18.673032 | Idx:  360/400    | loss: 0.381 | grad_norm: 0.457 | acc: 91.64% |
| 2025-10-16 19:36:19.995728 | Idx:  400/400    | loss: 0.381 | grad_norm: 0.068 | acc: 93.25% |
Train Loss: 0.3806 Acc: 90.71%
Acc: 87.78%


| val_epoch_acc: 87.78% | epoch: 02 | avg_train_loss: 0.3772 | avg_train_acc: 91.2367 | 


Epoch 4/8
----------
| 2025-10-16 19:36:45.841166 | Idx:   40/400    | loss: 0.356 | grad_norm: 0.370 | acc: 88.69% |
| 2025-10-16 19:36:47.176403 | Idx:   80/400    | loss: 0.358 | grad_norm: 0.384 | acc: 93.17% |
| 2025-10-16 19:36:49.941344 | Idx:  120/400    | loss: 0.366 | grad_norm: 0.191 | acc: 93.54% |
| 2025-10-16 19:36:51.278793 | Idx:  160/400    | loss: 0.364 | grad_norm: 0.117 | acc: 93.77% |
| 2025-10-16 19:36:52.609494 | Idx:  200/400    | loss: 0.371 | grad_norm: 0.554 | acc: 94.38% |
| 2025-10-16 19:36:55.349635 | Idx:  240/400    | loss: 0.375 | grad_norm: 0.530 | acc: 89.92% |
| 2025-10-16 19:36:56.689671 | Idx:  280/400    | loss: 0.376 | grad_norm: 0.408 | acc: 91.75% |
| 2025-10-16 19:36:59.712729 | Idx:  320/400    | loss: 0.382 | grad_norm: 0.064 | acc: 90.02% |
| 2025-10-16 19:37:01.059229 | Idx:  360/400    | loss: 0.379 | grad_norm: 0.396 | acc: 92.22% |
| 2025-10-16 19:37:02.374824 | Idx:  400/400    | loss: 0.378 | grad_norm: 0.155 | acc: 89.43% |
Train Loss: 0.3779 Acc: 90.79%
Acc: 92.33%


| val_epoch_acc: 92.33% | epoch: 03 | avg_train_loss: 0.3744 | avg_train_acc: 91.0176 | 


Epoch 5/8
----------
| 2025-10-16 19:37:28.723369 | Idx:   40/400    | loss: 0.373 | grad_norm: 0.494 | acc: 94.28% |
| 2025-10-16 19:37:30.060443 | Idx:   80/400    | loss: 0.376 | grad_norm: 0.079 | acc: 87.17% |
| 2025-10-16 19:37:32.772385 | Idx:  120/400    | loss: 0.375 | grad_norm: 1.074 | acc: 91.48% |
| 2025-10-16 19:37:34.113222 | Idx:  160/400    | loss: 0.377 | grad_norm: 1.476 | acc: 92.95% |
| 2025-10-16 19:37:35.468118 | Idx:  200/400    | loss: 0.378 | grad_norm: 0.228 | acc: 92.42% |
| 2025-10-16 19:37:38.358801 | Idx:  240/400    | loss: 0.377 | grad_norm: 0.370 | acc: 84.71% |
| 2025-10-16 19:37:39.692061 | Idx:  280/400    | loss: 0.379 | grad_norm: 1.517 | acc: 93.34% |
| 2025-10-16 19:37:42.424320 | Idx:  320/400    | loss: 0.379 | grad_norm: 0.573 | acc: 86.88% |
| 2025-10-16 19:37:43.808047 | Idx:  360/400    | loss: 0.378 | grad_norm: 1.072 | acc: 87.80% |
| 2025-10-16 19:37:45.131394 | Idx:  400/400    | loss: 0.378 | grad_norm: 0.710 | acc: 88.05% |
Train Loss: 0.3776 Acc: 90.72%
Acc: 92.61%


| val_epoch_acc: 92.61% | epoch: 04 | avg_train_loss: 0.3738 | avg_train_acc: 90.8947 | 


Epoch 6/8
----------
| 2025-10-16 19:38:11.578750 | Idx:   40/400    | loss: 0.414 | grad_norm: 0.151 | acc: 86.35% |
| 2025-10-16 19:38:12.925962 | Idx:   80/400    | loss: 0.386 | grad_norm: 0.400 | acc: 92.38% |
| 2025-10-16 19:38:15.656754 | Idx:  120/400    | loss: 0.384 | grad_norm: 0.231 | acc: 91.96% |
| 2025-10-16 19:38:16.990123 | Idx:  160/400    | loss: 0.378 | grad_norm: 0.625 | acc: 94.16% |
| 2025-10-16 19:38:18.322410 | Idx:  200/400    | loss: 0.380 | grad_norm: 0.802 | acc: 94.30% |
| 2025-10-16 19:38:21.063698 | Idx:  240/400    | loss: 0.378 | grad_norm: 1.222 | acc: 89.57% |
| 2025-10-16 19:38:22.414673 | Idx:  280/400    | loss: 0.378 | grad_norm: 0.196 | acc: 90.76% |
| 2025-10-16 19:38:25.180713 | Idx:  320/400    | loss: 0.378 | grad_norm: 0.790 | acc: 89.28% |
| 2025-10-16 19:38:26.529626 | Idx:  360/400    | loss: 0.380 | grad_norm: 0.284 | acc: 90.09% |
| 2025-10-16 19:38:27.735412 | Idx:  400/400    | loss: 0.382 | grad_norm: 0.253 | acc: 95.88% |
Train Loss: 0.3824 Acc: 90.65%
Acc: 89.90%


| val_epoch_acc: 89.90% | epoch: 05 | avg_train_loss: 0.3971 | avg_train_acc: 90.6871 | 


Epoch 7/8
----------
| 2025-10-16 19:38:54.076961 | Idx:   40/400    | loss: 0.399 | grad_norm: 0.206 | acc: 97.39% |
| 2025-10-16 19:38:55.410245 | Idx:   80/400    | loss: 0.402 | grad_norm: 0.365 | acc: 89.94% |
| 2025-10-16 19:38:58.339681 | Idx:  120/400    | loss: 0.391 | grad_norm: 0.527 | acc: 95.58% |
| 2025-10-16 19:38:59.676510 | Idx:  160/400    | loss: 0.394 | grad_norm: 0.232 | acc: 89.31% |
| 2025-10-16 19:39:01.010628 | Idx:  200/400    | loss: 0.394 | grad_norm: 0.576 | acc: 90.63% |
| 2025-10-16 19:39:03.709466 | Idx:  240/400    | loss: 0.392 | grad_norm: 0.679 | acc: 85.59% |
| 2025-10-16 19:39:05.080453 | Idx:  280/400    | loss: 0.391 | grad_norm: 0.847 | acc: 90.03% |
| 2025-10-16 19:39:07.943499 | Idx:  320/400    | loss: 0.389 | grad_norm: 0.209 | acc: 93.65% |
| 2025-10-16 19:39:09.279122 | Idx:  360/400    | loss: 0.388 | grad_norm: 1.691 | acc: 86.27% |
| 2025-10-16 19:39:10.595643 | Idx:  400/400    | loss: 0.388 | grad_norm: 0.322 | acc: 91.02% |
Train Loss: 0.3883 Acc: 90.50%
Acc: 91.54%


| val_epoch_acc: 91.54% | epoch: 06 | avg_train_loss: 0.3918 | avg_train_acc: 90.2649 | 


Epoch 8/8
----------
| 2025-10-16 19:39:36.760431 | Idx:   40/400    | loss: 0.352 | grad_norm: 0.559 | acc: 89.75% |
| 2025-10-16 19:39:38.162984 | Idx:   80/400    | loss: 0.350 | grad_norm: 0.161 | acc: 89.21% |
| 2025-10-16 19:39:40.849748 | Idx:  120/400    | loss: 0.365 | grad_norm: 1.211 | acc: 93.85% |
| 2025-10-16 19:39:42.213222 | Idx:  160/400    | loss: 0.367 | grad_norm: 0.263 | acc: 90.04% |
| 2025-10-16 19:39:43.577124 | Idx:  200/400    | loss: 0.369 | grad_norm: 0.786 | acc: 92.63% |
| 2025-10-16 19:39:46.564653 | Idx:  240/400    | loss: 0.373 | grad_norm: 0.602 | acc: 92.23% |
| 2025-10-16 19:39:47.913771 | Idx:  280/400    | loss: 0.378 | grad_norm: 0.818 | acc: 87.69% |
| 2025-10-16 19:39:50.830395 | Idx:  320/400    | loss: 0.376 | grad_norm: 0.324 | acc: 90.69% |
| 2025-10-16 19:39:52.053400 | Idx:  360/400    | loss: 0.375 | grad_norm: 0.273 | acc: 93.62% |
| 2025-10-16 19:39:53.247798 | Idx:  400/400    | loss: 0.376 | grad_norm: 0.660 | acc: 92.98% |
Train Loss: 0.3757 Acc: 90.85%
Acc: 92.61%


| val_epoch_acc: 92.61% | epoch: 07 | avg_train_loss: 0.3826 | avg_train_acc: 90.5640 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_193423-0zufg1q7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_193423-0zufg1q7\logs[0m
rule: B35678/S5678 \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_19-40-18_tiny_2_layer_seq_cnn__200-200-B35678_S5678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
����Conv2d: 1-1                            [1, 1, 200, 200]          51
����BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
����ReLU: 1-3                              [1, 1, 200, 200]          --
����Conv2d: 1-4                            [1, 1, 200, 200]          26
����BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
����ReLU: 1-6                              [1, 1, 200, 200]          --
����Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:40:32.625186 | Idx:   40/400    | loss: 1.579 | grad_norm: 0.262 | acc: 99.52% |
| 2025-10-16 19:40:34.014822 | Idx:   80/400    | loss: 0.884 | grad_norm: 0.124 | acc: 98.00% |
| 2025-10-16 19:40:36.570743 | Idx:  120/400    | loss: 0.701 | grad_norm: 0.054 | acc: 99.51% |
| 2025-10-16 19:40:37.909555 | Idx:  160/400    | loss: 0.550 | grad_norm: 0.101 | acc: 99.54% |
| 2025-10-16 19:40:39.259705 | Idx:  200/400    | loss: 0.454 | grad_norm: 0.117 | acc: 96.69% |
| 2025-10-16 19:40:42.030838 | Idx:  240/400    | loss: 0.391 | grad_norm: 0.104 | acc: 96.86% |
| 2025-10-16 19:40:43.375338 | Idx:  280/400    | loss: 0.394 | grad_norm: 205.215 | acc: 18.29% |
| 2025-10-16 19:40:45.962704 | Idx:  320/400    | loss: 0.352 | grad_norm: 0.023 | acc: 98.56% |
| 2025-10-16 19:40:47.291381 | Idx:  360/400    | loss: 0.324 | grad_norm: 0.073 | acc: 98.96% |
| 2025-10-16 19:40:48.616714 | Idx:  400/400    | loss: 0.300 | grad_norm: 0.120 | acc: 98.91% |
Train Loss: 0.2998 Acc: 97.63%
Acc: 99.33%


| val_epoch_acc: 99.33% | epoch: 00 | avg_train_loss: 0.0928 | avg_train_acc: 98.3878 | 


Epoch 2/8
----------
| 2025-10-16 19:41:14.447133 | Idx:   40/400    | loss: 0.058 | grad_norm: 0.106 | acc: 99.12% |
| 2025-10-16 19:41:15.798554 | Idx:   80/400    | loss: 0.063 | grad_norm: 0.035 | acc: 99.79% |
| 2025-10-16 19:41:18.379931 | Idx:  120/400    | loss: 0.065 | grad_norm: 0.010 | acc: 99.97% |
| 2025-10-16 19:41:19.711111 | Idx:  160/400    | loss: 0.067 | grad_norm: 3.191 | acc: 98.82% |
| 2025-10-16 19:41:21.050015 | Idx:  200/400    | loss: 0.064 | grad_norm: 0.007 | acc: 99.99% |
| 2025-10-16 19:41:23.645158 | Idx:  240/400    | loss: 0.060 | grad_norm: 0.318 | acc: 98.21% |
| 2025-10-16 19:41:24.977072 | Idx:  280/400    | loss: 0.061 | grad_norm: 0.200 | acc: 98.59% |
| 2025-10-16 19:41:27.892588 | Idx:  320/400    | loss: 0.063 | grad_norm: 0.031 | acc: 99.78% |
| 2025-10-16 19:41:29.232482 | Idx:  360/400    | loss: 0.061 | grad_norm: 0.024 | acc: 99.10% |
| 2025-10-16 19:41:30.555682 | Idx:  400/400    | loss: 0.061 | grad_norm: 0.307 | acc: 99.78% |
Train Loss: 0.0607 Acc: 99.05%
Acc: 99.21%


| val_epoch_acc: 99.21% | epoch: 01 | avg_train_loss: 0.0572 | avg_train_acc: 99.0202 | 


Epoch 3/8
----------
| 2025-10-16 19:41:56.413286 | Idx:   40/400    | loss: 0.047 | grad_norm: 0.203 | acc: 98.79% |
| 2025-10-16 19:41:57.749147 | Idx:   80/400    | loss: 0.046 | grad_norm: 0.193 | acc: 99.22% |
| 2025-10-16 19:42:00.325035 | Idx:  120/400    | loss: 0.050 | grad_norm: 0.215 | acc: 98.37% |
| 2025-10-16 19:42:01.747342 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.176 | acc: 98.83% |
| 2025-10-16 19:42:03.128512 | Idx:  200/400    | loss: 0.049 | grad_norm: 0.011 | acc: 99.73% |
| 2025-10-16 19:42:05.793996 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.071 | acc: 99.78% |
| 2025-10-16 19:42:07.147671 | Idx:  280/400    | loss: 0.053 | grad_norm: 0.044 | acc: 99.83% |
| 2025-10-16 19:42:10.162285 | Idx:  320/400    | loss: 0.051 | grad_norm: 0.220 | acc: 98.56% |
| 2025-10-16 19:42:11.508491 | Idx:  360/400    | loss: 0.050 | grad_norm: 0.100 | acc: 99.38% |
| 2025-10-16 19:42:12.840320 | Idx:  400/400    | loss: 0.051 | grad_norm: 0.144 | acc: 98.41% |
Train Loss: 0.0509 Acc: 98.94%
Acc: 98.96%


| val_epoch_acc: 98.96% | epoch: 02 | avg_train_loss: 0.0643 | avg_train_acc: 99.0139 | 


Epoch 4/8
----------
| 2025-10-16 19:42:38.796471 | Idx:   40/400    | loss: 0.049 | grad_norm: 0.066 | acc: 99.65% |
| 2025-10-16 19:42:40.181109 | Idx:   80/400    | loss: 0.040 | grad_norm: 0.138 | acc: 99.85% |
| 2025-10-16 19:42:42.732219 | Idx:  120/400    | loss: 0.040 | grad_norm: 0.046 | acc: 99.37% |
| 2025-10-16 19:42:44.113594 | Idx:  160/400    | loss: 0.042 | grad_norm: 0.272 | acc: 98.17% |
| 2025-10-16 19:42:45.468278 | Idx:  200/400    | loss: 0.043 | grad_norm: 0.116 | acc: 99.42% |
| 2025-10-16 19:42:48.031119 | Idx:  240/400    | loss: 0.042 | grad_norm: 0.023 | acc: 99.47% |
| 2025-10-16 19:42:49.375761 | Idx:  280/400    | loss: 0.044 | grad_norm: 0.039 | acc: 99.52% |
| 2025-10-16 19:42:51.998992 | Idx:  320/400    | loss: 0.043 | grad_norm: 0.055 | acc: 99.96% |
| 2025-10-16 19:42:53.358951 | Idx:  360/400    | loss: 0.046 | grad_norm: 0.127 | acc: 98.59% |
| 2025-10-16 19:42:54.690440 | Idx:  400/400    | loss: 0.046 | grad_norm: 0.096 | acc: 99.20% |
Train Loss: 0.0455 Acc: 99.20%
Acc: 99.11%


| val_epoch_acc: 99.11% | epoch: 03 | avg_train_loss: 0.0401 | avg_train_acc: 99.2169 | 


Epoch 5/8
----------
| 2025-10-16 19:43:20.944631 | Idx:   40/400    | loss: 0.046 | grad_norm: 0.102 | acc: 97.39% |
| 2025-10-16 19:43:22.277622 | Idx:   80/400    | loss: 0.717 | grad_norm: 1.302 | acc: 95.85% |
| 2025-10-16 19:43:24.892645 | Idx:  120/400    | loss: 0.494 | grad_norm: 0.023 | acc: 99.64% |
| 2025-10-16 19:43:26.288828 | Idx:  160/400    | loss: 0.388 | grad_norm: 0.369 | acc: 97.64% |
| 2025-10-16 19:43:27.627887 | Idx:  200/400    | loss: 0.338 | grad_norm: 0.018 | acc: 99.78% |
| 2025-10-16 19:43:30.352702 | Idx:  240/400    | loss: 0.291 | grad_norm: 0.145 | acc: 99.23% |
| 2025-10-16 19:43:31.746891 | Idx:  280/400    | loss: 0.256 | grad_norm: 0.019 | acc: 98.93% |
| 2025-10-16 19:43:34.331210 | Idx:  320/400    | loss: 0.229 | grad_norm: 0.012 | acc: 99.53% |
| 2025-10-16 19:43:35.665387 | Idx:  360/400    | loss: 0.209 | grad_norm: 0.013 | acc: 99.89% |
| 2025-10-16 19:43:36.987619 | Idx:  400/400    | loss: 0.192 | grad_norm: 0.225 | acc: 98.77% |
Train Loss: 0.1924 Acc: 98.24%
Acc: 99.43%


| val_epoch_acc: 99.43% | epoch: 04 | avg_train_loss: 0.0496 | avg_train_acc: 98.9904 | 


Epoch 6/8
----------
| 2025-10-16 19:44:03.079099 | Idx:   40/400    | loss: 0.067 | grad_norm: 0.093 | acc: 99.44% |
| 2025-10-16 19:44:04.408527 | Idx:   80/400    | loss: 0.053 | grad_norm: 0.080 | acc: 99.72% |
| 2025-10-16 19:44:06.964573 | Idx:  120/400    | loss: 0.060 | grad_norm: 0.228 | acc: 99.11% |
| 2025-10-16 19:44:08.296070 | Idx:  160/400    | loss: 0.055 | grad_norm: 0.007 | acc: 99.75% |
| 2025-10-16 19:44:09.629064 | Idx:  200/400    | loss: 0.053 | grad_norm: 0.127 | acc: 99.23% |
| 2025-10-16 19:44:12.312638 | Idx:  240/400    | loss: 0.050 | grad_norm: 0.098 | acc: 99.71% |
| 2025-10-16 19:44:13.645124 | Idx:  280/400    | loss: 0.049 | grad_norm: 0.010 | acc: 99.18% |
| 2025-10-16 19:44:16.232221 | Idx:  320/400    | loss: 0.048 | grad_norm: 0.299 | acc: 99.73% |
| 2025-10-16 19:44:17.566015 | Idx:  360/400    | loss: 0.046 | grad_norm: 0.070 | acc: 97.70% |
| 2025-10-16 19:44:18.880383 | Idx:  400/400    | loss: 0.044 | grad_norm: 0.061 | acc: 99.70% |
Train Loss: 0.0445 Acc: 99.23%
Acc: 99.33%


| val_epoch_acc: 99.33% | epoch: 05 | avg_train_loss: 0.0306 | avg_train_acc: 99.4035 | 


Epoch 7/8
----------
| 2025-10-16 19:44:44.981881 | Idx:   40/400    | loss: 0.043 | grad_norm: 0.087 | acc: 99.79% |
| 2025-10-16 19:44:46.327539 | Idx:   80/400    | loss: 0.039 | grad_norm: 0.020 | acc: 99.98% |
| 2025-10-16 19:44:48.881296 | Idx:  120/400    | loss: 0.038 | grad_norm: 0.024 | acc: 99.55% |
| 2025-10-16 19:44:50.229439 | Idx:  160/400    | loss: 0.038 | grad_norm: 0.334 | acc: 98.88% |
| 2025-10-16 19:44:51.580858 | Idx:  200/400    | loss: 0.044 | grad_norm: 0.127 | acc: 99.42% |
| 2025-10-16 19:44:54.187399 | Idx:  240/400    | loss: 0.044 | grad_norm: 0.153 | acc: 99.16% |
| 2025-10-16 19:44:55.532208 | Idx:  280/400    | loss: 0.045 | grad_norm: 0.014 | acc: 99.77% |
| 2025-10-16 19:44:58.098010 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.050 | acc: 99.55% |
| 2025-10-16 19:44:59.448043 | Idx:  360/400    | loss: 0.044 | grad_norm: 0.064 | acc: 99.71% |
| 2025-10-16 19:45:00.803390 | Idx:  400/400    | loss: 0.044 | grad_norm: 0.371 | acc: 98.31% |
Train Loss: 0.0443 Acc: 99.23%
Acc: 99.22%


| val_epoch_acc: 99.22% | epoch: 06 | avg_train_loss: 0.0439 | avg_train_acc: 99.2910 | 


Epoch 8/8
----------
| 2025-10-16 19:45:26.730532 | Idx:   40/400    | loss: 0.041 | grad_norm: 0.004 | acc: 99.97% |
| 2025-10-16 19:45:28.065446 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.152 | acc: 99.41% |
| 2025-10-16 19:45:30.709159 | Idx:  120/400    | loss: 0.040 | grad_norm: 0.180 | acc: 99.30% |
| 2025-10-16 19:45:32.045452 | Idx:  160/400    | loss: 0.046 | grad_norm: 0.282 | acc: 98.56% |
| 2025-10-16 19:45:33.401060 | Idx:  200/400    | loss: 0.050 | grad_norm: 0.122 | acc: 99.43% |
| 2025-10-16 19:45:35.980822 | Idx:  240/400    | loss: 0.048 | grad_norm: 0.042 | acc: 98.96% |
| 2025-10-16 19:45:37.359685 | Idx:  280/400    | loss: 0.047 | grad_norm: 0.004 | acc: 99.91% |
| 2025-10-16 19:45:39.927126 | Idx:  320/400    | loss: 0.046 | grad_norm: 0.016 | acc: 99.79% |
| 2025-10-16 19:45:41.264174 | Idx:  360/400    | loss: 0.047 | grad_norm: 0.270 | acc: 98.86% |
| 2025-10-16 19:45:42.588813 | Idx:  400/400    | loss: 0.048 | grad_norm: 0.042 | acc: 99.43% |
Train Loss: 0.0480 Acc: 99.17%
Acc: 99.33%


| val_epoch_acc: 99.33% | epoch: 07 | avg_train_loss: 0.0677 | avg_train_acc: 99.0328 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_194018-sg5gpc4e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_194018-sg5gpc4e\logs[0m
rule: B3/S23 \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_19-46-07_multiscale_0__200-200-B3_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
����Sequential: 1-1                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-1                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-3                    [1, 2, 200, 200]          --
����Sequential: 1-2                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-4                       [1, 2, 200, 200]          102
��    ����BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-6                    [1, 2, 200, 200]          --
����Sequential: 1-3                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-7                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-9                    [1, 2, 200, 200]          --
����Sequential: 1-4                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-10                      [1, 4, 200, 200]          604
��    ����BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
��    ����ReLU: 2-12                        [1, 4, 200, 200]          --
��    ����Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:46:25.170183 | Idx:   40/400    | loss: 0.736 | grad_norm: 0.631 | acc: 88.01% |
| 2025-10-16 19:46:28.974797 | Idx:   80/400    | loss: 0.570 | grad_norm: 0.491 | acc: 92.84% |
| 2025-10-16 19:46:34.108181 | Idx:  120/400    | loss: 0.485 | grad_norm: 0.371 | acc: 91.83% |
| 2025-10-16 19:46:37.910696 | Idx:  160/400    | loss: 0.447 | grad_norm: 0.334 | acc: 92.71% |
| 2025-10-16 19:46:41.710631 | Idx:  200/400    | loss: 0.409 | grad_norm: 3.713 | acc: 85.25% |
| 2025-10-16 19:46:47.113262 | Idx:  240/400    | loss: 0.373 | grad_norm: 0.291 | acc: 96.32% |
| 2025-10-16 19:46:50.925977 | Idx:  280/400    | loss: 0.354 | grad_norm: 0.126 | acc: 97.47% |
| 2025-10-16 19:46:56.192122 | Idx:  320/400    | loss: 0.324 | grad_norm: 0.131 | acc: 98.49% |
| 2025-10-16 19:46:59.995763 | Idx:  360/400    | loss: 0.303 | grad_norm: 0.308 | acc: 97.07% |
| 2025-10-16 19:47:03.798672 | Idx:  400/400    | loss: 0.279 | grad_norm: 0.231 | acc: 99.80% |
Train Loss: 0.2789 Acc: 92.42%
Acc: 99.44%


| val_epoch_acc: 99.44% | epoch: 00 | avg_train_loss: 0.0552 | avg_train_acc: 98.7649 | 


Epoch 2/8
----------
| 2025-10-16 19:47:33.211119 | Idx:   40/400    | loss: 0.107 | grad_norm: 2.679 | acc: 98.87% |
| 2025-10-16 19:47:36.994428 | Idx:   80/400    | loss: 0.087 | grad_norm: 0.512 | acc: 97.37% |
| 2025-10-16 19:47:42.168981 | Idx:  120/400    | loss: 0.106 | grad_norm: 1.395 | acc: 97.59% |
| 2025-10-16 19:47:46.004326 | Idx:  160/400    | loss: 0.103 | grad_norm: 0.084 | acc: 99.79% |
| 2025-10-16 19:47:49.831251 | Idx:  200/400    | loss: 0.099 | grad_norm: 0.137 | acc: 99.86% |
| 2025-10-16 19:47:54.991659 | Idx:  240/400    | loss: 0.091 | grad_norm: 0.115 | acc: 99.77% |
| 2025-10-16 19:47:58.787553 | Idx:  280/400    | loss: 0.083 | grad_norm: 1.117 | acc: 95.28% |
| 2025-10-16 19:48:04.292489 | Idx:  320/400    | loss: 0.098 | grad_norm: 0.346 | acc: 98.30% |
| 2025-10-16 19:48:08.182085 | Idx:  360/400    | loss: 0.093 | grad_norm: 0.153 | acc: 99.88% |
| 2025-10-16 19:48:11.988846 | Idx:  400/400    | loss: 0.091 | grad_norm: 0.136 | acc: 99.51% |
Train Loss: 0.0913 Acc: 98.50%
Acc: 99.65%


| val_epoch_acc: 99.65% | epoch: 01 | avg_train_loss: 0.0953 | avg_train_acc: 98.3466 | 


Epoch 3/8
----------
| 2025-10-16 19:48:41.211256 | Idx:   40/400    | loss: 0.021 | grad_norm: 0.061 | acc: 99.91% |
| 2025-10-16 19:48:45.031038 | Idx:   80/400    | loss: 0.022 | grad_norm: 0.203 | acc: 99.95% |
| 2025-10-16 19:48:50.245050 | Idx:  120/400    | loss: 0.049 | grad_norm: 0.018 | acc: 99.97% |
| 2025-10-16 19:48:54.029938 | Idx:  160/400    | loss: 0.052 | grad_norm: 0.537 | acc: 97.57% |
| 2025-10-16 19:48:57.848519 | Idx:  200/400    | loss: 0.051 | grad_norm: 0.417 | acc: 98.29% |
| 2025-10-16 19:49:03.094168 | Idx:  240/400    | loss: 0.047 | grad_norm: 1.225 | acc: 98.45% |
| 2025-10-16 19:49:06.895592 | Idx:  280/400    | loss: 0.057 | grad_norm: 0.064 | acc: 99.96% |
| 2025-10-16 19:49:12.386426 | Idx:  320/400    | loss: 0.051 | grad_norm: 0.044 | acc: 100.00% |
| 2025-10-16 19:49:16.277595 | Idx:  360/400    | loss: 0.049 | grad_norm: 0.024 | acc: 100.00% |
| 2025-10-16 19:49:20.257634 | Idx:  400/400    | loss: 0.047 | grad_norm: 0.064 | acc: 100.00% |
Train Loss: 0.0465 Acc: 99.34%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0152 | avg_train_acc: 99.8013 | 


Epoch 4/8
----------
| 2025-10-16 19:49:50.746885 | Idx:   40/400    | loss: 0.042 | grad_norm: 0.133 | acc: 99.72% |
| 2025-10-16 19:49:54.798290 | Idx:   80/400    | loss: 0.037 | grad_norm: 0.114 | acc: 99.84% |
| 2025-10-16 19:50:00.134613 | Idx:  120/400    | loss: 0.052 | grad_norm: 0.648 | acc: 97.67% |
| 2025-10-16 19:50:03.977682 | Idx:  160/400    | loss: 0.067 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 19:50:07.844609 | Idx:  200/400    | loss: 0.055 | grad_norm: 0.123 | acc: 100.00% |
| 2025-10-16 19:50:13.149718 | Idx:  240/400    | loss: 0.060 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-16 19:50:16.960262 | Idx:  280/400    | loss: 0.064 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-16 19:50:22.208678 | Idx:  320/400    | loss: 0.068 | grad_norm: 0.023 | acc: 99.97% |
| 2025-10-16 19:50:26.012329 | Idx:  360/400    | loss: 0.064 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 19:50:29.771278 | Idx:  400/400    | loss: 0.059 | grad_norm: 0.487 | acc: 99.35% |
Train Loss: 0.0588 Acc: 99.32%
Acc: 95.81%


| val_epoch_acc: 95.81% | epoch: 03 | avg_train_loss: 0.0173 | avg_train_acc: 99.7279 | 


Epoch 5/8
----------
| 2025-10-16 19:50:59.447387 | Idx:   40/400    | loss: 0.047 | grad_norm: 0.054 | acc: 99.94% |
| 2025-10-16 19:51:03.472682 | Idx:   80/400    | loss: 0.031 | grad_norm: 0.113 | acc: 99.93% |
| 2025-10-16 19:51:08.953833 | Idx:  120/400    | loss: 0.052 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-16 19:51:12.966435 | Idx:  160/400    | loss: 0.065 | grad_norm: 0.027 | acc: 100.00% |
| 2025-10-16 19:51:16.980766 | Idx:  200/400    | loss: 0.078 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-16 19:51:22.415063 | Idx:  240/400    | loss: 0.066 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 19:51:26.414605 | Idx:  280/400    | loss: 0.060 | grad_norm: 0.006 | acc: 99.99% |
| 2025-10-16 19:51:31.850493 | Idx:  320/400    | loss: 0.054 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-16 19:51:35.844788 | Idx:  360/400    | loss: 0.049 | grad_norm: 0.022 | acc: 100.00% |
| 2025-10-16 19:51:39.790246 | Idx:  400/400    | loss: 0.051 | grad_norm: 16.106 | acc: 84.67% |
Train Loss: 0.0508 Acc: 99.56%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 04 | avg_train_loss: 0.0917 | avg_train_acc: 99.4847 | 


Epoch 6/8
----------
| 2025-10-16 19:52:07.683000 | Idx:   40/400    | loss: 0.021 | grad_norm: 0.396 | acc: 98.64% |
| 2025-10-16 19:52:10.809119 | Idx:   80/400    | loss: 0.076 | grad_norm: 0.030 | acc: 99.99% |
| 2025-10-16 19:52:15.242551 | Idx:  120/400    | loss: 0.054 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 19:52:18.363701 | Idx:  160/400    | loss: 0.043 | grad_norm: 0.157 | acc: 99.69% |
| 2025-10-16 19:52:21.493308 | Idx:  200/400    | loss: 0.039 | grad_norm: 0.024 | acc: 99.99% |
| 2025-10-16 19:52:26.023334 | Idx:  240/400    | loss: 0.047 | grad_norm: 0.105 | acc: 99.86% |
| 2025-10-16 19:52:29.149103 | Idx:  280/400    | loss: 0.044 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-16 19:52:33.566797 | Idx:  320/400    | loss: 0.046 | grad_norm: 0.033 | acc: 100.00% |
| 2025-10-16 19:52:36.693933 | Idx:  360/400    | loss: 0.045 | grad_norm: 0.017 | acc: 99.99% |
| 2025-10-16 19:52:39.816273 | Idx:  400/400    | loss: 0.050 | grad_norm: 6.872 | acc: 89.94% |
Train Loss: 0.0498 Acc: 99.47%
Acc: 97.94%


| val_epoch_acc: 97.94% | epoch: 05 | avg_train_loss: 0.1172 | avg_train_acc: 98.8043 | 


Epoch 7/8
----------
| 2025-10-16 19:53:06.881404 | Idx:   40/400    | loss: 0.144 | grad_norm: 16.812 | acc: 82.84% |
| 2025-10-16 19:53:09.997452 | Idx:   80/400    | loss: 0.075 | grad_norm: 0.022 | acc: 100.00% |
| 2025-10-16 19:53:14.426386 | Idx:  120/400    | loss: 0.059 | grad_norm: 0.407 | acc: 98.70% |
| 2025-10-16 19:53:17.544583 | Idx:  160/400    | loss: 0.053 | grad_norm: 0.402 | acc: 99.59% |
| 2025-10-16 19:53:20.665048 | Idx:  200/400    | loss: 0.058 | grad_norm: 0.132 | acc: 99.74% |
| 2025-10-16 19:53:25.106697 | Idx:  240/400    | loss: 0.072 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 19:53:28.225238 | Idx:  280/400    | loss: 0.064 | grad_norm: 0.340 | acc: 98.78% |
| 2025-10-16 19:53:32.670251 | Idx:  320/400    | loss: 0.058 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 19:53:35.796967 | Idx:  360/400    | loss: 0.053 | grad_norm: 0.031 | acc: 100.00% |
| 2025-10-16 19:53:38.926628 | Idx:  400/400    | loss: 0.054 | grad_norm: 0.540 | acc: 99.50% |
Train Loss: 0.0538 Acc: 99.60%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 06 | avg_train_loss: 0.0315 | avg_train_acc: 99.6790 | 


Epoch 8/8
----------
| 2025-10-16 19:54:06.018253 | Idx:   40/400    | loss: 0.006 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 19:54:09.152551 | Idx:   80/400    | loss: 0.018 | grad_norm: 0.157 | acc: 99.89% |
| 2025-10-16 19:54:13.609401 | Idx:  120/400    | loss: 0.020 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 19:54:16.724910 | Idx:  160/400    | loss: 0.016 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 19:54:19.855817 | Idx:  200/400    | loss: 0.013 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 19:54:24.333178 | Idx:  240/400    | loss: 0.020 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 19:54:27.465951 | Idx:  280/400    | loss: 0.023 | grad_norm: 0.075 | acc: 99.86% |
| 2025-10-16 19:54:31.893258 | Idx:  320/400    | loss: 0.023 | grad_norm: 0.014 | acc: 99.98% |
| 2025-10-16 19:54:35.023883 | Idx:  360/400    | loss: 0.040 | grad_norm: 0.053 | acc: 99.92% |
| 2025-10-16 19:54:38.132487 | Idx:  400/400    | loss: 0.041 | grad_norm: 0.004 | acc: 100.00% |
Train Loss: 0.0415 Acc: 99.62%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 07 | avg_train_loss: 0.0684 | avg_train_acc: 99.3350 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_194607-nw8d6zd2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_194607-nw8d6zd2\logs[0m
rule: B36/S23 \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_19-55-02_multiscale_0__200-200-B36_S23


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
����Sequential: 1-1                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-1                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-3                    [1, 2, 200, 200]          --
����Sequential: 1-2                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-4                       [1, 2, 200, 200]          102
��    ����BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-6                    [1, 2, 200, 200]          --
����Sequential: 1-3                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-7                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-9                    [1, 2, 200, 200]          --
����Sequential: 1-4                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-10                      [1, 4, 200, 200]          604
��    ����BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
��    ����ReLU: 2-12                        [1, 4, 200, 200]          --
��    ����Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 19:55:20.524898 | Idx:   40/400    | loss: 0.760 | grad_norm: 0.388 | acc: 83.71% |
| 2025-10-16 19:55:24.465215 | Idx:   80/400    | loss: 0.634 | grad_norm: 0.442 | acc: 88.14% |
| 2025-10-16 19:55:29.896857 | Idx:  120/400    | loss: 0.562 | grad_norm: 0.190 | acc: 87.83% |
| 2025-10-16 19:55:33.808095 | Idx:  160/400    | loss: 0.520 | grad_norm: 1.138 | acc: 84.86% |
| 2025-10-16 19:55:37.759067 | Idx:  200/400    | loss: 0.485 | grad_norm: 0.261 | acc: 90.91% |
| 2025-10-16 19:55:43.292821 | Idx:  240/400    | loss: 0.458 | grad_norm: 0.509 | acc: 88.48% |
| 2025-10-16 19:55:47.195251 | Idx:  280/400    | loss: 0.439 | grad_norm: 1.557 | acc: 88.59% |
| 2025-10-16 19:55:52.565916 | Idx:  320/400    | loss: 0.416 | grad_norm: 0.399 | acc: 91.63% |
| 2025-10-16 19:55:56.532591 | Idx:  360/400    | loss: 0.400 | grad_norm: 0.118 | acc: 93.27% |
| 2025-10-16 19:56:00.460363 | Idx:  400/400    | loss: 0.383 | grad_norm: 0.255 | acc: 93.56% |
Train Loss: 0.3829 Acc: 89.17%
Acc: 95.79%


| val_epoch_acc: 95.79% | epoch: 00 | avg_train_loss: 0.2375 | avg_train_acc: 93.2753 | 


Epoch 2/8
----------
| 2025-10-16 19:56:30.315557 | Idx:   40/400    | loss: 0.344 | grad_norm: 1.404 | acc: 90.78% |
| 2025-10-16 19:56:34.232871 | Idx:   80/400    | loss: 0.274 | grad_norm: 0.245 | acc: 93.06% |
| 2025-10-16 19:56:39.574941 | Idx:  120/400    | loss: 0.252 | grad_norm: 0.460 | acc: 93.74% |
| 2025-10-16 19:56:43.498750 | Idx:  160/400    | loss: 0.233 | grad_norm: 0.401 | acc: 97.78% |
| 2025-10-16 19:56:47.467359 | Idx:  200/400    | loss: 0.220 | grad_norm: 0.137 | acc: 96.69% |
| 2025-10-16 19:56:52.846940 | Idx:  240/400    | loss: 0.216 | grad_norm: 0.106 | acc: 96.93% |
| 2025-10-16 19:56:56.817211 | Idx:  280/400    | loss: 0.212 | grad_norm: 0.515 | acc: 98.86% |
| 2025-10-16 19:57:02.447152 | Idx:  320/400    | loss: 0.217 | grad_norm: 0.298 | acc: 96.96% |
| 2025-10-16 19:57:06.383065 | Idx:  360/400    | loss: 0.207 | grad_norm: 0.237 | acc: 97.52% |
| 2025-10-16 19:57:10.287402 | Idx:  400/400    | loss: 0.205 | grad_norm: 0.522 | acc: 95.00% |
Train Loss: 0.2049 Acc: 95.19%
Acc: 97.33%


| val_epoch_acc: 97.33% | epoch: 01 | avg_train_loss: 0.2048 | avg_train_acc: 95.7545 | 


Epoch 3/8
----------
| 2025-10-16 19:57:39.383015 | Idx:   40/400    | loss: 0.151 | grad_norm: 0.128 | acc: 97.81% |
| 2025-10-16 19:57:43.011057 | Idx:   80/400    | loss: 0.151 | grad_norm: 0.157 | acc: 98.52% |
| 2025-10-16 19:57:48.202106 | Idx:  120/400    | loss: 0.142 | grad_norm: 0.173 | acc: 98.78% |
| 2025-10-16 19:57:51.993721 | Idx:  160/400    | loss: 0.129 | grad_norm: 0.181 | acc: 98.98% |
| 2025-10-16 19:57:55.822297 | Idx:  200/400    | loss: 0.130 | grad_norm: 0.130 | acc: 98.61% |
| 2025-10-16 19:58:01.150315 | Idx:  240/400    | loss: 0.122 | grad_norm: 0.145 | acc: 99.33% |
| 2025-10-16 19:58:05.109609 | Idx:  280/400    | loss: 0.148 | grad_norm: 0.107 | acc: 98.54% |
| 2025-10-16 19:58:10.767422 | Idx:  320/400    | loss: 0.156 | grad_norm: 0.418 | acc: 96.11% |
| 2025-10-16 19:58:14.806943 | Idx:  360/400    | loss: 0.156 | grad_norm: 0.291 | acc: 99.25% |
| 2025-10-16 19:58:18.835620 | Idx:  400/400    | loss: 0.149 | grad_norm: 0.130 | acc: 99.12% |
Train Loss: 0.1486 Acc: 97.35%
Acc: 99.22%


| val_epoch_acc: 99.22% | epoch: 02 | avg_train_loss: 0.0749 | avg_train_acc: 98.6234 | 


Epoch 4/8
----------
| 2025-10-16 19:58:48.552387 | Idx:   40/400    | loss: 0.185 | grad_norm: 7.949 | acc: 93.37% |
| 2025-10-16 19:58:52.603633 | Idx:   80/400    | loss: 0.144 | grad_norm: 0.115 | acc: 98.80% |
| 2025-10-16 19:58:58.044732 | Idx:  120/400    | loss: 0.125 | grad_norm: 0.156 | acc: 98.95% |
| 2025-10-16 19:59:02.028244 | Idx:  160/400    | loss: 0.134 | grad_norm: 0.999 | acc: 97.98% |
| 2025-10-16 19:59:06.101065 | Idx:  200/400    | loss: 0.120 | grad_norm: 0.109 | acc: 98.95% |
| 2025-10-16 19:59:11.574805 | Idx:  240/400    | loss: 0.123 | grad_norm: 0.453 | acc: 99.19% |
| 2025-10-16 19:59:15.572830 | Idx:  280/400    | loss: 0.125 | grad_norm: 0.357 | acc: 99.40% |
| 2025-10-16 19:59:20.970647 | Idx:  320/400    | loss: 0.125 | grad_norm: 0.161 | acc: 99.23% |
| 2025-10-16 19:59:24.956950 | Idx:  360/400    | loss: 0.121 | grad_norm: 0.563 | acc: 99.22% |
| 2025-10-16 19:59:28.595177 | Idx:  400/400    | loss: 0.119 | grad_norm: 0.756 | acc: 95.11% |
Train Loss: 0.1189 Acc: 98.01%
Acc: 98.27%


| val_epoch_acc: 98.27% | epoch: 03 | avg_train_loss: 0.1144 | avg_train_acc: 98.1159 | 


Epoch 5/8
----------
| 2025-10-16 19:59:58.521273 | Idx:   40/400    | loss: 0.227 | grad_norm: 0.035 | acc: 99.21% |
| 2025-10-16 20:00:02.412027 | Idx:   80/400    | loss: 0.162 | grad_norm: 0.272 | acc: 99.43% |
| 2025-10-16 20:00:07.690808 | Idx:  120/400    | loss: 0.134 | grad_norm: 6.120 | acc: 90.81% |
| 2025-10-16 20:00:11.532561 | Idx:  160/400    | loss: 0.140 | grad_norm: 0.466 | acc: 97.23% |
| 2025-10-16 20:00:15.411993 | Idx:  200/400    | loss: 0.122 | grad_norm: 0.099 | acc: 99.23% |
| 2025-10-16 20:00:20.755215 | Idx:  240/400    | loss: 0.115 | grad_norm: 0.168 | acc: 99.50% |
| 2025-10-16 20:00:24.640104 | Idx:  280/400    | loss: 0.119 | grad_norm: 0.543 | acc: 96.92% |
| 2025-10-16 20:00:29.906360 | Idx:  320/400    | loss: 0.119 | grad_norm: 0.134 | acc: 98.85% |
| 2025-10-16 20:00:33.827324 | Idx:  360/400    | loss: 0.120 | grad_norm: 0.186 | acc: 99.09% |
| 2025-10-16 20:00:37.856240 | Idx:  400/400    | loss: 0.116 | grad_norm: 0.075 | acc: 99.37% |
Train Loss: 0.1165 Acc: 98.30%
Acc: 99.26%


| val_epoch_acc: 99.26% | epoch: 04 | avg_train_loss: 0.0870 | avg_train_acc: 98.5591 | 


Epoch 6/8
----------
| 2025-10-16 20:01:07.879215 | Idx:   40/400    | loss: 0.056 | grad_norm: 0.165 | acc: 99.23% |
| 2025-10-16 20:01:11.803282 | Idx:   80/400    | loss: 0.128 | grad_norm: 0.175 | acc: 99.28% |
| 2025-10-16 20:01:17.642479 | Idx:  120/400    | loss: 0.107 | grad_norm: 3.267 | acc: 98.46% |
| 2025-10-16 20:01:22.602639 | Idx:  160/400    | loss: 0.111 | grad_norm: 0.207 | acc: 98.47% |
| 2025-10-16 20:01:27.527923 | Idx:  200/400    | loss: 0.110 | grad_norm: 0.593 | acc: 97.12% |
| 2025-10-16 20:01:33.270410 | Idx:  240/400    | loss: 0.108 | grad_norm: 0.244 | acc: 98.73% |
| 2025-10-16 20:01:37.373979 | Idx:  280/400    | loss: 0.100 | grad_norm: 0.108 | acc: 99.22% |
| 2025-10-16 20:01:43.129137 | Idx:  320/400    | loss: 0.096 | grad_norm: 1.894 | acc: 92.73% |
| 2025-10-16 20:01:47.177443 | Idx:  360/400    | loss: 0.095 | grad_norm: 0.111 | acc: 99.39% |
| 2025-10-16 20:01:51.155457 | Idx:  400/400    | loss: 0.099 | grad_norm: 2.482 | acc: 95.23% |
Train Loss: 0.0993 Acc: 98.37%
Acc: 99.46%


| val_epoch_acc: 99.46% | epoch: 05 | avg_train_loss: 0.0827 | avg_train_acc: 98.5997 | 


Epoch 7/8
----------
| 2025-10-16 20:02:23.888171 | Idx:   40/400    | loss: 0.090 | grad_norm: 0.018 | acc: 99.27% |
| 2025-10-16 20:02:28.406291 | Idx:   80/400    | loss: 0.085 | grad_norm: 0.309 | acc: 98.88% |
| 2025-10-16 20:02:34.346396 | Idx:  120/400    | loss: 0.087 | grad_norm: 0.036 | acc: 99.12% |
| 2025-10-16 20:02:38.938026 | Idx:  160/400    | loss: 0.085 | grad_norm: 0.118 | acc: 99.34% |
| 2025-10-16 20:02:43.564564 | Idx:  200/400    | loss: 0.086 | grad_norm: 0.022 | acc: 99.32% |
| 2025-10-16 20:02:50.480913 | Idx:  240/400    | loss: 0.088 | grad_norm: 0.241 | acc: 99.53% |
| 2025-10-16 20:02:55.162765 | Idx:  280/400    | loss: 0.094 | grad_norm: 1.465 | acc: 93.48% |
| 2025-10-16 20:03:01.286134 | Idx:  320/400    | loss: 0.095 | grad_norm: 0.145 | acc: 99.36% |
| 2025-10-16 20:03:05.943902 | Idx:  360/400    | loss: 0.091 | grad_norm: 0.179 | acc: 99.29% |
| 2025-10-16 20:03:10.445743 | Idx:  400/400    | loss: 0.087 | grad_norm: 0.025 | acc: 99.41% |
Train Loss: 0.0869 Acc: 98.50%
Acc: 99.37%


| val_epoch_acc: 99.37% | epoch: 06 | avg_train_loss: 0.0562 | avg_train_acc: 98.9578 | 


Epoch 8/8
----------
| 2025-10-16 20:03:43.288793 | Idx:   40/400    | loss: 0.057 | grad_norm: 0.158 | acc: 99.58% |
| 2025-10-16 20:03:47.303532 | Idx:   80/400    | loss: 0.120 | grad_norm: 0.270 | acc: 98.31% |
| 2025-10-16 20:03:52.903103 | Idx:  120/400    | loss: 0.099 | grad_norm: 2.022 | acc: 98.62% |
| 2025-10-16 20:03:56.820606 | Idx:  160/400    | loss: 0.100 | grad_norm: 0.048 | acc: 99.38% |
| 2025-10-16 20:04:00.753516 | Idx:  200/400    | loss: 0.095 | grad_norm: 0.145 | acc: 99.35% |
| 2025-10-16 20:04:06.146084 | Idx:  240/400    | loss: 0.095 | grad_norm: 0.145 | acc: 98.99% |
| 2025-10-16 20:04:10.044160 | Idx:  280/400    | loss: 0.096 | grad_norm: 1.747 | acc: 91.83% |
| 2025-10-16 20:04:15.451392 | Idx:  320/400    | loss: 0.095 | grad_norm: 2.479 | acc: 95.16% |
| 2025-10-16 20:04:19.352627 | Idx:  360/400    | loss: 0.095 | grad_norm: 0.038 | acc: 99.31% |
| 2025-10-16 20:04:23.240148 | Idx:  400/400    | loss: 0.091 | grad_norm: 0.016 | acc: 99.40% |
Train Loss: 0.0912 Acc: 98.52%
Acc: 98.82%


| val_epoch_acc: 98.82% | epoch: 07 | avg_train_loss: 0.0615 | avg_train_acc: 98.8266 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_195502-jb0lrav7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_195502-jb0lrav7\logs[0m
rule: B3678/S34678 \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_20-04-48_multiscale_0__200-200-B3678_S34678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
����Sequential: 1-1                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-1                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-3                    [1, 2, 200, 200]          --
����Sequential: 1-2                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-4                       [1, 2, 200, 200]          102
��    ����BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-6                    [1, 2, 200, 200]          --
����Sequential: 1-3                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-7                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-9                    [1, 2, 200, 200]          --
����Sequential: 1-4                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-10                      [1, 4, 200, 200]          604
��    ����BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
��    ����ReLU: 2-12                        [1, 4, 200, 200]          --
��    ����Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 20:05:04.362955 | Idx:   40/400    | loss: 0.591 | grad_norm: 0.266 | acc: 91.66% |
| 2025-10-16 20:05:07.514544 | Idx:   80/400    | loss: 0.487 | grad_norm: 0.672 | acc: 82.89% |
| 2025-10-16 20:05:11.950086 | Idx:  120/400    | loss: 0.443 | grad_norm: 0.092 | acc: 87.22% |
| 2025-10-16 20:05:15.131080 | Idx:  160/400    | loss: 0.422 | grad_norm: 0.157 | acc: 93.35% |
| 2025-10-16 20:05:18.321195 | Idx:  200/400    | loss: 0.407 | grad_norm: 0.251 | acc: 89.26% |
| 2025-10-16 20:05:23.293630 | Idx:  240/400    | loss: 0.396 | grad_norm: 0.279 | acc: 88.50% |
| 2025-10-16 20:05:26.823391 | Idx:  280/400    | loss: 0.386 | grad_norm: 0.173 | acc: 92.40% |
| 2025-10-16 20:05:32.120106 | Idx:  320/400    | loss: 0.377 | grad_norm: 0.234 | acc: 91.41% |
| 2025-10-16 20:05:35.994463 | Idx:  360/400    | loss: 0.365 | grad_norm: 0.239 | acc: 93.99% |
| 2025-10-16 20:05:39.110376 | Idx:  400/400    | loss: 0.354 | grad_norm: 0.325 | acc: 92.46% |
Train Loss: 0.3537 Acc: 90.48%
Acc: 94.27%


| val_epoch_acc: 94.27% | epoch: 00 | avg_train_loss: 0.2498 | avg_train_acc: 93.0685 | 


Epoch 2/8
----------
| 2025-10-16 20:06:07.954186 | Idx:   40/400    | loss: 0.280 | grad_norm: 0.181 | acc: 95.68% |
| 2025-10-16 20:06:12.146343 | Idx:   80/400    | loss: 0.251 | grad_norm: 1.492 | acc: 91.80% |
| 2025-10-16 20:06:17.009483 | Idx:  120/400    | loss: 0.256 | grad_norm: 0.335 | acc: 95.25% |
| 2025-10-16 20:06:20.331593 | Idx:  160/400    | loss: 0.258 | grad_norm: 0.125 | acc: 98.30% |
| 2025-10-16 20:06:23.807477 | Idx:  200/400    | loss: 0.248 | grad_norm: 0.767 | acc: 94.12% |
| 2025-10-16 20:06:29.390970 | Idx:  240/400    | loss: 0.240 | grad_norm: 0.269 | acc: 95.46% |
| 2025-10-16 20:06:33.486117 | Idx:  280/400    | loss: 0.237 | grad_norm: 0.664 | acc: 95.49% |
| 2025-10-16 20:06:39.031252 | Idx:  320/400    | loss: 0.234 | grad_norm: 1.478 | acc: 90.60% |
| 2025-10-16 20:06:42.414724 | Idx:  360/400    | loss: 0.229 | grad_norm: 0.237 | acc: 95.75% |
| 2025-10-16 20:06:45.551801 | Idx:  400/400    | loss: 0.223 | grad_norm: 0.646 | acc: 97.03% |
Train Loss: 0.2227 Acc: 94.44%
Acc: 92.51%


| val_epoch_acc: 92.51% | epoch: 01 | avg_train_loss: 0.1750 | avg_train_acc: 96.0291 | 


Epoch 3/8
----------
| 2025-10-16 20:07:15.080343 | Idx:   40/400    | loss: 0.160 | grad_norm: 0.970 | acc: 97.30% |
| 2025-10-16 20:07:18.978718 | Idx:   80/400    | loss: 0.167 | grad_norm: 0.542 | acc: 96.57% |
| 2025-10-16 20:07:24.211098 | Idx:  120/400    | loss: 0.178 | grad_norm: 0.563 | acc: 97.01% |
| 2025-10-16 20:07:28.090059 | Idx:  160/400    | loss: 0.184 | grad_norm: 1.350 | acc: 94.94% |
| 2025-10-16 20:07:31.949316 | Idx:  200/400    | loss: 0.188 | grad_norm: 0.341 | acc: 96.11% |
| 2025-10-16 20:07:37.160793 | Idx:  240/400    | loss: 0.178 | grad_norm: 0.163 | acc: 98.05% |
| 2025-10-16 20:07:41.039198 | Idx:  280/400    | loss: 0.175 | grad_norm: 0.554 | acc: 97.73% |
| 2025-10-16 20:07:46.586171 | Idx:  320/400    | loss: 0.172 | grad_norm: 2.421 | acc: 85.91% |
| 2025-10-16 20:07:50.465330 | Idx:  360/400    | loss: 0.176 | grad_norm: 0.563 | acc: 96.43% |
| 2025-10-16 20:07:54.331955 | Idx:  400/400    | loss: 0.172 | grad_norm: 0.265 | acc: 98.43% |
Train Loss: 0.1720 Acc: 96.25%
Acc: 98.31%


| val_epoch_acc: 98.31% | epoch: 02 | avg_train_loss: 0.1327 | avg_train_acc: 97.1487 | 


Epoch 4/8
----------
| 2025-10-16 20:08:23.736193 | Idx:   40/400    | loss: 0.200 | grad_norm: 0.443 | acc: 96.83% |
| 2025-10-16 20:08:27.725114 | Idx:   80/400    | loss: 0.187 | grad_norm: 0.255 | acc: 98.82% |
| 2025-10-16 20:08:33.632226 | Idx:  120/400    | loss: 0.156 | grad_norm: 0.286 | acc: 98.77% |
| 2025-10-16 20:08:38.016358 | Idx:  160/400    | loss: 0.159 | grad_norm: 0.281 | acc: 99.06% |
| 2025-10-16 20:08:42.104479 | Idx:  200/400    | loss: 0.150 | grad_norm: 0.767 | acc: 97.23% |
| 2025-10-16 20:08:47.658933 | Idx:  240/400    | loss: 0.146 | grad_norm: 1.089 | acc: 93.41% |
| 2025-10-16 20:08:51.582504 | Idx:  280/400    | loss: 0.138 | grad_norm: 0.317 | acc: 99.46% |
| 2025-10-16 20:08:57.512066 | Idx:  320/400    | loss: 0.140 | grad_norm: 0.769 | acc: 97.29% |
| 2025-10-16 20:09:02.394429 | Idx:  360/400    | loss: 0.138 | grad_norm: 0.679 | acc: 95.64% |
| 2025-10-16 20:09:07.129281 | Idx:  400/400    | loss: 0.139 | grad_norm: 0.233 | acc: 99.24% |
Train Loss: 0.1386 Acc: 97.43%
Acc: 97.16%


| val_epoch_acc: 97.16% | epoch: 03 | avg_train_loss: 0.1229 | avg_train_acc: 97.7576 | 


Epoch 5/8
----------
| 2025-10-16 20:09:39.484135 | Idx:   40/400    | loss: 0.086 | grad_norm: 0.344 | acc: 98.95% |
| 2025-10-16 20:09:43.753672 | Idx:   80/400    | loss: 0.078 | grad_norm: 0.334 | acc: 99.44% |
| 2025-10-16 20:09:49.367396 | Idx:  120/400    | loss: 0.065 | grad_norm: 0.118 | acc: 99.95% |
| 2025-10-16 20:09:53.427133 | Idx:  160/400    | loss: 0.060 | grad_norm: 0.280 | acc: 99.95% |
| 2025-10-16 20:09:57.311335 | Idx:  200/400    | loss: 0.067 | grad_norm: 0.253 | acc: 99.53% |
| 2025-10-16 20:10:02.538723 | Idx:  240/400    | loss: 0.061 | grad_norm: 0.297 | acc: 99.98% |
| 2025-10-16 20:10:06.382947 | Idx:  280/400    | loss: 0.068 | grad_norm: 0.148 | acc: 99.87% |
| 2025-10-16 20:10:11.668749 | Idx:  320/400    | loss: 0.067 | grad_norm: 0.096 | acc: 100.00% |
| 2025-10-16 20:10:15.845770 | Idx:  360/400    | loss: 0.069 | grad_norm: 3.987 | acc: 91.95% |
| 2025-10-16 20:10:20.084359 | Idx:  400/400    | loss: 0.082 | grad_norm: 0.721 | acc: 97.67% |
Train Loss: 0.0816 Acc: 98.89%
Acc: 97.05%


| val_epoch_acc: 97.05% | epoch: 04 | avg_train_loss: 0.1166 | avg_train_acc: 97.9060 | 


Epoch 6/8
----------
| 2025-10-16 20:10:50.680943 | Idx:   40/400    | loss: 0.083 | grad_norm: 0.132 | acc: 99.94% |
| 2025-10-16 20:10:54.919390 | Idx:   80/400    | loss: 0.085 | grad_norm: 0.042 | acc: 99.99% |
| 2025-10-16 20:11:00.632211 | Idx:  120/400    | loss: 0.071 | grad_norm: 0.188 | acc: 99.97% |
| 2025-10-16 20:11:04.807190 | Idx:  160/400    | loss: 0.063 | grad_norm: 0.056 | acc: 100.00% |
| 2025-10-16 20:11:08.793215 | Idx:  200/400    | loss: 0.065 | grad_norm: 0.435 | acc: 99.85% |
| 2025-10-16 20:11:14.062121 | Idx:  240/400    | loss: 0.066 | grad_norm: 0.554 | acc: 99.22% |
| 2025-10-16 20:11:17.918924 | Idx:  280/400    | loss: 0.077 | grad_norm: 1.031 | acc: 95.26% |
| 2025-10-16 20:11:23.280285 | Idx:  320/400    | loss: 0.072 | grad_norm: 0.218 | acc: 99.79% |
| 2025-10-16 20:11:27.245692 | Idx:  360/400    | loss: 0.068 | grad_norm: 0.157 | acc: 100.00% |
| 2025-10-16 20:11:31.113777 | Idx:  400/400    | loss: 0.065 | grad_norm: 0.670 | acc: 99.54% |
Train Loss: 0.0648 Acc: 99.16%
Acc: 99.74%


| val_epoch_acc: 99.74% | epoch: 05 | avg_train_loss: 0.0377 | avg_train_acc: 99.5580 | 


Epoch 7/8
----------
| 2025-10-16 20:12:03.600278 | Idx:   40/400    | loss: 0.109 | grad_norm: 0.062 | acc: 99.98% |
| 2025-10-16 20:12:07.649210 | Idx:   80/400    | loss: 0.072 | grad_norm: 2.075 | acc: 95.65% |
| 2025-10-16 20:12:12.996642 | Idx:  120/400    | loss: 0.080 | grad_norm: 0.273 | acc: 99.75% |
| 2025-10-16 20:12:17.101723 | Idx:  160/400    | loss: 0.072 | grad_norm: 0.143 | acc: 100.00% |
| 2025-10-16 20:12:21.574617 | Idx:  200/400    | loss: 0.079 | grad_norm: 0.087 | acc: 99.96% |
| 2025-10-16 20:12:27.520687 | Idx:  240/400    | loss: 0.074 | grad_norm: 0.189 | acc: 99.93% |
| 2025-10-16 20:12:32.076966 | Idx:  280/400    | loss: 0.068 | grad_norm: 0.124 | acc: 99.98% |
| 2025-10-16 20:12:38.701914 | Idx:  320/400    | loss: 0.062 | grad_norm: 0.024 | acc: 100.00% |
| 2025-10-16 20:12:43.820866 | Idx:  360/400    | loss: 0.059 | grad_norm: 0.448 | acc: 99.91% |
| 2025-10-16 20:12:48.935683 | Idx:  400/400    | loss: 0.061 | grad_norm: 0.022 | acc: 100.00% |
Train Loss: 0.0605 Acc: 99.24%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 06 | avg_train_loss: 0.0478 | avg_train_acc: 99.3980 | 


Epoch 8/8
----------
| 2025-10-16 20:13:20.941396 | Idx:   40/400    | loss: 0.067 | grad_norm: 0.600 | acc: 99.70% |
| 2025-10-16 20:13:24.824700 | Idx:   80/400    | loss: 0.059 | grad_norm: 0.510 | acc: 99.49% |
| 2025-10-16 20:13:30.014392 | Idx:  120/400    | loss: 0.048 | grad_norm: 0.750 | acc: 98.18% |
| 2025-10-16 20:13:33.893932 | Idx:  160/400    | loss: 0.043 | grad_norm: 0.336 | acc: 99.71% |
| 2025-10-16 20:13:37.775201 | Idx:  200/400    | loss: 0.042 | grad_norm: 1.537 | acc: 94.71% |
| 2025-10-16 20:13:42.917686 | Idx:  240/400    | loss: 0.044 | grad_norm: 1.677 | acc: 97.63% |
| 2025-10-16 20:13:46.566554 | Idx:  280/400    | loss: 0.049 | grad_norm: 7.217 | acc: 85.66% |
| 2025-10-16 20:13:51.268740 | Idx:  320/400    | loss: 0.065 | grad_norm: 1.008 | acc: 99.17% |
| 2025-10-16 20:13:54.633074 | Idx:  360/400    | loss: 0.066 | grad_norm: 0.059 | acc: 100.00% |
| 2025-10-16 20:13:57.684710 | Idx:  400/400    | loss: 0.067 | grad_norm: 1.642 | acc: 95.38% |
Train Loss: 0.0671 Acc: 99.18%
Acc: 99.95%


| val_epoch_acc: 99.95% | epoch: 07 | avg_train_loss: 0.0973 | avg_train_acc: 98.6867 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_200448-6a5xr0yh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_200448-6a5xr0yh\logs[0m
rule: B35678/S5678 \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_20-14-21_multiscale_0__200-200-B35678_S5678


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
����Sequential: 1-1                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-1                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-3                    [1, 2, 200, 200]          --
����Sequential: 1-2                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-4                       [1, 2, 200, 200]          102
��    ����BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-6                    [1, 2, 200, 200]          --
����Sequential: 1-3                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-7                       [1, 2, 200, 200]          38
��    ����BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
��    ����LeakyReLU: 2-9                    [1, 2, 200, 200]          --
����Sequential: 1-4                        [1, 2, 200, 200]          --
��    ����Conv2d: 2-10                      [1, 4, 200, 200]          604
��    ����BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
��    ����ReLU: 2-12                        [1, 4, 200, 200]          --
��    ����Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-16 20:14:38.602137 | Idx:   40/400    | loss: 0.302 | grad_norm: 0.304 | acc: 98.73% |
| 2025-10-16 20:14:41.890286 | Idx:   80/400    | loss: 0.185 | grad_norm: 0.060 | acc: 98.20% |
| 2025-10-16 20:14:46.354756 | Idx:  120/400    | loss: 0.146 | grad_norm: 0.133 | acc: 99.62% |
| 2025-10-16 20:14:49.669725 | Idx:  160/400    | loss: 0.122 | grad_norm: 0.012 | acc: 99.45% |
| 2025-10-16 20:14:52.962977 | Idx:  200/400    | loss: 0.111 | grad_norm: 0.343 | acc: 98.33% |
| 2025-10-16 20:14:57.639127 | Idx:  240/400    | loss: 0.104 | grad_norm: 0.084 | acc: 96.82% |
| 2025-10-16 20:15:00.942810 | Idx:  280/400    | loss: 0.125 | grad_norm: 1.020 | acc: 93.37% |
| 2025-10-16 20:15:05.570020 | Idx:  320/400    | loss: 0.117 | grad_norm: 0.015 | acc: 99.97% |
| 2025-10-16 20:15:08.874600 | Idx:  360/400    | loss: 0.109 | grad_norm: 0.003 | acc: 99.96% |
| 2025-10-16 20:15:12.175065 | Idx:  400/400    | loss: 0.103 | grad_norm: 0.042 | acc: 99.13% |
Train Loss: 0.1026 Acc: 98.39%
Acc: 99.24%


| val_epoch_acc: 99.24% | epoch: 00 | avg_train_loss: 0.0507 | avg_train_acc: 98.9639 | 


Epoch 2/8
----------
| 2025-10-16 20:15:39.549463 | Idx:   40/400    | loss: 0.039 | grad_norm: 0.042 | acc: 99.80% |
| 2025-10-16 20:15:42.840756 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.035 | acc: 99.62% |
| 2025-10-16 20:15:47.340654 | Idx:  120/400    | loss: 0.039 | grad_norm: 0.312 | acc: 99.62% |
| 2025-10-16 20:15:50.646158 | Idx:  160/400    | loss: 0.039 | grad_norm: 0.133 | acc: 97.89% |
| 2025-10-16 20:15:53.962762 | Idx:  200/400    | loss: 0.042 | grad_norm: 0.027 | acc: 99.80% |
| 2025-10-16 20:15:58.919405 | Idx:  240/400    | loss: 0.049 | grad_norm: 0.065 | acc: 98.80% |
| 2025-10-16 20:16:02.692947 | Idx:  280/400    | loss: 0.048 | grad_norm: 0.017 | acc: 99.61% |
| 2025-10-16 20:16:07.940001 | Idx:  320/400    | loss: 0.052 | grad_norm: 0.071 | acc: 99.59% |
| 2025-10-16 20:16:11.074956 | Idx:  360/400    | loss: 0.050 | grad_norm: 0.035 | acc: 98.92% |
| 2025-10-16 20:16:14.205155 | Idx:  400/400    | loss: 0.049 | grad_norm: 0.038 | acc: 98.54% |
Train Loss: 0.0489 Acc: 99.18%
Acc: 99.35%


| val_epoch_acc: 99.35% | epoch: 01 | avg_train_loss: 0.0357 | avg_train_acc: 99.3300 | 


Epoch 3/8
----------
| 2025-10-16 20:16:40.598683 | Idx:   40/400    | loss: 0.035 | grad_norm: 0.014 | acc: 99.72% |
| 2025-10-16 20:16:43.759719 | Idx:   80/400    | loss: 0.035 | grad_norm: 0.041 | acc: 99.76% |
| 2025-10-16 20:16:48.940181 | Idx:  120/400    | loss: 0.033 | grad_norm: 0.007 | acc: 99.79% |
| 2025-10-16 20:16:52.774522 | Idx:  160/400    | loss: 0.033 | grad_norm: 0.033 | acc: 99.94% |
| 2025-10-16 20:16:56.564389 | Idx:  200/400    | loss: 0.032 | grad_norm: 0.026 | acc: 99.87% |
| 2025-10-16 20:17:01.673459 | Idx:  240/400    | loss: 0.033 | grad_norm: 0.095 | acc: 98.30% |
| 2025-10-16 20:17:05.494086 | Idx:  280/400    | loss: 0.038 | grad_norm: 0.051 | acc: 99.72% |
| 2025-10-16 20:17:10.755384 | Idx:  320/400    | loss: 0.038 | grad_norm: 0.054 | acc: 99.45% |
| 2025-10-16 20:17:14.487529 | Idx:  360/400    | loss: 0.043 | grad_norm: 0.139 | acc: 99.42% |
| 2025-10-16 20:17:17.794906 | Idx:  400/400    | loss: 0.045 | grad_norm: 0.037 | acc: 99.42% |
Train Loss: 0.0446 Acc: 99.22%
Acc: 99.45%


| val_epoch_acc: 99.45% | epoch: 02 | avg_train_loss: 0.0509 | avg_train_acc: 98.9987 | 


Epoch 4/8
----------
| 2025-10-16 20:17:44.851576 | Idx:   40/400    | loss: 0.039 | grad_norm: 0.033 | acc: 99.96% |
| 2025-10-16 20:17:48.155159 | Idx:   80/400    | loss: 0.050 | grad_norm: 0.262 | acc: 98.46% |
| 2025-10-16 20:17:52.773347 | Idx:  120/400    | loss: 0.052 | grad_norm: 0.120 | acc: 98.39% |
| 2025-10-16 20:17:56.089824 | Idx:  160/400    | loss: 0.050 | grad_norm: 0.054 | acc: 99.57% |
| 2025-10-16 20:17:59.395351 | Idx:  200/400    | loss: 0.051 | grad_norm: 0.108 | acc: 99.79% |
| 2025-10-16 20:18:03.911876 | Idx:  240/400    | loss: 0.046 | grad_norm: 0.002 | acc: 99.92% |
| 2025-10-16 20:18:07.254308 | Idx:  280/400    | loss: 0.044 | grad_norm: 0.012 | acc: 99.97% |
| 2025-10-16 20:18:11.812158 | Idx:  320/400    | loss: 0.045 | grad_norm: 0.138 | acc: 99.75% |
| 2025-10-16 20:18:15.144243 | Idx:  360/400    | loss: 0.048 | grad_norm: 0.028 | acc: 98.95% |
| 2025-10-16 20:18:18.564647 | Idx:  400/400    | loss: 0.047 | grad_norm: 0.116 | acc: 99.27% |
Train Loss: 0.0471 Acc: 99.14%
Acc: 99.52%


| val_epoch_acc: 99.52% | epoch: 03 | avg_train_loss: 0.0471 | avg_train_acc: 99.0515 | 


Epoch 5/8
----------
| 2025-10-16 20:18:47.686356 | Idx:   40/400    | loss: 0.059 | grad_norm: 0.009 | acc: 99.80% |
| 2025-10-16 20:18:51.016028 | Idx:   80/400    | loss: 0.053 | grad_norm: 0.122 | acc: 98.88% |
| 2025-10-16 20:18:55.569756 | Idx:  120/400    | loss: 0.046 | grad_norm: 0.013 | acc: 99.49% |
| 2025-10-16 20:18:58.883451 | Idx:  160/400    | loss: 0.043 | grad_norm: 0.036 | acc: 98.73% |
| 2025-10-16 20:19:02.206517 | Idx:  200/400    | loss: 0.042 | grad_norm: 2.320 | acc: 95.84% |
| 2025-10-16 20:19:06.810420 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.071 | acc: 99.06% |
| 2025-10-16 20:19:10.136772 | Idx:  280/400    | loss: 0.037 | grad_norm: 0.119 | acc: 98.65% |
| 2025-10-16 20:19:14.669484 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.164 | acc: 99.16% |
| 2025-10-16 20:19:17.964849 | Idx:  360/400    | loss: 0.035 | grad_norm: 0.027 | acc: 99.71% |
| 2025-10-16 20:19:21.260271 | Idx:  400/400    | loss: 0.035 | grad_norm: 0.010 | acc: 99.78% |
Train Loss: 0.0351 Acc: 99.34%
Acc: 99.60%


| val_epoch_acc: 99.60% | epoch: 04 | avg_train_loss: 0.0370 | avg_train_acc: 99.2346 | 


Epoch 6/8
----------
| 2025-10-16 20:19:49.880888 | Idx:   40/400    | loss: 0.032 | grad_norm: 0.058 | acc: 99.19% |
| 2025-10-16 20:19:53.696918 | Idx:   80/400    | loss: 0.027 | grad_norm: 0.075 | acc: 99.72% |
| 2025-10-16 20:19:58.705420 | Idx:  120/400    | loss: 0.026 | grad_norm: 0.026 | acc: 99.67% |
| 2025-10-16 20:20:02.339369 | Idx:  160/400    | loss: 0.029 | grad_norm: 0.409 | acc: 96.99% |
| 2025-10-16 20:20:05.666295 | Idx:  200/400    | loss: 0.029 | grad_norm: 0.039 | acc: 99.00% |
| 2025-10-16 20:20:10.231137 | Idx:  240/400    | loss: 0.029 | grad_norm: 0.296 | acc: 97.15% |
| 2025-10-16 20:20:13.602368 | Idx:  280/400    | loss: 0.032 | grad_norm: 0.099 | acc: 99.13% |
| 2025-10-16 20:20:18.105037 | Idx:  320/400    | loss: 0.030 | grad_norm: 0.038 | acc: 99.02% |
| 2025-10-16 20:20:21.504147 | Idx:  360/400    | loss: 0.030 | grad_norm: 0.451 | acc: 98.26% |
| 2025-10-16 20:20:25.276434 | Idx:  400/400    | loss: 0.030 | grad_norm: 0.039 | acc: 99.74% |
Train Loss: 0.0299 Acc: 99.43%
Acc: 99.58%


| val_epoch_acc: 99.58% | epoch: 05 | avg_train_loss: 0.0267 | avg_train_acc: 99.4125 | 


Epoch 7/8
----------
| 2025-10-16 20:20:53.864832 | Idx:   40/400    | loss: 0.019 | grad_norm: 0.073 | acc: 99.25% |
| 2025-10-16 20:20:57.174775 | Idx:   80/400    | loss: 0.020 | grad_norm: 0.009 | acc: 99.75% |
| 2025-10-16 20:21:01.729621 | Idx:  120/400    | loss: 0.023 | grad_norm: 0.029 | acc: 99.76% |
| 2025-10-16 20:21:05.063069 | Idx:  160/400    | loss: 0.024 | grad_norm: 0.047 | acc: 99.07% |
| 2025-10-16 20:21:08.371585 | Idx:  200/400    | loss: 0.025 | grad_norm: 0.161 | acc: 99.41% |
| 2025-10-16 20:21:13.342920 | Idx:  240/400    | loss: 0.025 | grad_norm: 0.094 | acc: 99.41% |
| 2025-10-16 20:21:17.144737 | Idx:  280/400    | loss: 0.025 | grad_norm: 0.006 | acc: 99.91% |
| 2025-10-16 20:21:22.320816 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.008 | acc: 99.90% |
| 2025-10-16 20:21:26.103383 | Idx:  360/400    | loss: 0.024 | grad_norm: 0.096 | acc: 99.33% |
| 2025-10-16 20:21:29.940235 | Idx:  400/400    | loss: 0.026 | grad_norm: 0.174 | acc: 97.34% |
Train Loss: 0.0258 Acc: 99.50%
Acc: 99.57%


| val_epoch_acc: 99.57% | epoch: 06 | avg_train_loss: 0.0427 | avg_train_acc: 99.1847 | 


Epoch 8/8
----------
| 2025-10-16 20:21:58.900030 | Idx:   40/400    | loss: 0.025 | grad_norm: 0.005 | acc: 99.96% |
| 2025-10-16 20:22:02.683846 | Idx:   80/400    | loss: 0.028 | grad_norm: 0.134 | acc: 97.47% |
| 2025-10-16 20:22:07.639945 | Idx:  120/400    | loss: 0.025 | grad_norm: 0.033 | acc: 99.48% |
| 2025-10-16 20:22:11.444846 | Idx:  160/400    | loss: 0.023 | grad_norm: 0.001 | acc: 99.99% |
| 2025-10-16 20:22:15.233774 | Idx:  200/400    | loss: 0.022 | grad_norm: 0.050 | acc: 99.67% |
| 2025-10-16 20:22:20.390881 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.109 | acc: 98.14% |
| 2025-10-16 20:22:24.195137 | Idx:  280/400    | loss: 0.044 | grad_norm: 0.048 | acc: 99.83% |
| 2025-10-16 20:22:29.118777 | Idx:  320/400    | loss: 0.042 | grad_norm: 0.001 | acc: 99.99% |
| 2025-10-16 20:22:32.337751 | Idx:  360/400    | loss: 0.042 | grad_norm: 0.015 | acc: 99.85% |
| 2025-10-16 20:22:35.542123 | Idx:  400/400    | loss: 0.041 | grad_norm: 0.029 | acc: 99.72% |
Train Loss: 0.0407 Acc: 99.21%
Acc: 99.65%


| val_epoch_acc: 99.65% | epoch: 07 | avg_train_loss: 0.0334 | avg_train_acc: 99.4128 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_201421-flnm7kfx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_201421-flnm7kfx\logs[0m
rule: B3/S23 \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B3/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3_S23/
Saving Base Directory: 2025-10-16_20-22-59_multiscale_0__200-200-B3_S23


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
����Sequential: 1-1                                       --                        --
��    ����R2Conv: 2-1                                      --                        26
��    ��    ����BlocksBasisExpansion: 3-1                   --                        --
��    ����InnerBatchNorm: 2-2                              --                        --
��    ��    ����BatchNorm3d: 3-2                            --                        4
��    ����ReLU: 2-3                                        --                        --
����Sequential: 1-2                                       --                        --
��    ����R2Conv: 2-4                                      --                        46
��    ��    ����BlocksBasisExpansion: 3-3                   --                        --
��    ����InnerBatchNorm: 2-5                              --                        --
��    ��    ����BatchNorm3d: 3-4                            --                        4
��    ����ReLU: 2-6                                        --                        --
����Sequential: 1-3                                       --                        --
��    ����R2Conv: 2-7                                      --                        34
��    ��    ����BlocksBasisExpansion: 3-5                   --                        --
��    ����InnerBatchNorm: 2-8                              --                        --
��    ��    ����BatchNorm3d: 3-6                            --                        4
��    ����ReLU: 2-9                                        --                        --
����Sequential: 1-4                                       --                        --
��    ����R2Conv: 2-10                                     --                        1,060
��    ��    ����BlocksBasisExpansion: 3-7                   --                        --
��    ����InnerBatchNorm: 2-11                             --                        --
��    ��    ����BatchNorm3d: 3-8                            --                        8
��    ����ReLU: 2-12                                       --                        --
��    ����R2Conv: 2-13                                     --                        90
��    ��    ����BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-16 20:23:19.026771 | Idx:   40/400    | loss: 0.532 | grad_norm: 0.217 | acc: 91.72% |
| 2025-10-16 20:23:25.136327 | Idx:   80/400    | loss: 0.405 | grad_norm: 1.026 | acc: 92.62% |
| 2025-10-16 20:23:32.588102 | Idx:  120/400    | loss: 0.316 | grad_norm: 1.048 | acc: 94.86% |
| 2025-10-16 20:23:38.716602 | Idx:  160/400    | loss: 0.269 | grad_norm: 0.525 | acc: 97.44% |
| 2025-10-16 20:23:44.838191 | Idx:  200/400    | loss: 0.243 | grad_norm: 0.658 | acc: 97.35% |
| 2025-10-16 20:23:52.470433 | Idx:  240/400    | loss: 0.213 | grad_norm: 0.579 | acc: 97.60% |
| 2025-10-16 20:23:58.610284 | Idx:  280/400    | loss: 0.194 | grad_norm: 0.481 | acc: 98.35% |
| 2025-10-16 20:24:06.105869 | Idx:  320/400    | loss: 0.177 | grad_norm: 1.535 | acc: 97.27% |
| 2025-10-16 20:24:12.255720 | Idx:  360/400    | loss: 0.173 | grad_norm: 0.509 | acc: 98.38% |
| 2025-10-16 20:24:18.575967 | Idx:  400/400    | loss: 0.160 | grad_norm: 0.321 | acc: 99.02% |
Train Loss: 0.1598 Acc: 96.14%
Acc: 99.51%


| val_epoch_acc: 99.51% | epoch: 00 | avg_train_loss: 0.0395 | avg_train_acc: 99.2247 | 


Epoch 2/8
----------
| 2025-10-16 20:24:51.383502 | Idx:   40/400    | loss: 0.040 | grad_norm: 0.661 | acc: 98.25% |
| 2025-10-16 20:24:58.859152 | Idx:   80/400    | loss: 0.034 | grad_norm: 0.761 | acc: 98.21% |
| 2025-10-16 20:25:06.419450 | Idx:  120/400    | loss: 0.045 | grad_norm: 0.713 | acc: 98.29% |
| 2025-10-16 20:25:12.588557 | Idx:  160/400    | loss: 0.040 | grad_norm: 0.126 | acc: 99.73% |
| 2025-10-16 20:25:18.742280 | Idx:  200/400    | loss: 0.043 | grad_norm: 0.938 | acc: 99.91% |
| 2025-10-16 20:25:26.482614 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.125 | acc: 99.86% |
| 2025-10-16 20:25:32.661622 | Idx:  280/400    | loss: 0.037 | grad_norm: 0.190 | acc: 99.97% |
| 2025-10-16 20:25:40.224855 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.013 | acc: 99.98% |
| 2025-10-16 20:25:46.406535 | Idx:  360/400    | loss: 0.034 | grad_norm: 0.022 | acc: 99.99% |
| 2025-10-16 20:25:52.570376 | Idx:  400/400    | loss: 0.032 | grad_norm: 0.135 | acc: 99.89% |
Train Loss: 0.0325 Acc: 99.47%
Acc: 99.97%


| val_epoch_acc: 99.97% | epoch: 01 | avg_train_loss: 0.0124 | avg_train_acc: 99.8432 | 


Epoch 3/8
----------
| 2025-10-16 20:26:22.884272 | Idx:   40/400    | loss: 0.019 | grad_norm: 0.032 | acc: 100.00% |
| 2025-10-16 20:26:29.020917 | Idx:   80/400    | loss: 0.016 | grad_norm: 0.437 | acc: 99.08% |
| 2025-10-16 20:26:36.473263 | Idx:  120/400    | loss: 0.018 | grad_norm: 0.031 | acc: 100.00% |
| 2025-10-16 20:26:42.612234 | Idx:  160/400    | loss: 0.020 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-16 20:26:48.763126 | Idx:  200/400    | loss: 0.018 | grad_norm: 0.022 | acc: 100.00% |
| 2025-10-16 20:26:56.256729 | Idx:  240/400    | loss: 0.018 | grad_norm: 0.327 | acc: 99.60% |
| 2025-10-16 20:27:02.398988 | Idx:  280/400    | loss: 0.017 | grad_norm: 0.025 | acc: 99.99% |
| 2025-10-16 20:27:10.197010 | Idx:  320/400    | loss: 0.016 | grad_norm: 0.133 | acc: 99.85% |
| 2025-10-16 20:27:16.345611 | Idx:  360/400    | loss: 0.015 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-16 20:27:22.467791 | Idx:  400/400    | loss: 0.015 | grad_norm: 0.041 | acc: 100.00% |
Train Loss: 0.0153 Acc: 99.80%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0207 | avg_train_acc: 99.7806 | 


Epoch 4/8
----------
| 2025-10-16 20:27:52.992973 | Idx:   40/400    | loss: 0.030 | grad_norm: 0.068 | acc: 100.00% |
| 2025-10-16 20:27:59.135499 | Idx:   80/400    | loss: 0.018 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-16 20:28:06.609629 | Idx:  120/400    | loss: 0.015 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-16 20:28:12.752637 | Idx:  160/400    | loss: 0.013 | grad_norm: 0.017 | acc: 99.99% |
| 2025-10-16 20:28:18.907415 | Idx:  200/400    | loss: 0.013 | grad_norm: 0.176 | acc: 99.80% |
| 2025-10-16 20:28:26.448643 | Idx:  240/400    | loss: 0.013 | grad_norm: 0.021 | acc: 99.99% |
| 2025-10-16 20:28:33.077273 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.130 | acc: 99.87% |
| 2025-10-16 20:28:42.150513 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 20:28:49.606509 | Idx:  360/400    | loss: 0.011 | grad_norm: 0.022 | acc: 99.99% |
| 2025-10-16 20:28:57.088289 | Idx:  400/400    | loss: 0.010 | grad_norm: 0.113 | acc: 99.99% |
Train Loss: 0.0100 Acc: 99.90%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0055 | avg_train_acc: 99.9759 | 


Epoch 5/8
----------
| 2025-10-16 20:29:29.910758 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.043 | acc: 99.98% |
| 2025-10-16 20:29:37.414855 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 20:29:46.227612 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-16 20:29:53.710900 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-16 20:29:59.900478 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-16 20:30:07.765721 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-16 20:30:15.204685 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.042 | acc: 99.98% |
| 2025-10-16 20:30:23.989750 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-16 20:30:31.503746 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.009 | acc: 99.99% |
| 2025-10-16 20:30:38.734547 | Idx:  400/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0040 Acc: 99.99%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_202259-kthqxzzc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_202259-kthqxzzc\logs[0m
rule: B36/S23 \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B36/S23', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B36_S23/
Saving Base Directory: 2025-10-16_20-31-03_multiscale_0__200-200-B36_S23


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
����Sequential: 1-1                                       --                        --
��    ����R2Conv: 2-1                                      --                        26
��    ��    ����BlocksBasisExpansion: 3-1                   --                        --
��    ����InnerBatchNorm: 2-2                              --                        --
��    ��    ����BatchNorm3d: 3-2                            --                        4
��    ����ReLU: 2-3                                        --                        --
����Sequential: 1-2                                       --                        --
��    ����R2Conv: 2-4                                      --                        46
��    ��    ����BlocksBasisExpansion: 3-3                   --                        --
��    ����InnerBatchNorm: 2-5                              --                        --
��    ��    ����BatchNorm3d: 3-4                            --                        4
��    ����ReLU: 2-6                                        --                        --
����Sequential: 1-3                                       --                        --
��    ����R2Conv: 2-7                                      --                        34
��    ��    ����BlocksBasisExpansion: 3-5                   --                        --
��    ����InnerBatchNorm: 2-8                              --                        --
��    ��    ����BatchNorm3d: 3-6                            --                        4
��    ����ReLU: 2-9                                        --                        --
����Sequential: 1-4                                       --                        --
��    ����R2Conv: 2-10                                     --                        1,060
��    ��    ����BlocksBasisExpansion: 3-7                   --                        --
��    ����InnerBatchNorm: 2-11                             --                        --
��    ��    ����BatchNorm3d: 3-8                            --                        8
��    ����ReLU: 2-12                                       --                        --
��    ����R2Conv: 2-13                                     --                        90
��    ��    ����BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-16 20:31:22.979161 | Idx:   40/400    | loss: 1.255 | grad_norm: 0.725 | acc: 90.81% |
| 2025-10-16 20:31:29.112890 | Idx:   80/400    | loss: 0.818 | grad_norm: 0.251 | acc: 93.48% |
| 2025-10-16 20:31:36.572840 | Idx:  120/400    | loss: 0.636 | grad_norm: 0.252 | acc: 94.28% |
| 2025-10-16 20:31:42.854118 | Idx:  160/400    | loss: 0.548 | grad_norm: 3.570 | acc: 87.86% |
| 2025-10-16 20:31:49.025312 | Idx:  200/400    | loss: 0.487 | grad_norm: 0.042 | acc: 94.30% |
| 2025-10-16 20:31:56.752459 | Idx:  240/400    | loss: 0.441 | grad_norm: 0.253 | acc: 96.85% |
| 2025-10-16 20:32:02.918383 | Idx:  280/400    | loss: 0.406 | grad_norm: 0.143 | acc: 95.76% |
| 2025-10-16 20:32:10.491525 | Idx:  320/400    | loss: 0.379 | grad_norm: 0.501 | acc: 93.81% |
| 2025-10-16 20:32:16.636890 | Idx:  360/400    | loss: 0.358 | grad_norm: 0.049 | acc: 96.77% |
| 2025-10-16 20:32:22.784400 | Idx:  400/400    | loss: 0.338 | grad_norm: 0.876 | acc: 95.46% |
Train Loss: 0.3377 Acc: 92.75%
Acc: 96.12%


| val_epoch_acc: 96.12% | epoch: 00 | avg_train_loss: 0.1571 | avg_train_acc: 95.8909 | 


Epoch 2/8
----------
| 2025-10-16 20:32:53.557935 | Idx:   40/400    | loss: 0.143 | grad_norm: 2.007 | acc: 93.87% |
| 2025-10-16 20:32:59.684824 | Idx:   80/400    | loss: 0.124 | grad_norm: 0.382 | acc: 98.50% |
| 2025-10-16 20:33:07.202232 | Idx:  120/400    | loss: 0.117 | grad_norm: 0.135 | acc: 98.63% |
| 2025-10-16 20:33:13.336538 | Idx:  160/400    | loss: 0.107 | grad_norm: 0.040 | acc: 99.12% |
| 2025-10-16 20:33:19.488088 | Idx:  200/400    | loss: 0.099 | grad_norm: 1.727 | acc: 96.58% |
| 2025-10-16 20:33:27.128243 | Idx:  240/400    | loss: 0.091 | grad_norm: 0.155 | acc: 99.32% |
| 2025-10-16 20:33:33.383037 | Idx:  280/400    | loss: 0.083 | grad_norm: 0.229 | acc: 99.40% |
| 2025-10-16 20:33:40.917305 | Idx:  320/400    | loss: 0.080 | grad_norm: 0.145 | acc: 99.42% |
| 2025-10-16 20:33:47.035178 | Idx:  360/400    | loss: 0.075 | grad_norm: 0.871 | acc: 99.54% |
| 2025-10-16 20:33:53.172350 | Idx:  400/400    | loss: 0.071 | grad_norm: 0.239 | acc: 99.30% |
Train Loss: 0.0708 Acc: 98.49%
Acc: 99.13%


| val_epoch_acc: 99.13% | epoch: 01 | avg_train_loss: 0.0365 | avg_train_acc: 99.4302 | 


Epoch 3/8
----------
| 2025-10-16 20:34:23.651038 | Idx:   40/400    | loss: 0.033 | grad_norm: 0.029 | acc: 99.72% |
| 2025-10-16 20:34:29.814577 | Idx:   80/400    | loss: 0.040 | grad_norm: 1.129 | acc: 97.51% |
| 2025-10-16 20:34:37.314954 | Idx:  120/400    | loss: 0.037 | grad_norm: 0.013 | acc: 99.74% |
| 2025-10-16 20:34:43.453872 | Idx:  160/400    | loss: 0.034 | grad_norm: 0.156 | acc: 99.50% |
| 2025-10-16 20:34:49.579958 | Idx:  200/400    | loss: 0.032 | grad_norm: 0.069 | acc: 99.69% |
| 2025-10-16 20:34:57.045230 | Idx:  240/400    | loss: 0.032 | grad_norm: 0.354 | acc: 99.10% |
| 2025-10-16 20:35:03.217963 | Idx:  280/400    | loss: 0.032 | grad_norm: 0.945 | acc: 99.68% |
| 2025-10-16 20:35:12.220619 | Idx:  320/400    | loss: 0.031 | grad_norm: 0.042 | acc: 99.76% |
| 2025-10-16 20:35:19.739220 | Idx:  360/400    | loss: 0.034 | grad_norm: 0.163 | acc: 99.42% |
| 2025-10-16 20:35:27.213509 | Idx:  400/400    | loss: 0.034 | grad_norm: 0.052 | acc: 99.73% |
Train Loss: 0.0343 Acc: 99.49%
Acc: 99.67%


| val_epoch_acc: 99.67% | epoch: 02 | avg_train_loss: 0.0272 | avg_train_acc: 99.6262 | 


Epoch 4/8
----------
| 2025-10-16 20:35:58.775692 | Idx:   40/400    | loss: 0.021 | grad_norm: 0.011 | acc: 99.75% |
| 2025-10-16 20:36:06.295308 | Idx:   80/400    | loss: 0.021 | grad_norm: 1.807 | acc: 99.35% |
| 2025-10-16 20:36:15.267130 | Idx:  120/400    | loss: 0.023 | grad_norm: 0.062 | acc: 99.82% |
| 2025-10-16 20:36:22.766707 | Idx:  160/400    | loss: 0.022 | grad_norm: 0.014 | acc: 99.81% |
| 2025-10-16 20:36:30.256783 | Idx:  200/400    | loss: 0.023 | grad_norm: 0.194 | acc: 99.44% |
| 2025-10-16 20:36:39.136817 | Idx:  240/400    | loss: 0.024 | grad_norm: 0.990 | acc: 99.69% |
| 2025-10-16 20:36:45.923672 | Idx:  280/400    | loss: 0.023 | grad_norm: 0.057 | acc: 99.79% |
| 2025-10-16 20:36:53.990602 | Idx:  320/400    | loss: 0.023 | grad_norm: 0.031 | acc: 99.81% |
| 2025-10-16 20:37:00.458735 | Idx:  360/400    | loss: 0.022 | grad_norm: 0.040 | acc: 99.77% |
| 2025-10-16 20:37:06.704887 | Idx:  400/400    | loss: 0.022 | grad_norm: 0.017 | acc: 99.81% |
Train Loss: 0.0216 Acc: 99.73%
Acc: 99.81%


| val_epoch_acc: 99.81% | epoch: 03 | avg_train_loss: 0.0175 | avg_train_acc: 99.8042 | 


Epoch 5/8
----------
| 2025-10-16 20:37:37.821476 | Idx:   40/400    | loss: 0.017 | grad_norm: 0.034 | acc: 99.83% |
| 2025-10-16 20:37:43.650957 | Idx:   80/400    | loss: 0.017 | grad_norm: 0.051 | acc: 99.83% |
| 2025-10-16 20:37:50.826568 | Idx:  120/400    | loss: 0.017 | grad_norm: 0.022 | acc: 99.82% |
| 2025-10-16 20:37:56.682318 | Idx:  160/400    | loss: 0.016 | grad_norm: 0.019 | acc: 99.83% |
| 2025-10-16 20:38:02.538437 | Idx:  200/400    | loss: 0.016 | grad_norm: 0.014 | acc: 99.85% |
| 2025-10-16 20:38:09.643290 | Idx:  240/400    | loss: 0.016 | grad_norm: 0.047 | acc: 99.79% |
| 2025-10-16 20:38:15.495269 | Idx:  280/400    | loss: 0.059 | grad_norm: 0.399 | acc: 99.47% |
| 2025-10-16 20:38:22.754242 | Idx:  320/400    | loss: 0.059 | grad_norm: 0.268 | acc: 99.62% |
| 2025-10-16 20:38:28.697689 | Idx:  360/400    | loss: 0.058 | grad_norm: 0.319 | acc: 98.83% |
| 2025-10-16 20:38:34.674136 | Idx:  400/400    | loss: 0.057 | grad_norm: 0.127 | acc: 99.70% |
Train Loss: 0.0575 Acc: 99.47%
Acc: 99.44%


| val_epoch_acc: 99.44% | epoch: 04 | avg_train_loss: 0.0365 | avg_train_acc: 99.4653 | 


Epoch 6/8
----------
| 2025-10-16 20:39:04.653827 | Idx:   40/400    | loss: 0.024 | grad_norm: 0.285 | acc: 99.80% |
| 2025-10-16 20:39:10.467514 | Idx:   80/400    | loss: 0.039 | grad_norm: 0.134 | acc: 99.76% |
| 2025-10-16 20:39:17.554425 | Idx:  120/400    | loss: 0.036 | grad_norm: 0.415 | acc: 99.77% |
| 2025-10-16 20:39:23.354948 | Idx:  160/400    | loss: 0.034 | grad_norm: 0.018 | acc: 99.77% |
| 2025-10-16 20:39:29.640739 | Idx:  200/400    | loss: 0.031 | grad_norm: 0.023 | acc: 99.81% |
| 2025-10-16 20:39:36.984084 | Idx:  240/400    | loss: 0.030 | grad_norm: 0.035 | acc: 99.76% |
| 2025-10-16 20:39:42.882092 | Idx:  280/400    | loss: 0.028 | grad_norm: 0.056 | acc: 99.77% |
| 2025-10-16 20:39:50.112025 | Idx:  320/400    | loss: 0.027 | grad_norm: 0.036 | acc: 99.79% |
| 2025-10-16 20:39:56.079582 | Idx:  360/400    | loss: 0.026 | grad_norm: 0.014 | acc: 99.83% |
| 2025-10-16 20:40:02.096684 | Idx:  400/400    | loss: 0.025 | grad_norm: 0.028 | acc: 99.81% |
Train Loss: 0.0247 Acc: 99.69%
Acc: 99.81%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_203103-ixx3kgkf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_203103-ixx3kgkf\logs[0m
rule: B3678/S34678 \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B3678/S34678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B3678_S34678/
Saving Base Directory: 2025-10-16_20-40-25_multiscale_0__200-200-B3678_S34678


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
����Sequential: 1-1                                       --                        --
��    ����R2Conv: 2-1                                      --                        26
��    ��    ����BlocksBasisExpansion: 3-1                   --                        --
��    ����InnerBatchNorm: 2-2                              --                        --
��    ��    ����BatchNorm3d: 3-2                            --                        4
��    ����ReLU: 2-3                                        --                        --
����Sequential: 1-2                                       --                        --
��    ����R2Conv: 2-4                                      --                        46
��    ��    ����BlocksBasisExpansion: 3-3                   --                        --
��    ����InnerBatchNorm: 2-5                              --                        --
��    ��    ����BatchNorm3d: 3-4                            --                        4
��    ����ReLU: 2-6                                        --                        --
����Sequential: 1-3                                       --                        --
��    ����R2Conv: 2-7                                      --                        34
��    ��    ����BlocksBasisExpansion: 3-5                   --                        --
��    ����InnerBatchNorm: 2-8                              --                        --
��    ��    ����BatchNorm3d: 3-6                            --                        4
��    ����ReLU: 2-9                                        --                        --
����Sequential: 1-4                                       --                        --
��    ����R2Conv: 2-10                                     --                        1,060
��    ��    ����BlocksBasisExpansion: 3-7                   --                        --
��    ����InnerBatchNorm: 2-11                             --                        --
��    ��    ����BatchNorm3d: 3-8                            --                        8
��    ����ReLU: 2-12                                       --                        --
��    ����R2Conv: 2-13                                     --                        90
��    ��    ����BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-16 20:40:44.732618 | Idx:   40/400    | loss: 1.079 | grad_norm: 0.232 | acc: 93.84% |
| 2025-10-16 20:40:50.532553 | Idx:   80/400    | loss: 0.713 | grad_norm: 0.200 | acc: 92.44% |
| 2025-10-16 20:40:57.710242 | Idx:  120/400    | loss: 0.573 | grad_norm: 0.194 | acc: 94.78% |
| 2025-10-16 20:41:03.567247 | Idx:  160/400    | loss: 0.509 | grad_norm: 0.133 | acc: 92.62% |
| 2025-10-16 20:41:09.427601 | Idx:  200/400    | loss: 0.468 | grad_norm: 0.246 | acc: 90.77% |
| 2025-10-16 20:41:16.622853 | Idx:  240/400    | loss: 0.441 | grad_norm: 0.166 | acc: 90.14% |
| 2025-10-16 20:41:22.628233 | Idx:  280/400    | loss: 0.424 | grad_norm: 0.105 | acc: 91.32% |
| 2025-10-16 20:41:30.020643 | Idx:  320/400    | loss: 0.408 | grad_norm: 0.244 | acc: 93.23% |
| 2025-10-16 20:41:36.108661 | Idx:  360/400    | loss: 0.392 | grad_norm: 0.076 | acc: 95.68% |
| 2025-10-16 20:41:42.710598 | Idx:  400/400    | loss: 0.377 | grad_norm: 0.235 | acc: 96.69% |
Train Loss: 0.3765 Acc: 91.31%
Acc: 95.14%


| val_epoch_acc: 95.14% | epoch: 00 | avg_train_loss: 0.2297 | avg_train_acc: 94.4108 | 


Epoch 2/8
----------
| 2025-10-16 20:42:15.705168 | Idx:   40/400    | loss: 0.217 | grad_norm: 0.173 | acc: 95.12% |
| 2025-10-16 20:42:21.812806 | Idx:   80/400    | loss: 0.216 | grad_norm: 0.307 | acc: 94.09% |
| 2025-10-16 20:42:29.546457 | Idx:  120/400    | loss: 0.212 | grad_norm: 0.390 | acc: 95.64% |
| 2025-10-16 20:42:35.873421 | Idx:  160/400    | loss: 0.211 | grad_norm: 0.202 | acc: 95.34% |
| 2025-10-16 20:42:42.349964 | Idx:  200/400    | loss: 0.209 | grad_norm: 0.863 | acc: 92.22% |
| 2025-10-16 20:42:50.377239 | Idx:  240/400    | loss: 0.210 | grad_norm: 0.596 | acc: 93.88% |
| 2025-10-16 20:42:56.935088 | Idx:  280/400    | loss: 0.206 | grad_norm: 0.435 | acc: 96.64% |
| 2025-10-16 20:43:04.772063 | Idx:  320/400    | loss: 0.205 | grad_norm: 0.244 | acc: 97.27% |
| 2025-10-16 20:43:11.042408 | Idx:  360/400    | loss: 0.200 | grad_norm: 0.505 | acc: 97.53% |
| 2025-10-16 20:43:17.429979 | Idx:  400/400    | loss: 0.196 | grad_norm: 0.540 | acc: 97.25% |
Train Loss: 0.1958 Acc: 95.54%
Acc: 97.76%


| val_epoch_acc: 97.76% | epoch: 01 | avg_train_loss: 0.1679 | avg_train_acc: 96.3220 | 


Epoch 3/8
----------
| 2025-10-16 20:43:50.315929 | Idx:   40/400    | loss: 0.125 | grad_norm: 0.609 | acc: 97.80% |
| 2025-10-16 20:43:56.517412 | Idx:   80/400    | loss: 0.143 | grad_norm: 0.630 | acc: 96.06% |
| 2025-10-16 20:44:04.004226 | Idx:  120/400    | loss: 0.156 | grad_norm: 1.479 | acc: 90.48% |
| 2025-10-16 20:44:10.184383 | Idx:  160/400    | loss: 0.152 | grad_norm: 0.214 | acc: 97.88% |
| 2025-10-16 20:44:16.346850 | Idx:  200/400    | loss: 0.147 | grad_norm: 0.371 | acc: 97.37% |
| 2025-10-16 20:44:23.782242 | Idx:  240/400    | loss: 0.143 | grad_norm: 0.566 | acc: 98.63% |
| 2025-10-16 20:44:29.956652 | Idx:  280/400    | loss: 0.146 | grad_norm: 1.687 | acc: 89.84% |
| 2025-10-16 20:44:37.861632 | Idx:  320/400    | loss: 0.143 | grad_norm: 0.321 | acc: 98.89% |
| 2025-10-16 20:44:44.106241 | Idx:  360/400    | loss: 0.148 | grad_norm: 0.473 | acc: 97.79% |
| 2025-10-16 20:44:50.311399 | Idx:  400/400    | loss: 0.143 | grad_norm: 0.415 | acc: 98.30% |
Train Loss: 0.1431 Acc: 97.12%
Acc: 99.07%


| val_epoch_acc: 99.07% | epoch: 02 | avg_train_loss: 0.1026 | avg_train_acc: 98.2266 | 


Epoch 4/8
----------
| 2025-10-16 20:45:20.890093 | Idx:   40/400    | loss: 0.102 | grad_norm: 0.924 | acc: 99.06% |
| 2025-10-16 20:45:26.997055 | Idx:   80/400    | loss: 0.118 | grad_norm: 0.751 | acc: 97.65% |
| 2025-10-16 20:45:34.514720 | Idx:  120/400    | loss: 0.110 | grad_norm: 0.382 | acc: 97.73% |
| 2025-10-16 20:45:40.678208 | Idx:  160/400    | loss: 0.110 | grad_norm: 0.236 | acc: 98.97% |
| 2025-10-16 20:45:46.836103 | Idx:  200/400    | loss: 0.105 | grad_norm: 1.153 | acc: 96.54% |
| 2025-10-16 20:45:54.404542 | Idx:  240/400    | loss: 0.117 | grad_norm: 0.258 | acc: 98.70% |
| 2025-10-16 20:46:00.634056 | Idx:  280/400    | loss: 0.116 | grad_norm: 0.247 | acc: 98.48% |
| 2025-10-16 20:46:08.661299 | Idx:  320/400    | loss: 0.117 | grad_norm: 0.297 | acc: 98.80% |
| 2025-10-16 20:46:14.806277 | Idx:  360/400    | loss: 0.113 | grad_norm: 0.129 | acc: 98.81% |
| 2025-10-16 20:46:21.011112 | Idx:  400/400    | loss: 0.112 | grad_norm: 0.722 | acc: 97.94% |
Train Loss: 0.1121 Acc: 97.94%
Acc: 98.23%


| val_epoch_acc: 98.23% | epoch: 03 | avg_train_loss: 0.1180 | avg_train_acc: 97.6350 | 


Epoch 5/8
----------
| 2025-10-16 20:46:51.863845 | Idx:   40/400    | loss: 0.105 | grad_norm: 0.124 | acc: 99.28% |
| 2025-10-16 20:46:58.237760 | Idx:   80/400    | loss: 0.093 | grad_norm: 0.300 | acc: 99.19% |
| 2025-10-16 20:47:05.911675 | Idx:  120/400    | loss: 0.085 | grad_norm: 0.232 | acc: 98.60% |
| 2025-10-16 20:47:12.129752 | Idx:  160/400    | loss: 0.079 | grad_norm: 0.494 | acc: 98.88% |
| 2025-10-16 20:47:18.357954 | Idx:  200/400    | loss: 0.080 | grad_norm: 0.295 | acc: 98.56% |
| 2025-10-16 20:47:25.898893 | Idx:  240/400    | loss: 0.081 | grad_norm: 0.587 | acc: 98.57% |
| 2025-10-16 20:47:32.097119 | Idx:  280/400    | loss: 0.082 | grad_norm: 0.571 | acc: 98.49% |
| 2025-10-16 20:47:39.594063 | Idx:  320/400    | loss: 0.079 | grad_norm: 0.645 | acc: 98.41% |
| 2025-10-16 20:47:46.452505 | Idx:  360/400    | loss: 0.077 | grad_norm: 0.134 | acc: 99.54% |
| 2025-10-16 20:47:53.770990 | Idx:  400/400    | loss: 0.075 | grad_norm: 0.133 | acc: 99.49% |
Train Loss: 0.0748 Acc: 98.70%
Acc: 99.47%


| val_epoch_acc: 99.47% | epoch: 04 | avg_train_loss: 0.0534 | avg_train_acc: 99.1704 | 


Epoch 6/8
----------
| 2025-10-16 20:48:25.015644 | Idx:   40/400    | loss: 0.047 | grad_norm: 0.147 | acc: 99.52% |
| 2025-10-16 20:48:31.524444 | Idx:   80/400    | loss: 0.050 | grad_norm: 0.105 | acc: 99.68% |
| 2025-10-16 20:48:39.094006 | Idx:  120/400    | loss: 0.048 | grad_norm: 0.101 | acc: 99.89% |
| 2025-10-16 20:48:45.243229 | Idx:  160/400    | loss: 0.081 | grad_norm: 0.748 | acc: 97.87% |
| 2025-10-16 20:48:51.395147 | Idx:  200/400    | loss: 0.081 | grad_norm: 0.306 | acc: 99.24% |
| 2025-10-16 20:48:58.945054 | Idx:  240/400    | loss: 0.079 | grad_norm: 0.849 | acc: 97.53% |
| 2025-10-16 20:49:05.600263 | Idx:  280/400    | loss: 0.077 | grad_norm: 0.434 | acc: 99.44% |
| 2025-10-16 20:49:13.818340 | Idx:  320/400    | loss: 0.079 | grad_norm: 0.743 | acc: 98.12% |
| 2025-10-16 20:49:20.078873 | Idx:  360/400    | loss: 0.081 | grad_norm: 0.228 | acc: 99.27% |
| 2025-10-16 20:49:28.079441 | Idx:  400/400    | loss: 0.081 | grad_norm: 0.101 | acc: 99.54% |
Train Loss: 0.0809 Acc: 98.61%
Acc: 99.09%


| val_epoch_acc: 99.09% | epoch: 05 | avg_train_loss: 0.0859 | avg_train_acc: 98.4419 | 


Epoch 7/8
----------
| 2025-10-16 20:50:00.386133 | Idx:   40/400    | loss: 0.070 | grad_norm: 0.171 | acc: 99.55% |
| 2025-10-16 20:50:07.360967 | Idx:   80/400    | loss: 0.062 | grad_norm: 1.020 | acc: 98.16% |
| 2025-10-16 20:50:15.580922 | Idx:  120/400    | loss: 0.076 | grad_norm: 0.531 | acc: 97.96% |
| 2025-10-16 20:50:21.692224 | Idx:  160/400    | loss: 0.074 | grad_norm: 0.218 | acc: 98.95% |
| 2025-10-16 20:50:27.833116 | Idx:  200/400    | loss: 0.073 | grad_norm: 0.417 | acc: 98.64% |
| 2025-10-16 20:50:35.264263 | Idx:  240/400    | loss: 0.079 | grad_norm: 0.383 | acc: 99.13% |
| 2025-10-16 20:50:41.414610 | Idx:  280/400    | loss: 0.078 | grad_norm: 0.703 | acc: 98.64% |
| 2025-10-16 20:50:48.853204 | Idx:  320/400    | loss: 0.077 | grad_norm: 0.099 | acc: 99.59% |
| 2025-10-16 20:50:55.027641 | Idx:  360/400    | loss: 0.075 | grad_norm: 0.056 | acc: 99.73% |
| 2025-10-16 20:51:01.550424 | Idx:  400/400    | loss: 0.074 | grad_norm: 0.150 | acc: 99.38% |
Train Loss: 0.0736 Acc: 98.82%
Acc: 99.60%


| val_epoch_acc: 99.60% | epoch: 06 | avg_train_loss: 0.0545 | avg_train_acc: 99.0849 | 


Epoch 8/8
----------
| 2025-10-16 20:51:33.352891 | Idx:   40/400    | loss: 0.061 | grad_norm: 0.174 | acc: 99.51% |
| 2025-10-16 20:51:39.467073 | Idx:   80/400    | loss: 0.090 | grad_norm: 0.147 | acc: 99.61% |
| 2025-10-16 20:51:46.960845 | Idx:  120/400    | loss: 0.078 | grad_norm: 0.230 | acc: 99.07% |
| 2025-10-16 20:51:53.309341 | Idx:  160/400    | loss: 0.071 | grad_norm: 0.663 | acc: 98.06% |
| 2025-10-16 20:52:00.373525 | Idx:  200/400    | loss: 0.067 | grad_norm: 0.194 | acc: 99.12% |
| 2025-10-16 20:52:08.380956 | Idx:  240/400    | loss: 0.064 | grad_norm: 0.223 | acc: 99.06% |
| 2025-10-16 20:52:14.934090 | Idx:  280/400    | loss: 0.062 | grad_norm: 0.140 | acc: 99.72% |
| 2025-10-16 20:52:22.805113 | Idx:  320/400    | loss: 0.061 | grad_norm: 0.793 | acc: 97.45% |
| 2025-10-16 20:52:29.392430 | Idx:  360/400    | loss: 0.070 | grad_norm: 0.193 | acc: 99.11% |
| 2025-10-16 20:52:35.854311 | Idx:  400/400    | loss: 0.070 | grad_norm: 0.208 | acc: 99.34% |
Train Loss: 0.0703 Acc: 98.88%
Acc: 99.39%


| val_epoch_acc: 99.39% | epoch: 07 | avg_train_loss: 0.0766 | avg_train_acc: 98.7937 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_204025-awuwyoc6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_204025-awuwyoc6\logs[0m
rule: B35678/S5678 \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B35678/S5678', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B35678_S5678/
Saving Base Directory: 2025-10-16_20-53-00_multiscale_0__200-200-B35678_S5678


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
����Sequential: 1-1                                       --                        --
��    ����R2Conv: 2-1                                      --                        26
��    ��    ����BlocksBasisExpansion: 3-1                   --                        --
��    ����InnerBatchNorm: 2-2                              --                        --
��    ��    ����BatchNorm3d: 3-2                            --                        4
��    ����ReLU: 2-3                                        --                        --
����Sequential: 1-2                                       --                        --
��    ����R2Conv: 2-4                                      --                        46
��    ��    ����BlocksBasisExpansion: 3-3                   --                        --
��    ����InnerBatchNorm: 2-5                              --                        --
��    ��    ����BatchNorm3d: 3-4                            --                        4
��    ����ReLU: 2-6                                        --                        --
����Sequential: 1-3                                       --                        --
��    ����R2Conv: 2-7                                      --                        34
��    ��    ����BlocksBasisExpansion: 3-5                   --                        --
��    ����InnerBatchNorm: 2-8                              --                        --
��    ��    ����BatchNorm3d: 3-6                            --                        4
��    ����ReLU: 2-9                                        --                        --
����Sequential: 1-4                                       --                        --
��    ����R2Conv: 2-10                                     --                        1,060
��    ��    ����BlocksBasisExpansion: 3-7                   --                        --
��    ����InnerBatchNorm: 2-11                             --                        --
��    ��    ����BatchNorm3d: 3-8                            --                        8
��    ����ReLU: 2-12                                       --                        --
��    ����R2Conv: 2-13                                     --                        90
��    ��    ����BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-16 20:53:20.956307 | Idx:   40/400    | loss: 0.662 | grad_norm: 0.574 | acc: 98.54% |
| 2025-10-16 20:53:27.980307 | Idx:   80/400    | loss: 0.354 | grad_norm: 0.015 | acc: 99.84% |
| 2025-10-16 20:53:36.194969 | Idx:  120/400    | loss: 0.247 | grad_norm: 0.073 | acc: 99.77% |
| 2025-10-16 20:53:43.381139 | Idx:  160/400    | loss: 0.193 | grad_norm: 0.124 | acc: 98.69% |
| 2025-10-16 20:53:50.550698 | Idx:  200/400    | loss: 0.158 | grad_norm: 0.009 | acc: 99.77% |
| 2025-10-16 20:53:58.158539 | Idx:  240/400    | loss: 0.136 | grad_norm: 0.466 | acc: 98.65% |
| 2025-10-16 20:54:04.489307 | Idx:  280/400    | loss: 0.127 | grad_norm: 0.232 | acc: 99.48% |
| 2025-10-16 20:54:12.553622 | Idx:  320/400    | loss: 0.117 | grad_norm: 0.191 | acc: 99.11% |
| 2025-10-16 20:54:19.278097 | Idx:  360/400    | loss: 0.109 | grad_norm: 0.010 | acc: 99.85% |
| 2025-10-16 20:54:25.491107 | Idx:  400/400    | loss: 0.101 | grad_norm: 0.054 | acc: 99.19% |
Train Loss: 0.1011 Acc: 98.44%
Acc: 99.39%


| val_epoch_acc: 99.39% | epoch: 00 | avg_train_loss: 0.0284 | avg_train_acc: 99.3844 | 


Epoch 2/8
----------
| 2025-10-16 20:54:56.206037 | Idx:   40/400    | loss: 0.052 | grad_norm: 0.092 | acc: 98.39% |
| 2025-10-16 20:55:02.386532 | Idx:   80/400    | loss: 0.048 | grad_norm: 0.083 | acc: 99.18% |
| 2025-10-16 20:55:10.184943 | Idx:  120/400    | loss: 0.047 | grad_norm: 0.865 | acc: 96.84% |
| 2025-10-16 20:55:16.538081 | Idx:  160/400    | loss: 0.044 | grad_norm: 0.007 | acc: 99.46% |
| 2025-10-16 20:55:22.831329 | Idx:  200/400    | loss: 0.042 | grad_norm: 0.004 | acc: 99.92% |
| 2025-10-16 20:55:30.859107 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.004 | acc: 99.93% |
| 2025-10-16 20:55:36.960506 | Idx:  280/400    | loss: 0.042 | grad_norm: 0.012 | acc: 99.93% |
| 2025-10-16 20:55:44.245330 | Idx:  320/400    | loss: 0.039 | grad_norm: 0.012 | acc: 99.79% |
| 2025-10-16 20:55:50.498168 | Idx:  360/400    | loss: 0.037 | grad_norm: 0.004 | acc: 99.98% |
| 2025-10-16 20:55:56.879907 | Idx:  400/400    | loss: 0.036 | grad_norm: 0.077 | acc: 99.15% |
Train Loss: 0.0358 Acc: 99.38%
Acc: 99.52%


| val_epoch_acc: 99.52% | epoch: 01 | avg_train_loss: 0.0178 | avg_train_acc: 99.7430 | 


Epoch 3/8
----------
| 2025-10-16 20:56:26.904297 | Idx:   40/400    | loss: 0.028 | grad_norm: 0.095 | acc: 99.04% |
| 2025-10-16 20:56:33.645406 | Idx:   80/400    | loss: 0.027 | grad_norm: 0.129 | acc: 99.44% |
| 2025-10-16 20:56:41.326558 | Idx:  120/400    | loss: 0.026 | grad_norm: 0.007 | acc: 99.88% |
| 2025-10-16 20:56:47.605798 | Idx:  160/400    | loss: 0.025 | grad_norm: 0.004 | acc: 99.79% |
| 2025-10-16 20:56:53.907219 | Idx:  200/400    | loss: 0.025 | grad_norm: 0.003 | acc: 99.90% |
| 2025-10-16 20:57:01.642985 | Idx:  240/400    | loss: 0.026 | grad_norm: 0.021 | acc: 99.41% |
| 2025-10-16 20:57:07.921546 | Idx:  280/400    | loss: 0.025 | grad_norm: 0.046 | acc: 99.43% |
| 2025-10-16 20:57:15.775514 | Idx:  320/400    | loss: 0.024 | grad_norm: 0.004 | acc: 99.74% |
| 2025-10-16 20:57:22.067497 | Idx:  360/400    | loss: 0.028 | grad_norm: 0.023 | acc: 99.65% |
| 2025-10-16 20:57:28.343201 | Idx:  400/400    | loss: 0.028 | grad_norm: 0.038 | acc: 99.10% |
Train Loss: 0.0280 Acc: 99.32%
Acc: 99.56%


| val_epoch_acc: 99.56% | epoch: 02 | avg_train_loss: 0.0300 | avg_train_acc: 99.3890 | 


Epoch 4/8
----------
| 2025-10-16 20:58:01.042497 | Idx:   40/400    | loss: 0.033 | grad_norm: 0.062 | acc: 98.94% |
| 2025-10-16 20:58:07.911535 | Idx:   80/400    | loss: 0.042 | grad_norm: 0.047 | acc: 99.68% |
| 2025-10-16 20:58:15.936561 | Idx:  120/400    | loss: 0.047 | grad_norm: 0.419 | acc: 99.07% |
| 2025-10-16 20:58:22.352213 | Idx:  160/400    | loss: 0.045 | grad_norm: 0.050 | acc: 99.06% |
| 2025-10-16 20:58:28.887600 | Idx:  200/400    | loss: 0.042 | grad_norm: 0.033 | acc: 99.29% |
| 2025-10-16 20:58:36.790768 | Idx:  240/400    | loss: 0.039 | grad_norm: 0.005 | acc: 99.79% |
| 2025-10-16 20:58:44.074928 | Idx:  280/400    | loss: 0.037 | grad_norm: 0.193 | acc: 98.17% |
| 2025-10-16 20:58:52.433339 | Idx:  320/400    | loss: 0.036 | grad_norm: 0.004 | acc: 99.99% |
| 2025-10-16 20:58:59.695931 | Idx:  360/400    | loss: 0.034 | grad_norm: 0.004 | acc: 99.70% |
| 2025-10-16 20:59:06.367451 | Idx:  400/400    | loss: 0.033 | grad_norm: 0.003 | acc: 99.99% |
Train Loss: 0.0330 Acc: 99.41%
Acc: 99.62%


| val_epoch_acc: 99.62% | epoch: 03 | avg_train_loss: 0.0182 | avg_train_acc: 99.6572 | 


Epoch 5/8
----------
| 2025-10-16 20:59:38.657143 | Idx:   40/400    | loss: 0.015 | grad_norm: 0.017 | acc: 99.93% |
| 2025-10-16 20:59:44.906432 | Idx:   80/400    | loss: 0.022 | grad_norm: 0.025 | acc: 99.81% |
| 2025-10-16 20:59:52.356638 | Idx:  120/400    | loss: 0.020 | grad_norm: 0.008 | acc: 99.14% |
| 2025-10-16 20:59:59.081746 | Idx:  160/400    | loss: 0.027 | grad_norm: 9.857 | acc: 99.80% |
| 2025-10-16 21:00:05.666101 | Idx:  200/400    | loss: 0.031 | grad_norm: 0.034 | acc: 99.64% |
| 2025-10-16 21:00:13.273391 | Idx:  240/400    | loss: 0.030 | grad_norm: 0.006 | acc: 99.82% |
| 2025-10-16 21:00:19.532500 | Idx:  280/400    | loss: 0.032 | grad_norm: 0.014 | acc: 99.98% |
| 2025-10-16 21:00:27.023682 | Idx:  320/400    | loss: 0.034 | grad_norm: 0.009 | acc: 99.86% |
| 2025-10-16 21:00:33.542511 | Idx:  360/400    | loss: 0.034 | grad_norm: 0.028 | acc: 99.53% |
| 2025-10-16 21:00:39.787493 | Idx:  400/400    | loss: 0.033 | grad_norm: 0.009 | acc: 99.85% |
Train Loss: 0.0330 Acc: 99.47%
Acc: 99.44%


| val_epoch_acc: 99.44% | epoch: 04 | avg_train_loss: 0.0254 | avg_train_acc: 99.5276 | 


Epoch 6/8
----------
| 2025-10-16 21:01:11.677014 | Idx:   40/400    | loss: 0.035 | grad_norm: 0.138 | acc: 98.48% |
| 2025-10-16 21:01:18.033995 | Idx:   80/400    | loss: 0.026 | grad_norm: 0.003 | acc: 99.80% |
| 2025-10-16 21:01:25.843818 | Idx:  120/400    | loss: 0.026 | grad_norm: 0.064 | acc: 99.48% |
| 2025-10-16 21:01:32.503508 | Idx:  160/400    | loss: 0.026 | grad_norm: 0.027 | acc: 99.55% |
| 2025-10-16 21:01:39.730141 | Idx:  200/400    | loss: 0.025 | grad_norm: 0.021 | acc: 99.90% |
| 2025-10-16 21:01:48.932584 | Idx:  240/400    | loss: 0.025 | grad_norm: 0.154 | acc: 99.28% |
| 2025-10-16 21:01:57.028157 | Idx:  280/400    | loss: 0.026 | grad_norm: 0.052 | acc: 99.08% |
| 2025-10-16 21:02:06.604433 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.038 | acc: 99.53% |
| 2025-10-16 21:02:14.224779 | Idx:  360/400    | loss: 0.026 | grad_norm: 0.019 | acc: 99.24% |
| 2025-10-16 21:02:21.764490 | Idx:  400/400    | loss: 0.026 | grad_norm: 0.009 | acc: 99.92% |
Train Loss: 0.0256 Acc: 99.48%
Acc: 99.64%


| val_epoch_acc: 99.64% | epoch: 05 | avg_train_loss: 0.0230 | avg_train_acc: 99.5456 | 


Epoch 7/8
----------
| 2025-10-16 21:03:00.735087 | Idx:   40/400    | loss: 0.021 | grad_norm: 0.021 | acc: 99.68% |
| 2025-10-16 21:03:08.284755 | Idx:   80/400    | loss: 0.026 | grad_norm: 0.002 | acc: 99.96% |
| 2025-10-16 21:03:17.058916 | Idx:  120/400    | loss: 0.024 | grad_norm: 0.102 | acc: 97.70% |
| 2025-10-16 21:03:23.414375 | Idx:  160/400    | loss: 0.022 | grad_norm: 0.006 | acc: 99.81% |
| 2025-10-16 21:03:30.024114 | Idx:  200/400    | loss: 0.020 | grad_norm: 0.010 | acc: 99.75% |
| 2025-10-16 21:03:37.959909 | Idx:  240/400    | loss: 0.020 | grad_norm: 0.002 | acc: 99.95% |
| 2025-10-16 21:03:44.783086 | Idx:  280/400    | loss: 0.020 | grad_norm: 0.006 | acc: 99.92% |
| 2025-10-16 21:03:52.758920 | Idx:  320/400    | loss: 0.022 | grad_norm: 0.012 | acc: 99.76% |
| 2025-10-16 21:03:59.431603 | Idx:  360/400    | loss: 0.022 | grad_norm: 0.024 | acc: 98.74% |
| 2025-10-16 21:04:05.996791 | Idx:  400/400    | loss: 0.022 | grad_norm: 0.004 | acc: 99.94% |
Train Loss: 0.0222 Acc: 99.54%
Acc: 99.64%


| val_epoch_acc: 99.64% | epoch: 06 | avg_train_loss: 0.0315 | avg_train_acc: 99.3191 | 


Epoch 8/8
----------
| 2025-10-16 21:04:41.215085 | Idx:   40/400    | loss: 0.021 | grad_norm: 0.022 | acc: 99.46% |
| 2025-10-16 21:04:47.974155 | Idx:   80/400    | loss: 0.019 | grad_norm: 0.019 | acc: 99.73% |
| 2025-10-16 21:04:55.771091 | Idx:  120/400    | loss: 0.020 | grad_norm: 0.008 | acc: 99.83% |
| 2025-10-16 21:05:02.263669 | Idx:  160/400    | loss: 0.022 | grad_norm: 0.048 | acc: 98.79% |
| 2025-10-16 21:05:09.162877 | Idx:  200/400    | loss: 0.021 | grad_norm: 0.003 | acc: 99.92% |
| 2025-10-16 21:05:18.561076 | Idx:  240/400    | loss: 0.021 | grad_norm: 0.010 | acc: 99.82% |
| 2025-10-16 21:05:26.497276 | Idx:  280/400    | loss: 0.021 | grad_norm: 0.033 | acc: 99.20% |
| 2025-10-16 21:05:35.594513 | Idx:  320/400    | loss: 0.020 | grad_norm: 0.008 | acc: 99.86% |
| 2025-10-16 21:05:42.187056 | Idx:  360/400    | loss: 0.021 | grad_norm: 0.011 | acc: 99.71% |
| 2025-10-16 21:05:48.467690 | Idx:  400/400    | loss: 0.021 | grad_norm: 0.168 | acc: 97.63% |
Train Loss: 0.0215 Acc: 99.57%
Acc: 99.67%


| val_epoch_acc: 99.67% | epoch: 07 | avg_train_loss: 0.0306 | avg_train_acc: 99.3936 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251016_205300-2vuvtsoj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251016_205300-2vuvtsoj\logs[0m
