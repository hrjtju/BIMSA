rule: B2/S \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_19-41-24_small_2_layer_seq_p4cnn__200-200-B2_S


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
©À©¤R2Conv: 1-1                                      --                        176
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        16
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        2,816
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        16
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        178
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 19:41:51.153592 | Idx:   40/400    | loss: 0.759 | grad_norm: 2.469 | acc: 97.38% |
| 2025-10-17 19:41:59.225861 | Idx:   80/400    | loss: 0.421 | grad_norm: 0.104 | acc: 99.61% |
| 2025-10-17 19:42:08.671280 | Idx:  120/400    | loss: 0.291 | grad_norm: 0.231 | acc: 100.00% |
| 2025-10-17 19:42:16.671598 | Idx:  160/400    | loss: 0.222 | grad_norm: 0.127 | acc: 100.00% |
| 2025-10-17 19:42:24.770828 | Idx:  200/400    | loss: 0.179 | grad_norm: 0.021 | acc: 100.00% |
| 2025-10-17 19:42:34.610763 | Idx:  240/400    | loss: 0.151 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 19:42:42.840015 | Idx:  280/400    | loss: 0.131 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:42:52.914198 | Idx:  320/400    | loss: 0.115 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:43:01.148861 | Idx:  360/400    | loss: 0.103 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 19:43:09.164817 | Idx:  400/400    | loss: 0.093 | grad_norm: 0.027 | acc: 100.00% |
Train Loss: 0.0928 Acc: 97.73%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 00 | avg_train_loss: 0.0047 | avg_train_acc: 99.9972 | 


Epoch 2/8
----------
| 2025-10-17 19:43:44.120537 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 19:43:52.511672 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:44:02.063867 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:44:10.023175 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:44:17.961587 | Idx:  200/400    | loss: 0.031 | grad_norm: 0.273 | acc: 99.96% |
| 2025-10-17 19:44:27.769257 | Idx:  240/400    | loss: 0.028 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 19:44:35.909829 | Idx:  280/400    | loss: 0.026 | grad_norm: 0.252 | acc: 99.96% |
| 2025-10-17 19:44:45.566784 | Idx:  320/400    | loss: 0.024 | grad_norm: 0.171 | acc: 99.89% |
| 2025-10-17 19:44:53.552046 | Idx:  360/400    | loss: 0.022 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 19:45:01.491715 | Idx:  400/400    | loss: 0.020 | grad_norm: 0.007 | acc: 100.00% |
Train Loss: 0.0202 Acc: 99.78%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0047 | avg_train_acc: 99.9999 | 


Epoch 3/8
----------
| 2025-10-17 19:45:35.358093 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 19:45:43.234728 | Idx:   80/400    | loss: 0.005 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 19:45:52.551492 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:46:00.411415 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:46:08.302722 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:46:17.817824 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:46:25.659997 | Idx:  280/400    | loss: 0.028 | grad_norm: 1.279 | acc: 99.66% |
| 2025-10-17 19:46:34.927959 | Idx:  320/400    | loss: 0.027 | grad_norm: 0.030 | acc: 100.00% |
| 2025-10-17 19:46:42.802898 | Idx:  360/400    | loss: 0.024 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-17 19:46:50.652777 | Idx:  400/400    | loss: 0.022 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0225 Acc: 99.83%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0053 | avg_train_acc: 99.9999 | 


Epoch 4/8
----------
| 2025-10-17 19:47:23.348361 | Idx:   40/400    | loss: 0.006 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:47:31.203863 | Idx:   80/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 19:47:40.506914 | Idx:  120/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 19:47:48.362812 | Idx:  160/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 19:47:56.261510 | Idx:  200/400    | loss: 0.005 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:48:05.666639 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:48:13.704814 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 19:48:23.912164 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 19:48:32.031336 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:48:40.165339 | Idx:  400/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0041 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0033 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 19:49:14.185642 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 19:49:22.321898 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:49:31.946592 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:49:40.447378 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:49:48.823535 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.021 | acc: 99.99% |
| 2025-10-17 19:49:59.079295 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.028 | acc: 100.00% |
| 2025-10-17 19:50:07.340825 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:50:17.478566 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:50:25.529580 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:50:33.797322 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
Train Loss: 0.0028 Acc: 100.00%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_194124-tsww1nyz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_194124-tsww1nyz\logs[0m
rule: B345/S5 \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_19-51-00_small_2_layer_seq_p4cnn__200-200-B345_S5


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
©À©¤R2Conv: 1-1                                      --                        176
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        16
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        2,816
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        16
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        178
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 19:51:22.362899 | Idx:   40/400    | loss: 0.768 | grad_norm: 0.905 | acc: 95.79% |
| 2025-10-17 19:51:30.337330 | Idx:   80/400    | loss: 0.459 | grad_norm: 0.243 | acc: 99.17% |
| 2025-10-17 19:51:39.766424 | Idx:  120/400    | loss: 0.319 | grad_norm: 0.139 | acc: 99.93% |
| 2025-10-17 19:51:47.815146 | Idx:  160/400    | loss: 0.244 | grad_norm: 0.118 | acc: 99.95% |
| 2025-10-17 19:51:55.854169 | Idx:  200/400    | loss: 0.197 | grad_norm: 0.015 | acc: 99.99% |
| 2025-10-17 19:52:05.696184 | Idx:  240/400    | loss: 0.167 | grad_norm: 0.139 | acc: 99.96% |
| 2025-10-17 19:52:14.154025 | Idx:  280/400    | loss: 0.144 | grad_norm: 0.047 | acc: 99.99% |
| 2025-10-17 19:52:24.444539 | Idx:  320/400    | loss: 0.127 | grad_norm: 0.036 | acc: 100.00% |
| 2025-10-17 19:52:32.520270 | Idx:  360/400    | loss: 0.114 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:52:40.485856 | Idx:  400/400    | loss: 0.103 | grad_norm: 0.157 | acc: 99.96% |
Train Loss: 0.1030 Acc: 98.20%
Acc: 99.93%


| val_epoch_acc: 99.93% | epoch: 00 | avg_train_loss: 0.0055 | avg_train_acc: 99.9952 | 


Epoch 2/8
----------
| 2025-10-17 19:53:13.922526 | Idx:   40/400    | loss: 0.031 | grad_norm: 0.026 | acc: 100.00% |
| 2025-10-17 19:53:22.173451 | Idx:   80/400    | loss: 0.019 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-17 19:53:31.769242 | Idx:  120/400    | loss: 0.014 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:53:39.785694 | Idx:  160/400    | loss: 0.012 | grad_norm: 0.035 | acc: 99.98% |
| 2025-10-17 19:53:48.182078 | Idx:  200/400    | loss: 0.018 | grad_norm: 0.225 | acc: 99.90% |
| 2025-10-17 19:53:57.722126 | Idx:  240/400    | loss: 0.016 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:54:05.909083 | Idx:  280/400    | loss: 0.015 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:54:15.383553 | Idx:  320/400    | loss: 0.013 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:54:23.368672 | Idx:  360/400    | loss: 0.012 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:54:31.578687 | Idx:  400/400    | loss: 0.012 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0116 Acc: 99.87%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0042 | avg_train_acc: 99.9984 | 


Epoch 3/8
----------
| 2025-10-17 19:55:05.317948 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:55:13.256130 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 19:55:22.664955 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:55:30.608233 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 19:55:38.565353 | Idx:  200/400    | loss: 0.007 | grad_norm: 2.353 | acc: 98.65% |
| 2025-10-17 19:55:48.502212 | Idx:  240/400    | loss: 0.020 | grad_norm: 0.160 | acc: 99.98% |
| 2025-10-17 19:55:56.599715 | Idx:  280/400    | loss: 0.019 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:56:06.104981 | Idx:  320/400    | loss: 0.017 | grad_norm: 0.043 | acc: 99.99% |
| 2025-10-17 19:56:14.140314 | Idx:  360/400    | loss: 0.016 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:56:22.160993 | Idx:  400/400    | loss: 0.015 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0147 Acc: 99.84%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0042 | avg_train_acc: 99.9987 | 


Epoch 4/8
----------
| 2025-10-17 19:56:55.962343 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:57:04.077671 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:57:13.632416 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:57:21.966658 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:57:30.083655 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 19:57:39.565606 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.016 | acc: 100.00% |
| 2025-10-17 19:57:47.691315 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 19:57:57.869381 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 19:58:05.902138 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:58:14.205990 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.014 | acc: 100.00% |
Train Loss: 0.0034 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0027 | avg_train_acc: 99.9996 | 


Epoch 5/8
----------
| 2025-10-17 19:58:48.323883 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:58:56.590721 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 19:59:06.198668 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 19:59:14.231290 | Idx:  160/400    | loss: 0.017 | grad_norm: 0.035 | acc: 99.99% |
| 2025-10-17 19:59:22.296863 | Idx:  200/400    | loss: 0.020 | grad_norm: 0.093 | acc: 99.92% |
| 2025-10-17 19:59:32.075881 | Idx:  240/400    | loss: 0.019 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-17 19:59:40.149665 | Idx:  280/400    | loss: 0.017 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 19:59:49.870676 | Idx:  320/400    | loss: 0.015 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 19:59:57.903078 | Idx:  360/400    | loss: 0.015 | grad_norm: 0.079 | acc: 99.99% |
| 2025-10-17 20:00:06.046831 | Idx:  400/400    | loss: 0.023 | grad_norm: 0.024 | acc: 100.00% |
Train Loss: 0.0226 Acc: 99.74%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 04 | avg_train_loss: 0.0304 | avg_train_acc: 99.5329 | 


Epoch 6/8
----------
| 2025-10-17 20:00:39.693037 | Idx:   40/400    | loss: 0.013 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:00:47.939278 | Idx:   80/400    | loss: 0.013 | grad_norm: 0.009 | acc: 99.99% |
| 2025-10-17 20:00:57.470542 | Idx:  120/400    | loss: 0.010 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:01:05.552734 | Idx:  160/400    | loss: 0.008 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:01:13.591108 | Idx:  200/400    | loss: 0.007 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:01:23.069224 | Idx:  240/400    | loss: 0.007 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-17 20:01:31.099128 | Idx:  280/400    | loss: 0.006 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:01:40.686042 | Idx:  320/400    | loss: 0.006 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:01:48.984376 | Idx:  360/400    | loss: 0.006 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:01:57.273307 | Idx:  400/400    | loss: 0.005 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0054 Acc: 99.97%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_195100-0bghy8nx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_195100-0bghy8nx\logs[0m
rule: B13/S012V \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_20-02-21_small_2_layer_seq_p4cnn__200-200-B13_S012V


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
©À©¤R2Conv: 1-1                                      --                        176
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        16
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        2,816
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        16
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        178
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:02:43.312819 | Idx:   40/400    | loss: 0.551 | grad_norm: 0.649 | acc: 98.04% |
| 2025-10-17 20:02:51.297318 | Idx:   80/400    | loss: 0.329 | grad_norm: 0.189 | acc: 98.82% |
| 2025-10-17 20:03:00.891082 | Idx:  120/400    | loss: 0.235 | grad_norm: 0.102 | acc: 99.88% |
| 2025-10-17 20:03:09.075370 | Idx:  160/400    | loss: 0.181 | grad_norm: 0.022 | acc: 99.97% |
| 2025-10-17 20:03:17.230837 | Idx:  200/400    | loss: 0.146 | grad_norm: 0.040 | acc: 99.99% |
| 2025-10-17 20:03:26.869478 | Idx:  240/400    | loss: 0.123 | grad_norm: 0.023 | acc: 100.00% |
| 2025-10-17 20:03:35.026850 | Idx:  280/400    | loss: 0.106 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 20:03:44.656474 | Idx:  320/400    | loss: 0.093 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 20:03:52.693726 | Idx:  360/400    | loss: 0.083 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:04:00.654085 | Idx:  400/400    | loss: 0.075 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0752 Acc: 98.59%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 00 | avg_train_loss: 0.0031 | avg_train_acc: 99.9999 | 


Epoch 2/8
----------
| 2025-10-17 20:04:32.859598 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:04:41.342693 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:04:50.905349 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:04:58.888155 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:05:07.143151 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 20:05:16.621288 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:05:24.768230 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:05:34.621176 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:05:42.582114 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:05:50.543274 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0021 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0016 | avg_train_acc: 100.0000 | 


Epoch 3/8
----------
| 2025-10-17 20:06:22.482519 | Idx:   40/400    | loss: 0.302 | grad_norm: 0.761 | acc: 90.38% |
| 2025-10-17 20:06:30.667395 | Idx:   80/400    | loss: 0.250 | grad_norm: 0.433 | acc: 98.81% |
| 2025-10-17 20:06:40.052866 | Idx:  120/400    | loss: 0.186 | grad_norm: 0.099 | acc: 99.91% |
| 2025-10-17 20:06:48.085059 | Idx:  160/400    | loss: 0.142 | grad_norm: 0.046 | acc: 100.00% |
| 2025-10-17 20:06:56.111577 | Idx:  200/400    | loss: 0.115 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:07:05.609893 | Idx:  240/400    | loss: 0.097 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:07:13.894982 | Idx:  280/400    | loss: 0.084 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:07:23.417175 | Idx:  320/400    | loss: 0.074 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:07:31.546318 | Idx:  360/400    | loss: 0.066 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:07:39.621465 | Idx:  400/400    | loss: 0.060 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0601 Acc: 99.22%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0044 | avg_train_acc: 99.9995 | 


Epoch 4/8
----------
| 2025-10-17 20:08:11.587931 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:08:19.612133 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:08:29.265159 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:08:37.522024 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:08:45.585384 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 20:08:55.332171 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:09:03.481545 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:09:13.047114 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:09:21.074424 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:09:29.073526 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0031 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0024 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 20:10:03.513133 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:10:11.743774 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:10:21.633807 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:10:29.640973 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:10:37.650496 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:10:47.384731 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:10:55.612211 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:11:05.214423 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 20:11:13.188073 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:11:21.161400 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0020 Acc: 100.00%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_200221-5xb2shbg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_200221-5xb2shbg\logs[0m
rule: B2/S013V \t, network: small_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'small_2_layer_seq_p4cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_p4cnn.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_20-11-44_small_2_layer_seq_p4cnn__200-200-B2_S013V


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNSmall                                   --                        --
©À©¤R2Conv: 1-1                                      --                        176
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        16
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        2,816
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        16
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        178
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:12:05.847709 | Idx:   40/400    | loss: 1.021 | grad_norm: 1.191 | acc: 82.69% |
| 2025-10-17 20:12:14.040813 | Idx:   80/400    | loss: 0.753 | grad_norm: 0.410 | acc: 91.86% |
| 2025-10-17 20:12:23.634940 | Idx:  120/400    | loss: 0.618 | grad_norm: 0.850 | acc: 95.18% |
| 2025-10-17 20:12:31.711201 | Idx:  160/400    | loss: 0.499 | grad_norm: 0.341 | acc: 98.92% |
| 2025-10-17 20:12:39.672265 | Idx:  200/400    | loss: 0.413 | grad_norm: 0.362 | acc: 99.76% |
| 2025-10-17 20:12:49.245928 | Idx:  240/400    | loss: 0.355 | grad_norm: 0.260 | acc: 99.43% |
| 2025-10-17 20:12:57.365067 | Idx:  280/400    | loss: 0.308 | grad_norm: 0.292 | acc: 99.68% |
| 2025-10-17 20:13:06.835198 | Idx:  320/400    | loss: 0.273 | grad_norm: 0.043 | acc: 99.95% |
| 2025-10-17 20:13:14.780839 | Idx:  360/400    | loss: 0.244 | grad_norm: 0.050 | acc: 99.99% |
| 2025-10-17 20:13:22.715087 | Idx:  400/400    | loss: 0.222 | grad_norm: 0.274 | acc: 99.45% |
Train Loss: 0.2221 Acc: 94.86%
Acc: 98.04%


| val_epoch_acc: 98.04% | epoch: 00 | avg_train_loss: 0.0344 | avg_train_acc: 99.4934 | 


Epoch 2/8
----------
| 2025-10-17 20:13:54.116614 | Idx:   40/400    | loss: 0.036 | grad_norm: 0.027 | acc: 99.98% |
| 2025-10-17 20:14:02.117959 | Idx:   80/400    | loss: 0.023 | grad_norm: 0.020 | acc: 99.99% |
| 2025-10-17 20:14:11.795574 | Idx:  120/400    | loss: 0.019 | grad_norm: 0.056 | acc: 99.97% |
| 2025-10-17 20:14:19.856793 | Idx:  160/400    | loss: 0.016 | grad_norm: 0.017 | acc: 100.00% |
| 2025-10-17 20:14:28.026309 | Idx:  200/400    | loss: 0.014 | grad_norm: 0.018 | acc: 100.00% |
| 2025-10-17 20:14:37.526150 | Idx:  240/400    | loss: 0.013 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 20:14:45.618994 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:14:55.419945 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.093 | acc: 99.99% |
| 2025-10-17 20:15:03.626177 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 20:15:11.787076 | Idx:  400/400    | loss: 0.010 | grad_norm: 0.014 | acc: 100.00% |
Train Loss: 0.0099 Acc: 99.94%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0048 | avg_train_acc: 99.9984 | 


Epoch 3/8
----------
| 2025-10-17 20:15:43.554748 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 20:15:51.513466 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.017 | acc: 100.00% |
| 2025-10-17 20:16:01.397092 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:16:09.494330 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:16:17.461340 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:16:26.841713 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:16:34.991038 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 20:16:44.397841 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 20:16:52.382067 | Idx:  360/400    | loss: 0.055 | grad_norm: 0.452 | acc: 98.50% |
| 2025-10-17 20:17:00.618906 | Idx:  400/400    | loss: 0.054 | grad_norm: 0.191 | acc: 99.52% |
Train Loss: 0.0543 Acc: 99.24%
Acc: 99.67%


| val_epoch_acc: 99.67% | epoch: 02 | avg_train_loss: 0.0453 | avg_train_acc: 99.3714 | 


Epoch 4/8
----------
| 2025-10-17 20:17:33.008465 | Idx:   40/400    | loss: 0.019 | grad_norm: 0.024 | acc: 99.98% |
| 2025-10-17 20:17:41.059959 | Idx:   80/400    | loss: 0.014 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 20:17:50.698545 | Idx:  120/400    | loss: 0.012 | grad_norm: 0.046 | acc: 99.98% |
| 2025-10-17 20:17:58.845047 | Idx:  160/400    | loss: 0.011 | grad_norm: 0.019 | acc: 99.99% |
| 2025-10-17 20:18:06.818292 | Idx:  200/400    | loss: 0.019 | grad_norm: 0.166 | acc: 99.73% |
| 2025-10-17 20:18:16.394133 | Idx:  240/400    | loss: 0.017 | grad_norm: 0.033 | acc: 99.99% |
| 2025-10-17 20:18:24.616362 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:18:34.237244 | Idx:  320/400    | loss: 0.015 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-17 20:18:42.257186 | Idx:  360/400    | loss: 0.014 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-17 20:18:50.452977 | Idx:  400/400    | loss: 0.013 | grad_norm: 0.006 | acc: 100.00% |
Train Loss: 0.0127 Acc: 99.90%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0050 | avg_train_acc: 99.9986 | 


Epoch 5/8
----------
| 2025-10-17 20:19:21.794890 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:19:29.838828 | Idx:   80/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:19:39.556434 | Idx:  120/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:19:47.685825 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:19:55.983235 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.045 | acc: 99.99% |
| 2025-10-17 20:20:05.690346 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:20:13.913321 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-17 20:20:23.651001 | Idx:  320/400    | loss: 0.032 | grad_norm: 0.748 | acc: 95.19% |
| 2025-10-17 20:20:31.709884 | Idx:  360/400    | loss: 0.037 | grad_norm: 0.051 | acc: 99.96% |
| 2025-10-17 20:20:39.732306 | Idx:  400/400    | loss: 0.034 | grad_norm: 0.058 | acc: 99.98% |
Train Loss: 0.0345 Acc: 99.58%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_201144-ergkc9lb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_201144-ergkc9lb\logs[0m
rule: B2/S \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_20-21-02_tiny_2_layer_seq_cnn__200-200-B2_S


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
©À©¤R2Conv: 1-1                                      --                        22
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        2
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        44
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        2
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        24
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:21:19.727656 | Idx:   40/400    | loss: 1.398 | grad_norm: 0.315 | acc: 62.71% |
| 2025-10-17 20:21:22.813779 | Idx:   80/400    | loss: 1.181 | grad_norm: 0.198 | acc: 76.69% |
| 2025-10-17 20:21:27.377875 | Idx:  120/400    | loss: 1.046 | grad_norm: 0.295 | acc: 83.95% |
| 2025-10-17 20:21:30.435311 | Idx:  160/400    | loss: 0.955 | grad_norm: 0.352 | acc: 83.63% |
| 2025-10-17 20:21:33.521478 | Idx:  200/400    | loss: 0.895 | grad_norm: 0.376 | acc: 84.04% |
| 2025-10-17 20:21:38.565609 | Idx:  240/400    | loss: 0.854 | grad_norm: 0.396 | acc: 84.88% |
| 2025-10-17 20:21:41.696380 | Idx:  280/400    | loss: 0.823 | grad_norm: 0.516 | acc: 85.27% |
| 2025-10-17 20:21:46.353409 | Idx:  320/400    | loss: 0.798 | grad_norm: 0.262 | acc: 86.35% |
| 2025-10-17 20:21:49.487179 | Idx:  360/400    | loss: 0.779 | grad_norm: 0.536 | acc: 86.37% |
| 2025-10-17 20:21:52.575667 | Idx:  400/400    | loss: 0.762 | grad_norm: 0.405 | acc: 86.18% |
Train Loss: 0.7624 Acc: 81.11%
Acc: 85.30%


| val_epoch_acc: 85.30% | epoch: 00 | avg_train_loss: 0.6093 | avg_train_acc: 86.1532 | 


Epoch 2/8
----------
| 2025-10-17 20:22:20.932249 | Idx:   40/400    | loss: 0.613 | grad_norm: 0.115 | acc: 86.21% |
| 2025-10-17 20:22:23.978957 | Idx:   80/400    | loss: 0.611 | grad_norm: 0.455 | acc: 85.99% |
| 2025-10-17 20:22:28.567308 | Idx:  120/400    | loss: 0.609 | grad_norm: 0.385 | acc: 86.22% |
| 2025-10-17 20:22:31.623191 | Idx:  160/400    | loss: 0.724 | grad_norm: 1.076 | acc: 75.56% |
| 2025-10-17 20:22:34.714293 | Idx:  200/400    | loss: 0.753 | grad_norm: 0.146 | acc: 80.75% |
| 2025-10-17 20:22:39.350080 | Idx:  240/400    | loss: 0.767 | grad_norm: 1.024 | acc: 81.60% |
| 2025-10-17 20:22:42.421627 | Idx:  280/400    | loss: 0.774 | grad_norm: 0.881 | acc: 82.40% |
| 2025-10-17 20:22:47.474928 | Idx:  320/400    | loss: 0.778 | grad_norm: 2.510 | acc: 83.65% |
| 2025-10-17 20:22:50.648640 | Idx:  360/400    | loss: 0.774 | grad_norm: 0.750 | acc: 82.52% |
| 2025-10-17 20:22:53.794162 | Idx:  400/400    | loss: 0.758 | grad_norm: 0.812 | acc: 87.06% |
Train Loss: 0.7581 Acc: 82.78%
Acc: 79.10%


| val_epoch_acc: 79.10% | epoch: 01 | avg_train_loss: 0.5916 | avg_train_acc: 86.9744 | 


Epoch 3/8
----------
| 2025-10-17 20:23:22.719606 | Idx:   40/400    | loss: 0.600 | grad_norm: 0.844 | acc: 89.64% |
| 2025-10-17 20:23:25.876289 | Idx:   80/400    | loss: 0.538 | grad_norm: 2.200 | acc: 87.79% |
| 2025-10-17 20:23:30.550090 | Idx:  120/400    | loss: 0.511 | grad_norm: 1.391 | acc: 91.93% |
| 2025-10-17 20:23:33.678465 | Idx:  160/400    | loss: 0.493 | grad_norm: 1.695 | acc: 93.27% |
| 2025-10-17 20:23:36.810012 | Idx:  200/400    | loss: 0.470 | grad_norm: 0.935 | acc: 94.23% |
| 2025-10-17 20:23:41.476615 | Idx:  240/400    | loss: 0.465 | grad_norm: 1.953 | acc: 92.11% |
| 2025-10-17 20:23:44.627643 | Idx:  280/400    | loss: 0.450 | grad_norm: 0.718 | acc: 94.73% |
| 2025-10-17 20:23:49.528257 | Idx:  320/400    | loss: 0.439 | grad_norm: 5.402 | acc: 87.33% |
| 2025-10-17 20:23:52.668242 | Idx:  360/400    | loss: 0.434 | grad_norm: 1.610 | acc: 91.28% |
| 2025-10-17 20:23:55.763970 | Idx:  400/400    | loss: 0.430 | grad_norm: 2.489 | acc: 95.41% |
Train Loss: 0.4299 Acc: 91.06%
Acc: 93.17%


| val_epoch_acc: 93.17% | epoch: 02 | avg_train_loss: 0.3999 | avg_train_acc: 91.5646 | 


Epoch 4/8
----------
| 2025-10-17 20:24:24.001749 | Idx:   40/400    | loss: 0.321 | grad_norm: 1.858 | acc: 95.37% |
| 2025-10-17 20:24:27.130292 | Idx:   80/400    | loss: 0.361 | grad_norm: 4.626 | acc: 94.06% |
| 2025-10-17 20:24:31.764886 | Idx:  120/400    | loss: 0.368 | grad_norm: 2.342 | acc: 95.33% |
| 2025-10-17 20:24:34.904225 | Idx:  160/400    | loss: 0.349 | grad_norm: 2.728 | acc: 96.01% |
| 2025-10-17 20:24:38.070522 | Idx:  200/400    | loss: 0.339 | grad_norm: 3.196 | acc: 91.09% |
| 2025-10-17 20:24:42.672665 | Idx:  240/400    | loss: 0.350 | grad_norm: 8.925 | acc: 91.80% |
| 2025-10-17 20:24:45.766985 | Idx:  280/400    | loss: 0.342 | grad_norm: 1.633 | acc: 95.43% |
| 2025-10-17 20:24:50.367715 | Idx:  320/400    | loss: 0.342 | grad_norm: 0.414 | acc: 95.29% |
| 2025-10-17 20:24:53.449687 | Idx:  360/400    | loss: 0.355 | grad_norm: 0.491 | acc: 95.75% |
| 2025-10-17 20:24:56.500554 | Idx:  400/400    | loss: 0.349 | grad_norm: 3.715 | acc: 91.72% |
Train Loss: 0.3491 Acc: 93.23%
Acc: 65.28%


| val_epoch_acc: 65.28% | epoch: 03 | avg_train_loss: 0.3077 | avg_train_acc: 94.1609 | 


Epoch 5/8
----------
| 2025-10-17 20:25:25.297607 | Idx:   40/400    | loss: 0.337 | grad_norm: 1.898 | acc: 94.01% |
| 2025-10-17 20:25:28.379433 | Idx:   80/400    | loss: 0.360 | grad_norm: 7.489 | acc: 94.92% |
| 2025-10-17 20:25:32.890952 | Idx:  120/400    | loss: 0.342 | grad_norm: 3.155 | acc: 92.77% |
| 2025-10-17 20:25:35.965382 | Idx:  160/400    | loss: 0.343 | grad_norm: 1.347 | acc: 95.21% |
| 2025-10-17 20:25:39.030825 | Idx:  200/400    | loss: 0.337 | grad_norm: 2.263 | acc: 96.61% |
| 2025-10-17 20:25:43.567546 | Idx:  240/400    | loss: 0.333 | grad_norm: 1.377 | acc: 93.95% |
| 2025-10-17 20:25:46.639107 | Idx:  280/400    | loss: 0.339 | grad_norm: 6.435 | acc: 96.41% |
| 2025-10-17 20:25:51.194692 | Idx:  320/400    | loss: 0.343 | grad_norm: 0.938 | acc: 95.23% |
| 2025-10-17 20:25:54.316626 | Idx:  360/400    | loss: 0.334 | grad_norm: 2.296 | acc: 93.16% |
| 2025-10-17 20:25:57.472861 | Idx:  400/400    | loss: 0.332 | grad_norm: 3.537 | acc: 92.55% |
Train Loss: 0.3321 Acc: 93.51%
Acc: 92.52%


| val_epoch_acc: 92.52% | epoch: 04 | avg_train_loss: 0.3359 | avg_train_acc: 93.6497 | 


Epoch 6/8
----------
| 2025-10-17 20:26:26.646041 | Idx:   40/400    | loss: 0.358 | grad_norm: 8.280 | acc: 87.70% |
| 2025-10-17 20:26:29.799702 | Idx:   80/400    | loss: 0.369 | grad_norm: 6.314 | acc: 89.96% |
| 2025-10-17 20:26:34.447475 | Idx:  120/400    | loss: 0.397 | grad_norm: 1.564 | acc: 95.11% |
| 2025-10-17 20:26:37.572309 | Idx:  160/400    | loss: 0.374 | grad_norm: 2.035 | acc: 94.22% |
| 2025-10-17 20:26:40.702287 | Idx:  200/400    | loss: 0.364 | grad_norm: 2.036 | acc: 90.49% |
| 2025-10-17 20:26:45.466715 | Idx:  240/400    | loss: 0.373 | grad_norm: 1.913 | acc: 94.58% |
| 2025-10-17 20:26:48.641049 | Idx:  280/400    | loss: 0.376 | grad_norm: 2.552 | acc: 93.32% |
| 2025-10-17 20:26:53.271948 | Idx:  320/400    | loss: 0.366 | grad_norm: 6.117 | acc: 90.75% |
| 2025-10-17 20:26:56.382486 | Idx:  360/400    | loss: 0.369 | grad_norm: 11.911 | acc: 83.02% |
| 2025-10-17 20:26:59.521640 | Idx:  400/400    | loss: 0.370 | grad_norm: 1.060 | acc: 95.38% |
Train Loss: 0.3704 Acc: 93.48%
Acc: 93.57%


| val_epoch_acc: 93.57% | epoch: 05 | avg_train_loss: 0.4102 | avg_train_acc: 93.1923 | 


Epoch 7/8
----------
| 2025-10-17 20:27:28.083945 | Idx:   40/400    | loss: 0.251 | grad_norm: 2.994 | acc: 95.88% |
| 2025-10-17 20:27:31.137487 | Idx:   80/400    | loss: 0.265 | grad_norm: 6.213 | acc: 96.59% |
| 2025-10-17 20:27:35.759440 | Idx:  120/400    | loss: 0.309 | grad_norm: 4.089 | acc: 92.31% |
| 2025-10-17 20:27:38.796473 | Idx:  160/400    | loss: 0.305 | grad_norm: 1.705 | acc: 96.50% |
| 2025-10-17 20:27:41.850305 | Idx:  200/400    | loss: 0.301 | grad_norm: 2.343 | acc: 94.60% |
| 2025-10-17 20:27:46.398125 | Idx:  240/400    | loss: 0.318 | grad_norm: 2.217 | acc: 92.38% |
| 2025-10-17 20:27:49.437013 | Idx:  280/400    | loss: 0.314 | grad_norm: 1.086 | acc: 96.34% |
| 2025-10-17 20:27:53.915695 | Idx:  320/400    | loss: 0.306 | grad_norm: 1.749 | acc: 95.16% |
| 2025-10-17 20:27:57.063420 | Idx:  360/400    | loss: 0.324 | grad_norm: 1.059 | acc: 96.29% |
| 2025-10-17 20:28:00.094683 | Idx:  400/400    | loss: 0.315 | grad_norm: 7.094 | acc: 96.64% |
Train Loss: 0.3152 Acc: 94.34%
Acc: 90.77%


| val_epoch_acc: 90.77% | epoch: 06 | avg_train_loss: 0.2471 | avg_train_acc: 95.2142 | 


Epoch 8/8
----------
| 2025-10-17 20:28:28.179057 | Idx:   40/400    | loss: 0.409 | grad_norm: 3.018 | acc: 94.45% |
| 2025-10-17 20:28:31.198946 | Idx:   80/400    | loss: 0.400 | grad_norm: 5.237 | acc: 95.87% |
| 2025-10-17 20:28:35.706189 | Idx:  120/400    | loss: 0.383 | grad_norm: 1.010 | acc: 95.57% |
| 2025-10-17 20:28:38.744248 | Idx:  160/400    | loss: 0.380 | grad_norm: 2.095 | acc: 94.47% |
| 2025-10-17 20:28:41.807315 | Idx:  200/400    | loss: 0.357 | grad_norm: 2.429 | acc: 95.05% |
| 2025-10-17 20:28:46.355997 | Idx:  240/400    | loss: 0.354 | grad_norm: 18.236 | acc: 94.12% |
| 2025-10-17 20:28:49.403865 | Idx:  280/400    | loss: 0.347 | grad_norm: 4.762 | acc: 94.66% |
| 2025-10-17 20:28:53.959279 | Idx:  320/400    | loss: 0.352 | grad_norm: 10.302 | acc: 91.69% |
| 2025-10-17 20:28:57.022795 | Idx:  360/400    | loss: 0.339 | grad_norm: 3.051 | acc: 97.11% |
| 2025-10-17 20:29:00.037371 | Idx:  400/400    | loss: 0.428 | grad_norm: 1.296 | acc: 89.38% |
Train Loss: 0.4282 Acc: 93.03%
Acc: 88.79%


| val_epoch_acc: 88.79% | epoch: 07 | avg_train_loss: 1.1241 | avg_train_acc: 86.0639 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_202102-koze7yj9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_202102-koze7yj9\logs[0m
rule: B345/S5 \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_20-29-25_tiny_2_layer_seq_cnn__200-200-B345_S5


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
©À©¤R2Conv: 1-1                                      --                        22
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        2
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        44
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        2
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        24
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:29:42.020115 | Idx:   40/400    | loss: 1.082 | grad_norm: 0.294 | acc: 84.23% |
| 2025-10-17 20:29:45.056946 | Idx:   80/400    | loss: 0.851 | grad_norm: 0.145 | acc: 87.96% |
| 2025-10-17 20:29:50.134703 | Idx:  120/400    | loss: 0.739 | grad_norm: 0.426 | acc: 89.43% |
| 2025-10-17 20:29:53.402639 | Idx:  160/400    | loss: 0.662 | grad_norm: 0.689 | acc: 90.96% |
| 2025-10-17 20:29:56.464765 | Idx:  200/400    | loss: 0.609 | grad_norm: 0.518 | acc: 91.29% |
| 2025-10-17 20:30:01.174010 | Idx:  240/400    | loss: 0.572 | grad_norm: 1.024 | acc: 91.98% |
| 2025-10-17 20:30:04.228333 | Idx:  280/400    | loss: 0.543 | grad_norm: 0.240 | acc: 93.99% |
| 2025-10-17 20:30:08.631217 | Idx:  320/400    | loss: 0.514 | grad_norm: 0.346 | acc: 94.23% |
| 2025-10-17 20:30:11.699767 | Idx:  360/400    | loss: 0.488 | grad_norm: 0.285 | acc: 94.75% |
| 2025-10-17 20:30:14.744101 | Idx:  400/400    | loss: 0.463 | grad_norm: 1.019 | acc: 94.11% |
Train Loss: 0.4630 Acc: 89.49%
Acc: 96.40%


| val_epoch_acc: 96.40% | epoch: 00 | avg_train_loss: 0.2396 | avg_train_acc: 95.1501 | 


Epoch 2/8
----------
| 2025-10-17 20:30:43.399700 | Idx:   40/400    | loss: 0.231 | grad_norm: 0.501 | acc: 95.31% |
| 2025-10-17 20:30:46.483213 | Idx:   80/400    | loss: 0.240 | grad_norm: 0.661 | acc: 94.74% |
| 2025-10-17 20:30:51.073071 | Idx:  120/400    | loss: 0.232 | grad_norm: 0.474 | acc: 96.16% |
| 2025-10-17 20:30:54.131426 | Idx:  160/400    | loss: 0.221 | grad_norm: 0.534 | acc: 97.03% |
| 2025-10-17 20:30:57.283293 | Idx:  200/400    | loss: 0.219 | grad_norm: 1.089 | acc: 96.26% |
| 2025-10-17 20:31:02.010421 | Idx:  240/400    | loss: 0.217 | grad_norm: 0.937 | acc: 96.22% |
| 2025-10-17 20:31:05.385110 | Idx:  280/400    | loss: 0.214 | grad_norm: 0.440 | acc: 97.40% |
| 2025-10-17 20:31:10.373683 | Idx:  320/400    | loss: 0.209 | grad_norm: 1.164 | acc: 95.29% |
| 2025-10-17 20:31:13.411366 | Idx:  360/400    | loss: 0.205 | grad_norm: 0.512 | acc: 97.74% |
| 2025-10-17 20:31:16.457058 | Idx:  400/400    | loss: 0.202 | grad_norm: 0.308 | acc: 98.21% |
Train Loss: 0.2023 Acc: 95.95%
Acc: 98.04%


| val_epoch_acc: 98.04% | epoch: 01 | avg_train_loss: 0.1761 | avg_train_acc: 96.5088 | 


Epoch 3/8
----------
| 2025-10-17 20:31:44.523178 | Idx:   40/400    | loss: 0.196 | grad_norm: 0.229 | acc: 97.92% |
| 2025-10-17 20:31:47.556399 | Idx:   80/400    | loss: 0.190 | grad_norm: 0.578 | acc: 97.36% |
| 2025-10-17 20:31:52.097449 | Idx:  120/400    | loss: 0.192 | grad_norm: 1.079 | acc: 97.09% |
| 2025-10-17 20:31:55.132135 | Idx:  160/400    | loss: 0.191 | grad_norm: 1.805 | acc: 93.85% |
| 2025-10-17 20:31:58.163984 | Idx:  200/400    | loss: 0.187 | grad_norm: 1.042 | acc: 97.07% |
| 2025-10-17 20:32:02.680146 | Idx:  240/400    | loss: 0.183 | grad_norm: 0.170 | acc: 97.64% |
| 2025-10-17 20:32:05.786883 | Idx:  280/400    | loss: 0.181 | grad_norm: 1.218 | acc: 96.29% |
| 2025-10-17 20:32:10.491936 | Idx:  320/400    | loss: 0.179 | grad_norm: 0.213 | acc: 98.25% |
| 2025-10-17 20:32:13.586049 | Idx:  360/400    | loss: 0.177 | grad_norm: 1.068 | acc: 96.83% |
| 2025-10-17 20:32:16.615770 | Idx:  400/400    | loss: 0.178 | grad_norm: 1.740 | acc: 93.52% |
Train Loss: 0.1784 Acc: 96.51%
Acc: 97.56%


| val_epoch_acc: 97.56% | epoch: 02 | avg_train_loss: 0.1898 | avg_train_acc: 96.2762 | 


Epoch 4/8
----------
| 2025-10-17 20:32:44.487807 | Idx:   40/400    | loss: 0.169 | grad_norm: 0.282 | acc: 98.39% |
| 2025-10-17 20:32:47.543998 | Idx:   80/400    | loss: 0.174 | grad_norm: 0.591 | acc: 97.75% |
| 2025-10-17 20:32:52.047167 | Idx:  120/400    | loss: 0.172 | grad_norm: 0.435 | acc: 97.42% |
| 2025-10-17 20:32:55.100283 | Idx:  160/400    | loss: 0.174 | grad_norm: 2.769 | acc: 92.66% |
| 2025-10-17 20:32:58.180002 | Idx:  200/400    | loss: 0.172 | grad_norm: 0.835 | acc: 97.52% |
| 2025-10-17 20:33:02.756940 | Idx:  240/400    | loss: 0.173 | grad_norm: 1.856 | acc: 94.49% |
| 2025-10-17 20:33:05.859578 | Idx:  280/400    | loss: 0.171 | grad_norm: 0.364 | acc: 97.93% |
| 2025-10-17 20:33:10.402607 | Idx:  320/400    | loss: 0.172 | grad_norm: 0.875 | acc: 98.28% |
| 2025-10-17 20:33:13.463857 | Idx:  360/400    | loss: 0.175 | grad_norm: 0.326 | acc: 97.86% |
| 2025-10-17 20:33:16.531613 | Idx:  400/400    | loss: 0.174 | grad_norm: 0.769 | acc: 97.95% |
Train Loss: 0.1736 Acc: 96.73%
Acc: 95.80%


| val_epoch_acc: 95.80% | epoch: 03 | avg_train_loss: 0.1740 | avg_train_acc: 96.8402 | 


Epoch 5/8
----------
| 2025-10-17 20:33:44.910008 | Idx:   40/400    | loss: 0.162 | grad_norm: 1.942 | acc: 92.84% |
| 2025-10-17 20:33:47.943843 | Idx:   80/400    | loss: 0.177 | grad_norm: 0.526 | acc: 97.40% |
| 2025-10-17 20:33:52.456810 | Idx:  120/400    | loss: 0.173 | grad_norm: 0.313 | acc: 97.81% |
| 2025-10-17 20:33:55.579575 | Idx:  160/400    | loss: 0.187 | grad_norm: 0.518 | acc: 97.61% |
| 2025-10-17 20:33:58.644035 | Idx:  200/400    | loss: 0.190 | grad_norm: 0.911 | acc: 96.07% |
| 2025-10-17 20:34:03.179932 | Idx:  240/400    | loss: 0.184 | grad_norm: 0.369 | acc: 97.57% |
| 2025-10-17 20:34:06.234213 | Idx:  280/400    | loss: 0.183 | grad_norm: 0.689 | acc: 97.49% |
| 2025-10-17 20:34:10.929879 | Idx:  320/400    | loss: 0.183 | grad_norm: 0.737 | acc: 96.31% |
| 2025-10-17 20:34:13.978224 | Idx:  360/400    | loss: 0.179 | grad_norm: 0.823 | acc: 97.24% |
| 2025-10-17 20:34:17.023293 | Idx:  400/400    | loss: 0.180 | grad_norm: 0.647 | acc: 97.53% |
Train Loss: 0.1803 Acc: 96.62%
Acc: 94.45%


| val_epoch_acc: 94.45% | epoch: 04 | avg_train_loss: 0.2090 | avg_train_acc: 96.1966 | 


Epoch 6/8
----------
| 2025-10-17 20:34:46.412869 | Idx:   40/400    | loss: 0.174 | grad_norm: 1.938 | acc: 92.96% |
| 2025-10-17 20:34:49.543708 | Idx:   80/400    | loss: 0.170 | grad_norm: 0.378 | acc: 96.72% |
| 2025-10-17 20:34:54.171639 | Idx:  120/400    | loss: 0.161 | grad_norm: 0.357 | acc: 97.73% |
| 2025-10-17 20:34:57.269247 | Idx:  160/400    | loss: 0.167 | grad_norm: 0.618 | acc: 97.11% |
| 2025-10-17 20:35:00.515838 | Idx:  200/400    | loss: 0.166 | grad_norm: 0.167 | acc: 98.34% |
| 2025-10-17 20:35:05.205522 | Idx:  240/400    | loss: 0.169 | grad_norm: 0.628 | acc: 97.05% |
| 2025-10-17 20:35:08.372407 | Idx:  280/400    | loss: 0.171 | grad_norm: 0.572 | acc: 97.24% |
| 2025-10-17 20:35:12.884113 | Idx:  320/400    | loss: 0.168 | grad_norm: 0.243 | acc: 98.49% |
| 2025-10-17 20:35:16.005465 | Idx:  360/400    | loss: 0.165 | grad_norm: 1.578 | acc: 95.64% |
| 2025-10-17 20:35:19.157732 | Idx:  400/400    | loss: 0.164 | grad_norm: 0.432 | acc: 98.82% |
Train Loss: 0.1643 Acc: 97.03%
Acc: 98.01%


| val_epoch_acc: 98.01% | epoch: 05 | avg_train_loss: 0.1659 | avg_train_acc: 97.1123 | 


Epoch 7/8
----------
| 2025-10-17 20:35:49.692676 | Idx:   40/400    | loss: 0.160 | grad_norm: 0.163 | acc: 98.00% |
| 2025-10-17 20:35:52.874209 | Idx:   80/400    | loss: 0.159 | grad_norm: 0.461 | acc: 97.77% |
| 2025-10-17 20:35:57.346975 | Idx:  120/400    | loss: 0.156 | grad_norm: 0.753 | acc: 95.91% |
| 2025-10-17 20:36:00.330174 | Idx:  160/400    | loss: 0.151 | grad_norm: 1.046 | acc: 98.42% |
| 2025-10-17 20:36:03.325003 | Idx:  200/400    | loss: 0.154 | grad_norm: 0.455 | acc: 98.73% |
| 2025-10-17 20:36:07.791404 | Idx:  240/400    | loss: 0.160 | grad_norm: 0.338 | acc: 98.01% |
| 2025-10-17 20:36:10.788607 | Idx:  280/400    | loss: 0.158 | grad_norm: 0.694 | acc: 98.11% |
| 2025-10-17 20:36:15.206990 | Idx:  320/400    | loss: 0.156 | grad_norm: 1.248 | acc: 96.58% |
| 2025-10-17 20:36:18.187012 | Idx:  360/400    | loss: 0.156 | grad_norm: 1.828 | acc: 94.59% |
| 2025-10-17 20:36:21.173149 | Idx:  400/400    | loss: 0.155 | grad_norm: 0.425 | acc: 98.43% |
Train Loss: 0.1549 Acc: 97.25%
Acc: 96.29%


| val_epoch_acc: 96.29% | epoch: 06 | avg_train_loss: 0.1552 | avg_train_acc: 97.1580 | 


Epoch 8/8
----------
| 2025-10-17 20:36:49.200894 | Idx:   40/400    | loss: 0.174 | grad_norm: 0.447 | acc: 97.11% |
| 2025-10-17 20:36:52.172379 | Idx:   80/400    | loss: 0.183 | grad_norm: 0.692 | acc: 98.02% |
| 2025-10-17 20:36:56.621553 | Idx:  120/400    | loss: 0.180 | grad_norm: 0.328 | acc: 98.08% |
| 2025-10-17 20:36:59.582815 | Idx:  160/400    | loss: 0.174 | grad_norm: 0.743 | acc: 96.17% |
| 2025-10-17 20:37:02.552927 | Idx:  200/400    | loss: 0.172 | grad_norm: 1.640 | acc: 95.27% |
| 2025-10-17 20:37:07.124422 | Idx:  240/400    | loss: 0.173 | grad_norm: 2.355 | acc: 91.18% |
| 2025-10-17 20:37:10.423609 | Idx:  280/400    | loss: 0.174 | grad_norm: 0.518 | acc: 98.46% |
| 2025-10-17 20:37:15.139943 | Idx:  320/400    | loss: 0.172 | grad_norm: 1.236 | acc: 97.16% |
| 2025-10-17 20:37:18.325202 | Idx:  360/400    | loss: 0.170 | grad_norm: 0.936 | acc: 95.80% |
| 2025-10-17 20:37:21.416032 | Idx:  400/400    | loss: 0.170 | grad_norm: 0.765 | acc: 97.66% |
Train Loss: 0.1704 Acc: 96.92%
Acc: 98.34%


| val_epoch_acc: 98.34% | epoch: 07 | avg_train_loss: 0.1637 | avg_train_acc: 97.0446 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_202925-je1yxuj2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_202925-je1yxuj2\logs[0m
rule: B13/S012V \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_20-37-46_tiny_2_layer_seq_cnn__200-200-B13_S012V


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
©À©¤R2Conv: 1-1                                      --                        22
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        2
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        44
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        2
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        24
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:38:06.244923 | Idx:   40/400    | loss: 1.551 | grad_norm: 0.311 | acc: 89.96% |
| 2025-10-17 20:38:09.375081 | Idx:   80/400    | loss: 0.952 | grad_norm: 0.097 | acc: 92.14% |
| 2025-10-17 20:38:14.020424 | Idx:  120/400    | loss: 0.745 | grad_norm: 0.083 | acc: 91.84% |
| 2025-10-17 20:38:17.045140 | Idx:  160/400    | loss: 0.640 | grad_norm: 0.277 | acc: 87.91% |
| 2025-10-17 20:38:20.117116 | Idx:  200/400    | loss: 0.572 | grad_norm: 0.082 | acc: 95.50% |
| 2025-10-17 20:38:24.853847 | Idx:  240/400    | loss: 0.509 | grad_norm: 0.279 | acc: 98.05% |
| 2025-10-17 20:38:27.893159 | Idx:  280/400    | loss: 0.458 | grad_norm: 0.032 | acc: 99.29% |
| 2025-10-17 20:38:32.453994 | Idx:  320/400    | loss: 0.418 | grad_norm: 0.042 | acc: 99.63% |
| 2025-10-17 20:38:35.455387 | Idx:  360/400    | loss: 0.386 | grad_norm: 0.052 | acc: 99.42% |
| 2025-10-17 20:38:38.450125 | Idx:  400/400    | loss: 0.357 | grad_norm: 0.081 | acc: 98.69% |
Train Loss: 0.3568 Acc: 93.06%
Acc: 99.13%


| val_epoch_acc: 99.13% | epoch: 00 | avg_train_loss: 0.0988 | avg_train_acc: 99.2459 | 


Epoch 2/8
----------
| 2025-10-17 20:39:05.826134 | Idx:   40/400    | loss: 0.085 | grad_norm: 0.046 | acc: 99.39% |
| 2025-10-17 20:39:08.802851 | Idx:   80/400    | loss: 0.086 | grad_norm: 0.144 | acc: 98.91% |
| 2025-10-17 20:39:13.155789 | Idx:  120/400    | loss: 0.093 | grad_norm: 0.467 | acc: 98.77% |
| 2025-10-17 20:39:16.120877 | Idx:  160/400    | loss: 0.095 | grad_norm: 0.156 | acc: 99.32% |
| 2025-10-17 20:39:19.100205 | Idx:  200/400    | loss: 0.093 | grad_norm: 0.088 | acc: 99.29% |
| 2025-10-17 20:39:23.539935 | Idx:  240/400    | loss: 0.092 | grad_norm: 0.110 | acc: 99.61% |
| 2025-10-17 20:39:26.788203 | Idx:  280/400    | loss: 0.092 | grad_norm: 0.233 | acc: 98.56% |
| 2025-10-17 20:39:31.421180 | Idx:  320/400    | loss: 0.091 | grad_norm: 0.100 | acc: 99.67% |
| 2025-10-17 20:39:34.434106 | Idx:  360/400    | loss: 0.089 | grad_norm: 0.309 | acc: 98.54% |
| 2025-10-17 20:39:37.394181 | Idx:  400/400    | loss: 0.089 | grad_norm: 0.231 | acc: 98.83% |
Train Loss: 0.0888 Acc: 99.16%
Acc: 99.25%


| val_epoch_acc: 99.25% | epoch: 01 | avg_train_loss: 0.0855 | avg_train_acc: 99.1648 | 


Epoch 3/8
----------
| 2025-10-17 20:40:06.466901 | Idx:   40/400    | loss: 0.079 | grad_norm: 0.144 | acc: 99.31% |
| 2025-10-17 20:40:09.641634 | Idx:   80/400    | loss: 0.076 | grad_norm: 0.248 | acc: 98.90% |
| 2025-10-17 20:40:14.355527 | Idx:  120/400    | loss: 0.078 | grad_norm: 0.227 | acc: 97.89% |
| 2025-10-17 20:40:17.479535 | Idx:  160/400    | loss: 0.079 | grad_norm: 0.079 | acc: 99.25% |
| 2025-10-17 20:40:20.780836 | Idx:  200/400    | loss: 0.076 | grad_norm: 0.039 | acc: 99.56% |
| 2025-10-17 20:40:26.092179 | Idx:  240/400    | loss: 0.074 | grad_norm: 0.093 | acc: 98.69% |
| 2025-10-17 20:40:29.250981 | Idx:  280/400    | loss: 0.073 | grad_norm: 0.138 | acc: 99.10% |
| 2025-10-17 20:40:33.879255 | Idx:  320/400    | loss: 0.072 | grad_norm: 0.313 | acc: 98.74% |
| 2025-10-17 20:40:36.844784 | Idx:  360/400    | loss: 0.072 | grad_norm: 0.052 | acc: 99.41% |
| 2025-10-17 20:40:39.812258 | Idx:  400/400    | loss: 0.071 | grad_norm: 0.089 | acc: 99.51% |
Train Loss: 0.0709 Acc: 99.14%
Acc: 99.16%


| val_epoch_acc: 99.16% | epoch: 02 | avg_train_loss: 0.0622 | avg_train_acc: 99.1655 | 


Epoch 4/8
----------
| 2025-10-17 20:41:06.745935 | Idx:   40/400    | loss: 0.072 | grad_norm: 0.463 | acc: 99.28% |
| 2025-10-17 20:41:09.731897 | Idx:   80/400    | loss: 0.069 | grad_norm: 0.062 | acc: 99.43% |
| 2025-10-17 20:41:14.331619 | Idx:  120/400    | loss: 0.068 | grad_norm: 0.215 | acc: 98.44% |
| 2025-10-17 20:41:17.983983 | Idx:  160/400    | loss: 0.066 | grad_norm: 0.202 | acc: 98.90% |
| 2025-10-17 20:41:21.181462 | Idx:  200/400    | loss: 0.065 | grad_norm: 0.142 | acc: 98.49% |
| 2025-10-17 20:41:25.844163 | Idx:  240/400    | loss: 0.065 | grad_norm: 0.153 | acc: 98.68% |
| 2025-10-17 20:41:29.103007 | Idx:  280/400    | loss: 0.065 | grad_norm: 0.113 | acc: 99.56% |
| 2025-10-17 20:41:33.714529 | Idx:  320/400    | loss: 0.066 | grad_norm: 0.082 | acc: 99.35% |
| 2025-10-17 20:41:36.792544 | Idx:  360/400    | loss: 0.067 | grad_norm: 0.068 | acc: 99.32% |
| 2025-10-17 20:41:39.801701 | Idx:  400/400    | loss: 0.066 | grad_norm: 0.115 | acc: 99.67% |
Train Loss: 0.0662 Acc: 99.07%
Acc: 99.15%


| val_epoch_acc: 99.15% | epoch: 03 | avg_train_loss: 0.0624 | avg_train_acc: 99.1107 | 


Epoch 5/8
----------
| 2025-10-17 20:42:08.934092 | Idx:   40/400    | loss: 0.064 | grad_norm: 0.042 | acc: 99.26% |
| 2025-10-17 20:42:12.045553 | Idx:   80/400    | loss: 0.064 | grad_norm: 0.068 | acc: 99.42% |
| 2025-10-17 20:42:17.046129 | Idx:  120/400    | loss: 0.064 | grad_norm: 0.137 | acc: 99.04% |
| 2025-10-17 20:42:20.093469 | Idx:  160/400    | loss: 0.066 | grad_norm: 0.156 | acc: 99.23% |
| 2025-10-17 20:42:23.135872 | Idx:  200/400    | loss: 0.064 | grad_norm: 0.092 | acc: 99.25% |
| 2025-10-17 20:42:27.699152 | Idx:  240/400    | loss: 0.064 | grad_norm: 0.122 | acc: 99.30% |
| 2025-10-17 20:42:30.732653 | Idx:  280/400    | loss: 0.065 | grad_norm: 0.094 | acc: 99.48% |
| 2025-10-17 20:42:35.238206 | Idx:  320/400    | loss: 0.066 | grad_norm: 0.369 | acc: 97.95% |
| 2025-10-17 20:42:38.252091 | Idx:  360/400    | loss: 0.066 | grad_norm: 0.295 | acc: 98.81% |
| 2025-10-17 20:42:41.422037 | Idx:  400/400    | loss: 0.065 | grad_norm: 0.076 | acc: 99.15% |
Train Loss: 0.0653 Acc: 99.06%
Acc: 98.99%


| val_epoch_acc: 98.99% | epoch: 04 | avg_train_loss: 0.0620 | avg_train_acc: 99.1069 | 


Epoch 6/8
----------
| 2025-10-17 20:43:08.878309 | Idx:   40/400    | loss: 0.058 | grad_norm: 0.058 | acc: 99.28% |
| 2025-10-17 20:43:11.868823 | Idx:   80/400    | loss: 0.058 | grad_norm: 0.100 | acc: 99.20% |
| 2025-10-17 20:43:16.315631 | Idx:  120/400    | loss: 0.059 | grad_norm: 0.085 | acc: 99.59% |
| 2025-10-17 20:43:19.377791 | Idx:  160/400    | loss: 0.061 | grad_norm: 0.075 | acc: 99.44% |
| 2025-10-17 20:43:22.441329 | Idx:  200/400    | loss: 0.061 | grad_norm: 0.241 | acc: 99.11% |
| 2025-10-17 20:43:26.912263 | Idx:  240/400    | loss: 0.060 | grad_norm: 0.027 | acc: 99.18% |
| 2025-10-17 20:43:29.956465 | Idx:  280/400    | loss: 0.062 | grad_norm: 0.407 | acc: 98.96% |
| 2025-10-17 20:43:34.639114 | Idx:  320/400    | loss: 0.062 | grad_norm: 0.054 | acc: 99.35% |
| 2025-10-17 20:43:37.615325 | Idx:  360/400    | loss: 0.062 | grad_norm: 0.106 | acc: 98.34% |
| 2025-10-17 20:43:40.605224 | Idx:  400/400    | loss: 0.062 | grad_norm: 0.094 | acc: 98.95% |
Train Loss: 0.0618 Acc: 99.10%
Acc: 99.10%


| val_epoch_acc: 99.10% | epoch: 05 | avg_train_loss: 0.0591 | avg_train_acc: 99.1505 | 


Epoch 7/8
----------
| 2025-10-17 20:44:07.042552 | Idx:   40/400    | loss: 0.067 | grad_norm: 0.068 | acc: 99.18% |
| 2025-10-17 20:44:10.046597 | Idx:   80/400    | loss: 0.070 | grad_norm: 0.293 | acc: 97.87% |
| 2025-10-17 20:44:14.478856 | Idx:  120/400    | loss: 0.069 | grad_norm: 0.116 | acc: 99.60% |
| 2025-10-17 20:44:17.473107 | Idx:  160/400    | loss: 0.068 | grad_norm: 0.119 | acc: 99.17% |
| 2025-10-17 20:44:20.464119 | Idx:  200/400    | loss: 0.067 | grad_norm: 0.092 | acc: 99.28% |
| 2025-10-17 20:44:24.909986 | Idx:  240/400    | loss: 0.066 | grad_norm: 0.117 | acc: 99.61% |
| 2025-10-17 20:44:28.033582 | Idx:  280/400    | loss: 0.065 | grad_norm: 0.123 | acc: 99.16% |
| 2025-10-17 20:44:32.743964 | Idx:  320/400    | loss: 0.064 | grad_norm: 0.041 | acc: 99.63% |
| 2025-10-17 20:44:35.789623 | Idx:  360/400    | loss: 0.064 | grad_norm: 0.465 | acc: 97.35% |
| 2025-10-17 20:44:38.948084 | Idx:  400/400    | loss: 0.064 | grad_norm: 0.098 | acc: 99.55% |
Train Loss: 0.0640 Acc: 99.06%
Acc: 98.91%


| val_epoch_acc: 98.91% | epoch: 06 | avg_train_loss: 0.0629 | avg_train_acc: 99.0620 | 


Epoch 8/8
----------
| 2025-10-17 20:45:06.536775 | Idx:   40/400    | loss: 0.056 | grad_norm: 0.102 | acc: 99.49% |
| 2025-10-17 20:45:09.628225 | Idx:   80/400    | loss: 0.064 | grad_norm: 0.273 | acc: 98.98% |
| 2025-10-17 20:45:14.157838 | Idx:  120/400    | loss: 0.067 | grad_norm: 0.117 | acc: 99.22% |
| 2025-10-17 20:45:17.223742 | Idx:  160/400    | loss: 0.070 | grad_norm: 0.141 | acc: 98.79% |
| 2025-10-17 20:45:20.294350 | Idx:  200/400    | loss: 0.070 | grad_norm: 0.199 | acc: 99.41% |
| 2025-10-17 20:45:24.900311 | Idx:  240/400    | loss: 0.068 | grad_norm: 0.303 | acc: 98.66% |
| 2025-10-17 20:45:27.889616 | Idx:  280/400    | loss: 0.068 | grad_norm: 0.077 | acc: 98.81% |
| 2025-10-17 20:45:32.213970 | Idx:  320/400    | loss: 0.067 | grad_norm: 0.170 | acc: 99.35% |
| 2025-10-17 20:45:35.171343 | Idx:  360/400    | loss: 0.067 | grad_norm: 0.068 | acc: 98.77% |
| 2025-10-17 20:45:38.113104 | Idx:  400/400    | loss: 0.067 | grad_norm: 0.190 | acc: 97.98% |
Train Loss: 0.0666 Acc: 99.01%
Acc: 99.04%


| val_epoch_acc: 99.04% | epoch: 07 | avg_train_loss: 0.0621 | avg_train_acc: 99.0779 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_203746-8exfgtsy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_203746-8exfgtsy\logs[0m
rule: B2/S013V \t, network: tiny_2_layer_seq_p4cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleP4CNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_p4cnn.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_20-46-01_tiny_2_layer_seq_cnn__200-200-B2_S013V


====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
SimpleP4CNNTiny                                    --                        --
©À©¤R2Conv: 1-1                                      --                        22
©¦    ©¸©¤BlocksBasisExpansion: 2-1                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-1         --                        --
©À©¤InnerBatchNorm: 1-2                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-2                            --                        2
©À©¤ReLU: 1-3                                        --                        --
©À©¤R2Conv: 1-4                                      --                        44
©¦    ©¸©¤BlocksBasisExpansion: 2-3                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-2         --                        --
©À©¤InnerBatchNorm: 1-5                              --                        --
©¦    ©¸©¤BatchNorm3d: 2-4                            --                        2
©À©¤ReLU: 1-6                                        --                        --
©À©¤R2Conv: 1-7                                      --                        24
©¦    ©¸©¤BlocksBasisExpansion: 2-5                   --                        --
©¦    ©¦    ©¸©¤SingleBlockBasisExpansion: 3-3         --                        --
====================================================================================================
Total params: 94
Trainable params: 94
Non-trainable params: 0
Total mult-adds (M): 0
====================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
====================================================================================================
Epoch 1/8
----------
| 2025-10-17 20:46:18.234390 | Idx:   40/400    | loss: 1.787 | grad_norm: 0.101 | acc: 73.27% |
| 2025-10-17 20:46:21.209566 | Idx:   80/400    | loss: 1.412 | grad_norm: 0.237 | acc: 73.97% |
| 2025-10-17 20:46:25.628559 | Idx:  120/400    | loss: 1.215 | grad_norm: 0.845 | acc: 69.47% |
| 2025-10-17 20:46:28.596832 | Idx:  160/400    | loss: 1.106 | grad_norm: 0.185 | acc: 75.81% |
| 2025-10-17 20:46:31.628022 | Idx:  200/400    | loss: 1.036 | grad_norm: 0.109 | acc: 76.99% |
| 2025-10-17 20:46:36.178160 | Idx:  240/400    | loss: 0.992 | grad_norm: 0.159 | acc: 78.54% |
| 2025-10-17 20:46:39.125386 | Idx:  280/400    | loss: 0.956 | grad_norm: 0.494 | acc: 74.81% |
| 2025-10-17 20:46:43.464101 | Idx:  320/400    | loss: 0.930 | grad_norm: 0.255 | acc: 77.49% |
| 2025-10-17 20:46:46.397281 | Idx:  360/400    | loss: 0.909 | grad_norm: 0.244 | acc: 77.23% |
| 2025-10-17 20:46:49.337788 | Idx:  400/400    | loss: 0.891 | grad_norm: 0.667 | acc: 78.31% |
Train Loss: 0.8911 Acc: 73.80%
Acc: 75.65%


| val_epoch_acc: 75.65% | epoch: 00 | avg_train_loss: 0.7280 | avg_train_acc: 77.1070 | 


Epoch 2/8
----------
| 2025-10-17 20:47:14.917055 | Idx:   40/400    | loss: 0.738 | grad_norm: 0.609 | acc: 80.19% |
| 2025-10-17 20:47:17.862201 | Idx:   80/400    | loss: 0.740 | grad_norm: 0.497 | acc: 79.78% |
| 2025-10-17 20:47:22.364044 | Idx:  120/400    | loss: 0.724 | grad_norm: 0.779 | acc: 77.63% |
| 2025-10-17 20:47:25.325604 | Idx:  160/400    | loss: 0.720 | grad_norm: 0.338 | acc: 78.54% |
| 2025-10-17 20:47:28.271127 | Idx:  200/400    | loss: 0.721 | grad_norm: 1.251 | acc: 68.30% |
| 2025-10-17 20:47:32.717095 | Idx:  240/400    | loss: 0.715 | grad_norm: 0.421 | acc: 78.85% |
| 2025-10-17 20:47:35.728057 | Idx:  280/400    | loss: 0.715 | grad_norm: 0.505 | acc: 80.00% |
| 2025-10-17 20:47:40.318597 | Idx:  320/400    | loss: 0.712 | grad_norm: 0.317 | acc: 76.23% |
| 2025-10-17 20:47:43.306134 | Idx:  360/400    | loss: 0.707 | grad_norm: 0.409 | acc: 76.15% |
| 2025-10-17 20:47:46.382946 | Idx:  400/400    | loss: 0.703 | grad_norm: 0.389 | acc: 80.76% |
Train Loss: 0.7027 Acc: 77.37%
Acc: 80.20%


| val_epoch_acc: 80.20% | epoch: 01 | avg_train_loss: 0.6631 | avg_train_acc: 78.3471 | 


Epoch 3/8
----------
| 2025-10-17 20:48:12.577298 | Idx:   40/400    | loss: 0.683 | grad_norm: 0.208 | acc: 75.84% |
| 2025-10-17 20:48:15.616556 | Idx:   80/400    | loss: 0.678 | grad_norm: 0.806 | acc: 78.34% |
| 2025-10-17 20:48:20.213806 | Idx:  120/400    | loss: 0.679 | grad_norm: 0.394 | acc: 80.99% |
| 2025-10-17 20:48:23.275398 | Idx:  160/400    | loss: 0.673 | grad_norm: 0.221 | acc: 79.37% |
| 2025-10-17 20:48:26.278441 | Idx:  200/400    | loss: 0.669 | grad_norm: 0.791 | acc: 77.09% |
| 2025-10-17 20:48:30.690658 | Idx:  240/400    | loss: 0.674 | grad_norm: 0.271 | acc: 77.09% |
| 2025-10-17 20:48:33.729939 | Idx:  280/400    | loss: 0.674 | grad_norm: 0.854 | acc: 76.73% |
| 2025-10-17 20:48:38.696733 | Idx:  320/400    | loss: 0.670 | grad_norm: 0.659 | acc: 78.76% |
| 2025-10-17 20:48:41.766003 | Idx:  360/400    | loss: 0.667 | grad_norm: 0.474 | acc: 81.40% |
| 2025-10-17 20:48:44.849441 | Idx:  400/400    | loss: 0.665 | grad_norm: 0.659 | acc: 75.94% |
Train Loss: 0.6645 Acc: 78.35%
Acc: 81.47%


| val_epoch_acc: 81.47% | epoch: 02 | avg_train_loss: 0.6509 | avg_train_acc: 79.1403 | 


Epoch 4/8
----------
| 2025-10-17 20:49:12.134498 | Idx:   40/400    | loss: 0.651 | grad_norm: 0.816 | acc: 83.22% |
| 2025-10-17 20:49:15.123089 | Idx:   80/400    | loss: 0.651 | grad_norm: 0.572 | acc: 79.36% |
| 2025-10-17 20:49:19.651752 | Idx:  120/400    | loss: 0.656 | grad_norm: 0.151 | acc: 79.60% |
| 2025-10-17 20:49:22.631943 | Idx:  160/400    | loss: 0.646 | grad_norm: 0.571 | acc: 82.30% |
| 2025-10-17 20:49:25.618570 | Idx:  200/400    | loss: 0.649 | grad_norm: 0.251 | acc: 80.37% |
| 2025-10-17 20:49:30.152825 | Idx:  240/400    | loss: 0.649 | grad_norm: 0.398 | acc: 79.45% |
| 2025-10-17 20:49:33.127141 | Idx:  280/400    | loss: 0.645 | grad_norm: 1.399 | acc: 76.11% |
| 2025-10-17 20:49:37.651776 | Idx:  320/400    | loss: 0.646 | grad_norm: 0.342 | acc: 78.87% |
| 2025-10-17 20:49:40.622764 | Idx:  360/400    | loss: 0.644 | grad_norm: 0.461 | acc: 78.08% |
| 2025-10-17 20:49:43.612202 | Idx:  400/400    | loss: 0.644 | grad_norm: 0.526 | acc: 80.15% |
Train Loss: 0.6440 Acc: 79.16%
Acc: 75.86%


| val_epoch_acc: 75.86% | epoch: 03 | avg_train_loss: 0.6440 | avg_train_acc: 79.3055 | 


Epoch 5/8
----------
| 2025-10-17 20:50:09.886204 | Idx:   40/400    | loss: 0.656 | grad_norm: 0.446 | acc: 80.88% |
| 2025-10-17 20:50:12.840542 | Idx:   80/400    | loss: 0.648 | grad_norm: 0.846 | acc: 81.99% |
| 2025-10-17 20:50:17.500552 | Idx:  120/400    | loss: 0.643 | grad_norm: 0.488 | acc: 77.37% |
| 2025-10-17 20:50:20.508034 | Idx:  160/400    | loss: 0.639 | grad_norm: 0.559 | acc: 75.86% |
| 2025-10-17 20:50:23.494019 | Idx:  200/400    | loss: 0.647 | grad_norm: 1.097 | acc: 75.75% |
| 2025-10-17 20:50:27.886661 | Idx:  240/400    | loss: 0.646 | grad_norm: 0.694 | acc: 76.07% |
| 2025-10-17 20:50:30.867465 | Idx:  280/400    | loss: 0.648 | grad_norm: 0.327 | acc: 78.95% |
| 2025-10-17 20:50:35.404523 | Idx:  320/400    | loss: 0.648 | grad_norm: 0.490 | acc: 81.94% |
| 2025-10-17 20:50:38.455349 | Idx:  360/400    | loss: 0.646 | grad_norm: 1.074 | acc: 83.67% |
| 2025-10-17 20:50:41.448046 | Idx:  400/400    | loss: 0.646 | grad_norm: 0.419 | acc: 77.36% |
Train Loss: 0.6456 Acc: 79.47%
Acc: 83.85%


| val_epoch_acc: 83.85% | epoch: 04 | avg_train_loss: 0.6410 | avg_train_acc: 79.3151 | 


Epoch 6/8
----------
| 2025-10-17 20:51:08.075430 | Idx:   40/400    | loss: 0.645 | grad_norm: 0.522 | acc: 75.16% |
| 2025-10-17 20:51:11.097622 | Idx:   80/400    | loss: 0.647 | grad_norm: 0.341 | acc: 80.44% |
| 2025-10-17 20:51:15.552122 | Idx:  120/400    | loss: 0.651 | grad_norm: 0.351 | acc: 81.02% |
| 2025-10-17 20:51:18.486518 | Idx:  160/400    | loss: 0.654 | grad_norm: 0.482 | acc: 78.84% |
| 2025-10-17 20:51:21.432769 | Idx:  200/400    | loss: 0.645 | grad_norm: 0.347 | acc: 78.15% |
| 2025-10-17 20:51:25.894252 | Idx:  240/400    | loss: 0.644 | grad_norm: 0.116 | acc: 81.47% |
| 2025-10-17 20:51:28.914385 | Idx:  280/400    | loss: 0.642 | grad_norm: 0.794 | acc: 78.99% |
| 2025-10-17 20:51:33.818711 | Idx:  320/400    | loss: 0.642 | grad_norm: 1.019 | acc: 76.91% |
| 2025-10-17 20:51:36.846625 | Idx:  360/400    | loss: 0.642 | grad_norm: 0.790 | acc: 76.31% |
| 2025-10-17 20:51:39.828018 | Idx:  400/400    | loss: 0.641 | grad_norm: 1.230 | acc: 85.08% |
Train Loss: 0.6415 Acc: 79.59%
Acc: 82.29%


| val_epoch_acc: 82.29% | epoch: 05 | avg_train_loss: 0.6501 | avg_train_acc: 79.6517 | 


Epoch 7/8
----------
| 2025-10-17 20:52:05.594941 | Idx:   40/400    | loss: 0.644 | grad_norm: 1.020 | acc: 79.19% |
| 2025-10-17 20:52:08.881003 | Idx:   80/400    | loss: 0.639 | grad_norm: 0.625 | acc: 80.11% |
| 2025-10-17 20:52:13.533751 | Idx:  120/400    | loss: 0.638 | grad_norm: 0.903 | acc: 84.47% |
| 2025-10-17 20:52:16.552200 | Idx:  160/400    | loss: 0.633 | grad_norm: 0.824 | acc: 79.75% |
| 2025-10-17 20:52:19.554074 | Idx:  200/400    | loss: 0.634 | grad_norm: 0.517 | acc: 77.72% |
| 2025-10-17 20:52:24.074551 | Idx:  240/400    | loss: 0.634 | grad_norm: 0.644 | acc: 81.25% |
| 2025-10-17 20:52:27.093831 | Idx:  280/400    | loss: 0.636 | grad_norm: 2.270 | acc: 77.91% |
| 2025-10-17 20:52:31.688369 | Idx:  320/400    | loss: 0.637 | grad_norm: 0.205 | acc: 80.94% |
| 2025-10-17 20:52:34.736879 | Idx:  360/400    | loss: 0.638 | grad_norm: 1.000 | acc: 83.33% |
| 2025-10-17 20:52:37.755278 | Idx:  400/400    | loss: 0.637 | grad_norm: 1.894 | acc: 83.80% |
Train Loss: 0.6366 Acc: 79.88%
Acc: 83.42%


| val_epoch_acc: 83.42% | epoch: 06 | avg_train_loss: 0.6345 | avg_train_acc: 79.9926 | 


Epoch 8/8
----------
| 2025-10-17 20:53:04.495635 | Idx:   40/400    | loss: 0.650 | grad_norm: 2.143 | acc: 84.85% |
| 2025-10-17 20:53:07.528791 | Idx:   80/400    | loss: 0.636 | grad_norm: 1.513 | acc: 76.57% |
| 2025-10-17 20:53:12.009478 | Idx:  120/400    | loss: 0.631 | grad_norm: 0.758 | acc: 80.60% |
| 2025-10-17 20:53:15.038746 | Idx:  160/400    | loss: 0.630 | grad_norm: 0.681 | acc: 79.34% |
| 2025-10-17 20:53:18.068138 | Idx:  200/400    | loss: 0.626 | grad_norm: 0.445 | acc: 79.89% |
| 2025-10-17 20:53:22.612675 | Idx:  240/400    | loss: 0.623 | grad_norm: 0.363 | acc: 79.51% |
| 2025-10-17 20:53:25.642699 | Idx:  280/400    | loss: 0.623 | grad_norm: 0.210 | acc: 79.77% |
| 2025-10-17 20:53:30.252422 | Idx:  320/400    | loss: 0.624 | grad_norm: 0.791 | acc: 85.81% |
| 2025-10-17 20:53:33.541162 | Idx:  360/400    | loss: 0.622 | grad_norm: 0.440 | acc: 81.56% |
| 2025-10-17 20:53:37.070026 | Idx:  400/400    | loss: 0.623 | grad_norm: 0.909 | acc: 76.14% |
Train Loss: 0.6229 Acc: 80.27%
Acc: 84.71%


| val_epoch_acc: 84.71% | epoch: 07 | avg_train_loss: 0.6169 | avg_train_acc: 80.4482 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_204601-fqwm54db[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_204601-fqwm54db\logs[0m
rule: B2/S \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_20-54-00_tiny_2_layer_seq_cnn__200-200-B2_S


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 8, 200, 200]          408
©À©¤BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-3                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 8, 200, 200]          1,608
©À©¤BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-6                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 20:54:16.299880 | Idx:   40/400    | loss: 0.791 | grad_norm: 1.004 | acc: 94.77% |
| 2025-10-17 20:54:18.748630 | Idx:   80/400    | loss: 0.412 | grad_norm: 0.050 | acc: 99.98% |
| 2025-10-17 20:54:22.607717 | Idx:  120/400    | loss: 0.292 | grad_norm: 0.275 | acc: 99.53% |
| 2025-10-17 20:54:25.063666 | Idx:  160/400    | loss: 0.222 | grad_norm: 0.017 | acc: 100.00% |
| 2025-10-17 20:54:27.515080 | Idx:  200/400    | loss: 0.179 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 20:54:31.640292 | Idx:  240/400    | loss: 0.150 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:54:34.072956 | Idx:  280/400    | loss: 0.129 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 20:54:37.994829 | Idx:  320/400    | loss: 0.113 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 20:54:40.427912 | Idx:  360/400    | loss: 0.101 | grad_norm: 0.975 | acc: 98.03% |
| 2025-10-17 20:54:42.863990 | Idx:  400/400    | loss: 0.096 | grad_norm: 0.151 | acc: 99.75% |
Train Loss: 0.0963 Acc: 97.54%
Acc: 98.62%


| val_epoch_acc: 98.62% | epoch: 00 | avg_train_loss: 0.0632 | avg_train_acc: 99.2879 | 


Epoch 2/8
----------
| 2025-10-17 20:55:11.843012 | Idx:   40/400    | loss: 0.088 | grad_norm: 0.156 | acc: 99.85% |
| 2025-10-17 20:55:14.325481 | Idx:   80/400    | loss: 0.046 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:55:18.364717 | Idx:  120/400    | loss: 0.032 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:55:20.850850 | Idx:  160/400    | loss: 0.025 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:55:23.314966 | Idx:  200/400    | loss: 0.020 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:55:27.380244 | Idx:  240/400    | loss: 0.018 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:55:30.115281 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:55:34.769668 | Idx:  320/400    | loss: 0.014 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 20:55:37.248744 | Idx:  360/400    | loss: 0.013 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:55:39.712147 | Idx:  400/400    | loss: 0.013 | grad_norm: 0.306 | acc: 99.96% |
Train Loss: 0.0132 Acc: 99.84%
Acc: 99.96%


| val_epoch_acc: 99.96% | epoch: 01 | avg_train_loss: 0.0203 | avg_train_acc: 99.4987 | 


Epoch 3/8
----------
| 2025-10-17 20:56:08.999813 | Idx:   40/400    | loss: 0.059 | grad_norm: 0.011 | acc: 99.99% |
| 2025-10-17 20:56:11.461540 | Idx:   80/400    | loss: 0.033 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:15.476419 | Idx:  120/400    | loss: 0.023 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:17.947822 | Idx:  160/400    | loss: 0.018 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:20.430532 | Idx:  200/400    | loss: 0.015 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:24.400767 | Idx:  240/400    | loss: 0.013 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:26.894472 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:31.346767 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:56:33.821847 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-17 20:56:36.299213 | Idx:  400/400    | loss: 0.009 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0091 Acc: 99.92%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0026 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 20:57:06.158586 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:08.606410 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:12.608862 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:15.060752 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:17.502611 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:21.515362 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:23.962377 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:28.396860 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 20:57:30.837709 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:57:33.262088 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0031 Acc: 99.97%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0020 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 20:58:02.319783 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:04.759211 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.032 | acc: 100.00% |
| 2025-10-17 20:58:08.643767 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:11.089609 | Idx:  160/400    | loss: 0.026 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 20:58:13.526373 | Idx:  200/400    | loss: 0.021 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:17.518863 | Idx:  240/400    | loss: 0.018 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:19.960175 | Idx:  280/400    | loss: 0.017 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 20:58:23.958032 | Idx:  320/400    | loss: 0.015 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:26.396167 | Idx:  360/400    | loss: 0.014 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 20:58:28.817144 | Idx:  400/400    | loss: 0.013 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0126 Acc: 99.87%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_205400-kvhjm4ds[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_205400-kvhjm4ds\logs[0m
rule: B345/S5 \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_20-58-55_tiny_2_layer_seq_cnn__200-200-B345_S5


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 8, 200, 200]          408
©À©¤BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-3                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 8, 200, 200]          1,608
©À©¤BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-6                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 20:59:11.608269 | Idx:   40/400    | loss: 0.716 | grad_norm: 1.087 | acc: 89.56% |
| 2025-10-17 20:59:14.036825 | Idx:   80/400    | loss: 0.466 | grad_norm: 1.096 | acc: 98.64% |
| 2025-10-17 20:59:18.045351 | Idx:  120/400    | loss: 0.382 | grad_norm: 0.611 | acc: 94.20% |
| 2025-10-17 20:59:20.498574 | Idx:  160/400    | loss: 0.321 | grad_norm: 0.900 | acc: 99.00% |
| 2025-10-17 20:59:22.936501 | Idx:  200/400    | loss: 0.276 | grad_norm: 0.263 | acc: 99.03% |
| 2025-10-17 20:59:27.078757 | Idx:  240/400    | loss: 0.238 | grad_norm: 0.908 | acc: 99.18% |
| 2025-10-17 20:59:29.523650 | Idx:  280/400    | loss: 0.212 | grad_norm: 0.107 | acc: 99.66% |
| 2025-10-17 20:59:33.533165 | Idx:  320/400    | loss: 0.191 | grad_norm: 0.035 | acc: 99.88% |
| 2025-10-17 20:59:35.979228 | Idx:  360/400    | loss: 0.176 | grad_norm: 0.329 | acc: 99.39% |
| 2025-10-17 20:59:38.407838 | Idx:  400/400    | loss: 0.161 | grad_norm: 0.080 | acc: 99.89% |
Train Loss: 0.1610 Acc: 96.89%
Acc: 99.83%


| val_epoch_acc: 99.83% | epoch: 00 | avg_train_loss: 0.0287 | avg_train_acc: 99.6217 | 


Epoch 2/8
----------
| 2025-10-17 21:00:07.011518 | Idx:   40/400    | loss: 0.016 | grad_norm: 0.108 | acc: 99.62% |
| 2025-10-17 21:00:09.448936 | Idx:   80/400    | loss: 0.030 | grad_norm: 0.063 | acc: 99.76% |
| 2025-10-17 21:00:13.445659 | Idx:  120/400    | loss: 0.025 | grad_norm: 0.031 | acc: 99.83% |
| 2025-10-17 21:00:15.887207 | Idx:  160/400    | loss: 0.026 | grad_norm: 0.028 | acc: 99.96% |
| 2025-10-17 21:00:18.339953 | Idx:  200/400    | loss: 0.023 | grad_norm: 0.022 | acc: 99.98% |
| 2025-10-17 21:00:22.329269 | Idx:  240/400    | loss: 0.021 | grad_norm: 0.796 | acc: 97.99% |
| 2025-10-17 21:00:24.769856 | Idx:  280/400    | loss: 0.026 | grad_norm: 0.260 | acc: 99.89% |
| 2025-10-17 21:00:29.211777 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.082 | acc: 99.89% |
| 2025-10-17 21:00:31.652021 | Idx:  360/400    | loss: 0.027 | grad_norm: 0.425 | acc: 99.49% |
| 2025-10-17 21:00:34.077064 | Idx:  400/400    | loss: 0.027 | grad_norm: 0.030 | acc: 99.95% |
Train Loss: 0.0267 Acc: 99.65%
Acc: 99.94%


| val_epoch_acc: 99.94% | epoch: 01 | avg_train_loss: 0.0235 | avg_train_acc: 99.7238 | 


Epoch 3/8
----------
| 2025-10-17 21:01:02.474745 | Idx:   40/400    | loss: 0.025 | grad_norm: 0.118 | acc: 99.97% |
| 2025-10-17 21:01:04.909703 | Idx:   80/400    | loss: 0.019 | grad_norm: 0.025 | acc: 99.94% |
| 2025-10-17 21:01:08.848619 | Idx:  120/400    | loss: 0.015 | grad_norm: 0.003 | acc: 99.99% |
| 2025-10-17 21:01:11.280698 | Idx:  160/400    | loss: 0.013 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:01:13.934889 | Idx:  200/400    | loss: 0.012 | grad_norm: 0.092 | acc: 99.87% |
| 2025-10-17 21:01:18.149857 | Idx:  240/400    | loss: 0.011 | grad_norm: 0.012 | acc: 99.96% |
| 2025-10-17 21:01:20.641726 | Idx:  280/400    | loss: 0.010 | grad_norm: 0.091 | acc: 99.90% |
| 2025-10-17 21:01:25.013914 | Idx:  320/400    | loss: 0.014 | grad_norm: 1.334 | acc: 95.90% |
| 2025-10-17 21:01:27.543212 | Idx:  360/400    | loss: 0.016 | grad_norm: 0.033 | acc: 99.95% |
| 2025-10-17 21:01:29.978536 | Idx:  400/400    | loss: 0.021 | grad_norm: 0.748 | acc: 97.83% |
Train Loss: 0.0215 Acc: 99.73%
Acc: 99.25%


| val_epoch_acc: 99.25% | epoch: 02 | avg_train_loss: 0.0921 | avg_train_acc: 98.6279 | 


Epoch 4/8
----------
| 2025-10-17 21:01:58.002100 | Idx:   40/400    | loss: 0.025 | grad_norm: 0.148 | acc: 99.87% |
| 2025-10-17 21:02:00.423632 | Idx:   80/400    | loss: 0.017 | grad_norm: 0.005 | acc: 99.99% |
| 2025-10-17 21:02:04.348989 | Idx:  120/400    | loss: 0.014 | grad_norm: 0.007 | acc: 99.99% |
| 2025-10-17 21:02:06.891003 | Idx:  160/400    | loss: 0.012 | grad_norm: 0.011 | acc: 99.99% |
| 2025-10-17 21:02:09.287696 | Idx:  200/400    | loss: 0.011 | grad_norm: 0.004 | acc: 99.99% |
| 2025-10-17 21:02:13.286699 | Idx:  240/400    | loss: 0.010 | grad_norm: 0.019 | acc: 99.96% |
| 2025-10-17 21:02:15.708913 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.804 | acc: 98.79% |
| 2025-10-17 21:02:19.646267 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.104 | acc: 99.83% |
| 2025-10-17 21:02:22.080186 | Idx:  360/400    | loss: 0.023 | grad_norm: 0.236 | acc: 99.59% |
| 2025-10-17 21:02:24.470705 | Idx:  400/400    | loss: 0.022 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0217 Acc: 99.75%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0085 | avg_train_acc: 99.9711 | 


Epoch 5/8
----------
| 2025-10-17 21:02:52.371818 | Idx:   40/400    | loss: 0.007 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:02:54.827899 | Idx:   80/400    | loss: 0.007 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:02:58.848317 | Idx:  120/400    | loss: 0.014 | grad_norm: 0.292 | acc: 99.37% |
| 2025-10-17 21:03:01.339304 | Idx:  160/400    | loss: 0.035 | grad_norm: 0.046 | acc: 99.90% |
| 2025-10-17 21:03:03.805289 | Idx:  200/400    | loss: 0.032 | grad_norm: 0.006 | acc: 99.99% |
| 2025-10-17 21:03:07.800715 | Idx:  240/400    | loss: 0.031 | grad_norm: 1.684 | acc: 91.85% |
| 2025-10-17 21:03:10.259731 | Idx:  280/400    | loss: 0.034 | grad_norm: 0.531 | acc: 98.21% |
| 2025-10-17 21:03:14.277770 | Idx:  320/400    | loss: 0.031 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 21:03:16.774485 | Idx:  360/400    | loss: 0.028 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:03:19.238640 | Idx:  400/400    | loss: 0.026 | grad_norm: 0.004 | acc: 100.00% |
Train Loss: 0.0263 Acc: 99.69%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_205855-se58uxl3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_205855-se58uxl3\logs[0m
rule: B13/S012V \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_21-03-45_tiny_2_layer_seq_cnn__200-200-B13_S012V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 8, 200, 200]          408
©À©¤BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-3                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 8, 200, 200]          1,608
©À©¤BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-6                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:04:02.429547 | Idx:   40/400    | loss: 0.493 | grad_norm: 0.235 | acc: 88.99% |
| 2025-10-17 21:04:05.143802 | Idx:   80/400    | loss: 0.382 | grad_norm: 0.203 | acc: 92.51% |
| 2025-10-17 21:04:09.167040 | Idx:  120/400    | loss: 0.317 | grad_norm: 0.122 | acc: 97.85% |
| 2025-10-17 21:04:11.755870 | Idx:  160/400    | loss: 0.271 | grad_norm: 0.073 | acc: 97.91% |
| 2025-10-17 21:04:14.314038 | Idx:  200/400    | loss: 0.234 | grad_norm: 0.287 | acc: 98.90% |
| 2025-10-17 21:04:18.842338 | Idx:  240/400    | loss: 0.224 | grad_norm: 0.134 | acc: 94.50% |
| 2025-10-17 21:04:21.417606 | Idx:  280/400    | loss: 0.208 | grad_norm: 0.113 | acc: 98.68% |
| 2025-10-17 21:04:25.461936 | Idx:  320/400    | loss: 0.189 | grad_norm: 0.022 | acc: 99.72% |
| 2025-10-17 21:04:27.972951 | Idx:  360/400    | loss: 0.175 | grad_norm: 0.157 | acc: 97.41% |
| 2025-10-17 21:04:30.428927 | Idx:  400/400    | loss: 0.164 | grad_norm: 0.021 | acc: 99.71% |
Train Loss: 0.1639 Acc: 96.25%
Acc: 99.68%


| val_epoch_acc: 99.68% | epoch: 00 | avg_train_loss: 0.0494 | avg_train_acc: 99.1555 | 


Epoch 2/8
----------
| 2025-10-17 21:04:56.778248 | Idx:   40/400    | loss: 0.015 | grad_norm: 0.008 | acc: 99.94% |
| 2025-10-17 21:04:59.282716 | Idx:   80/400    | loss: 0.012 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-17 21:05:03.326369 | Idx:  120/400    | loss: 0.010 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:05:05.834112 | Idx:  160/400    | loss: 0.009 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:05:08.377749 | Idx:  200/400    | loss: 0.008 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:05:12.408420 | Idx:  240/400    | loss: 0.008 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:05:14.905489 | Idx:  280/400    | loss: 0.007 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:05:18.894740 | Idx:  320/400    | loss: 0.007 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:05:21.437798 | Idx:  360/400    | loss: 0.006 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:05:23.960103 | Idx:  400/400    | loss: 0.006 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0061 Acc: 99.98%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0034 | avg_train_acc: 100.0000 | 


Epoch 3/8
----------
| 2025-10-17 21:05:56.104586 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:05:58.788226 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:02.953408 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:06:05.457728 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:06:07.936950 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:11.895339 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:14.451014 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:18.483330 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:21.008551 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:06:23.520509 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.000 | acc: 100.00% |
Train Loss: 0.0028 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0023 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 21:06:50.380351 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:06:52.857908 | Idx:   80/400    | loss: 0.293 | grad_norm: 0.107 | acc: 95.67% |
| 2025-10-17 21:06:56.883731 | Idx:  120/400    | loss: 0.255 | grad_norm: 0.112 | acc: 96.77% |
| 2025-10-17 21:06:59.394384 | Idx:  160/400    | loss: 0.231 | grad_norm: 0.029 | acc: 97.42% |
| 2025-10-17 21:07:01.893475 | Idx:  200/400    | loss: 0.211 | grad_norm: 0.080 | acc: 98.44% |
| 2025-10-17 21:07:05.901200 | Idx:  240/400    | loss: 0.196 | grad_norm: 0.040 | acc: 98.14% |
| 2025-10-17 21:07:08.388421 | Idx:  280/400    | loss: 0.184 | grad_norm: 0.092 | acc: 99.29% |
| 2025-10-17 21:07:12.291695 | Idx:  320/400    | loss: 0.167 | grad_norm: 0.074 | acc: 98.59% |
| 2025-10-17 21:07:14.754083 | Idx:  360/400    | loss: 0.153 | grad_norm: 0.025 | acc: 99.77% |
| 2025-10-17 21:07:17.260959 | Idx:  400/400    | loss: 0.140 | grad_norm: 0.020 | acc: 99.46% |
Train Loss: 0.1405 Acc: 97.51%
Acc: 99.74%


| val_epoch_acc: 99.74% | epoch: 03 | avg_train_loss: 0.0267 | avg_train_acc: 99.6548 | 


Epoch 5/8
----------
| 2025-10-17 21:07:43.647698 | Idx:   40/400    | loss: 0.017 | grad_norm: 0.010 | acc: 99.94% |
| 2025-10-17 21:07:46.147403 | Idx:   80/400    | loss: 0.016 | grad_norm: 0.013 | acc: 99.98% |
| 2025-10-17 21:07:50.647246 | Idx:  120/400    | loss: 0.014 | grad_norm: 0.011 | acc: 99.98% |
| 2025-10-17 21:07:53.120201 | Idx:  160/400    | loss: 0.013 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:07:55.578983 | Idx:  200/400    | loss: 0.012 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:07:59.484740 | Idx:  240/400    | loss: 0.011 | grad_norm: 0.007 | acc: 99.99% |
| 2025-10-17 21:08:02.064695 | Idx:  280/400    | loss: 0.011 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:08:06.131960 | Idx:  320/400    | loss: 0.010 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:08:08.626853 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:08:11.099279 | Idx:  400/400    | loss: 0.009 | grad_norm: 0.018 | acc: 100.00% |
Train Loss: 0.0092 Acc: 99.97%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_210345-5w44djb2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_210345-5w44djb2\logs[0m
rule: B2/S013V \t, network: small_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNSmall'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/small_2_layer_seq_cnn.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_21-08-33_tiny_2_layer_seq_cnn__200-200-B2_S013V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNSmall                           [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 8, 200, 200]          408
©À©¤BatchNorm2d: 1-2                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-3                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 8, 200, 200]          1,608
©À©¤BatchNorm2d: 1-5                       [1, 8, 200, 200]          16
©À©¤ReLU: 1-6                              [1, 8, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          402
==========================================================================================
Total params: 2,450
Trainable params: 2,450
Non-trainable params: 0
Total mult-adds (M): 96.72
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 10.88
Params size (MB): 0.01
Estimated Total Size (MB): 11.21
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:08:50.044719 | Idx:   40/400    | loss: 0.857 | grad_norm: 0.267 | acc: 86.28% |
| 2025-10-17 21:08:52.536941 | Idx:   80/400    | loss: 0.698 | grad_norm: 0.223 | acc: 89.57% |
| 2025-10-17 21:08:56.461746 | Idx:  120/400    | loss: 0.563 | grad_norm: 0.176 | acc: 96.87% |
| 2025-10-17 21:08:58.941935 | Idx:  160/400    | loss: 0.449 | grad_norm: 0.319 | acc: 99.81% |
| 2025-10-17 21:09:01.385673 | Idx:  200/400    | loss: 0.369 | grad_norm: 0.070 | acc: 99.98% |
| 2025-10-17 21:09:05.400655 | Idx:  240/400    | loss: 0.320 | grad_norm: 0.163 | acc: 99.63% |
| 2025-10-17 21:09:07.889926 | Idx:  280/400    | loss: 0.278 | grad_norm: 0.121 | acc: 99.95% |
| 2025-10-17 21:09:11.899325 | Idx:  320/400    | loss: 0.244 | grad_norm: 0.019 | acc: 100.00% |
| 2025-10-17 21:09:14.387912 | Idx:  360/400    | loss: 0.218 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 21:09:16.867708 | Idx:  400/400    | loss: 0.197 | grad_norm: 0.006 | acc: 100.00% |
Train Loss: 0.1967 Acc: 95.19%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 00 | avg_train_loss: 0.0053 | avg_train_acc: 99.9997 | 


Epoch 2/8
----------
| 2025-10-17 21:09:43.918752 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.024 | acc: 100.00% |
| 2025-10-17 21:09:46.399058 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:09:50.385493 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:09:52.886962 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 21:09:55.384764 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:09:59.323544 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:10:01.820342 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:10:05.663084 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:10:08.033890 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.023 | acc: 100.00% |
| 2025-10-17 21:10:10.408066 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.128 | acc: 100.00% |
Train Loss: 0.0034 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0027 | avg_train_acc: 100.0000 | 


Epoch 3/8
----------
| 2025-10-17 21:10:36.262448 | Idx:   40/400    | loss: 0.445 | grad_norm: 0.271 | acc: 94.44% |
| 2025-10-17 21:10:38.689327 | Idx:   80/400    | loss: 0.268 | grad_norm: 0.159 | acc: 99.94% |
| 2025-10-17 21:10:42.600289 | Idx:  120/400    | loss: 0.184 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 21:10:45.004554 | Idx:  160/400    | loss: 0.140 | grad_norm: 0.029 | acc: 100.00% |
| 2025-10-17 21:10:47.400244 | Idx:  200/400    | loss: 0.114 | grad_norm: 0.018 | acc: 100.00% |
| 2025-10-17 21:10:51.336533 | Idx:  240/400    | loss: 0.096 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:10:53.731122 | Idx:  280/400    | loss: 0.083 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:10:57.650272 | Idx:  320/400    | loss: 0.073 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:11:00.092076 | Idx:  360/400    | loss: 0.066 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:11:02.502361 | Idx:  400/400    | loss: 0.059 | grad_norm: 0.007 | acc: 100.00% |
Train Loss: 0.0594 Acc: 99.19%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0045 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 21:11:28.420765 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:11:30.819604 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:11:34.744215 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:11:37.156001 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:11:39.548840 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:11:43.363045 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:11:45.797869 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:11:49.684075 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:11:52.076868 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:11:54.457536 | Idx:  400/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0036 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0029 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 21:12:19.905819 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:12:22.288433 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:12:26.545340 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:12:28.956921 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:12:31.388934 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:12:35.257424 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.023 | acc: 100.00% |
| 2025-10-17 21:12:37.675856 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:12:41.510210 | Idx:  320/400    | loss: 0.034 | grad_norm: 0.043 | acc: 99.99% |
| 2025-10-17 21:12:43.928506 | Idx:  360/400    | loss: 0.031 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:12:46.312276 | Idx:  400/400    | loss: 0.028 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0285 Acc: 99.68%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_210833-l9p28cli[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_210833-l9p28cli\logs[0m
rule: B2/S \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_21-13-09_tiny_2_layer_seq_cnn__200-200-B2_S


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 1, 200, 200]          51
©À©¤BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-3                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 1, 200, 200]          26
©À©¤BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-6                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:13:23.621712 | Idx:   40/400    | loss: 1.338 | grad_norm: 0.161 | acc: 63.87% |
| 2025-10-17 21:13:24.785014 | Idx:   80/400    | loss: 1.160 | grad_norm: 0.452 | acc: 80.95% |
| 2025-10-17 21:13:27.408369 | Idx:  120/400    | loss: 1.003 | grad_norm: 1.597 | acc: 83.86% |
| 2025-10-17 21:13:28.605038 | Idx:  160/400    | loss: 0.865 | grad_norm: 1.466 | acc: 96.08% |
| 2025-10-17 21:13:29.778856 | Idx:  200/400    | loss: 0.766 | grad_norm: 2.004 | acc: 94.16% |
| 2025-10-17 21:13:32.584762 | Idx:  240/400    | loss: 0.689 | grad_norm: 0.958 | acc: 95.32% |
| 2025-10-17 21:13:33.722403 | Idx:  280/400    | loss: 0.638 | grad_norm: 16.892 | acc: 83.15% |
| 2025-10-17 21:13:36.348891 | Idx:  320/400    | loss: 0.591 | grad_norm: 1.301 | acc: 94.97% |
| 2025-10-17 21:13:37.486599 | Idx:  360/400    | loss: 0.558 | grad_norm: 1.071 | acc: 96.45% |
| 2025-10-17 21:13:38.625084 | Idx:  400/400    | loss: 0.522 | grad_norm: 2.725 | acc: 93.27% |
Train Loss: 0.5217 Acc: 87.36%
Acc: 97.64%


| val_epoch_acc: 97.64% | epoch: 00 | avg_train_loss: 0.2057 | avg_train_acc: 97.0801 | 


Epoch 2/8
----------
| 2025-10-17 21:14:03.797726 | Idx:   40/400    | loss: 0.300 | grad_norm: 0.303 | acc: 98.56% |
| 2025-10-17 21:14:04.972482 | Idx:   80/400    | loss: 0.242 | grad_norm: 0.312 | acc: 98.73% |
| 2025-10-17 21:14:07.552829 | Idx:  120/400    | loss: 0.217 | grad_norm: 4.337 | acc: 97.79% |
| 2025-10-17 21:14:08.698735 | Idx:  160/400    | loss: 0.221 | grad_norm: 1.431 | acc: 98.47% |
| 2025-10-17 21:14:09.856537 | Idx:  200/400    | loss: 0.212 | grad_norm: 0.267 | acc: 98.63% |
| 2025-10-17 21:14:12.451353 | Idx:  240/400    | loss: 0.224 | grad_norm: 1.471 | acc: 98.98% |
| 2025-10-17 21:14:13.595840 | Idx:  280/400    | loss: 0.222 | grad_norm: 0.146 | acc: 98.97% |
| 2025-10-17 21:14:16.488805 | Idx:  320/400    | loss: 0.212 | grad_norm: 0.273 | acc: 99.08% |
| 2025-10-17 21:14:17.632562 | Idx:  360/400    | loss: 0.201 | grad_norm: 0.532 | acc: 98.75% |
| 2025-10-17 21:14:18.766532 | Idx:  400/400    | loss: 0.195 | grad_norm: 0.401 | acc: 99.56% |
Train Loss: 0.1949 Acc: 97.27%
Acc: 99.58%


| val_epoch_acc: 99.58% | epoch: 01 | avg_train_loss: 0.1604 | avg_train_acc: 98.3257 | 


Epoch 3/8
----------
| 2025-10-17 21:14:44.911391 | Idx:   40/400    | loss: 0.182 | grad_norm: 0.500 | acc: 99.21% |
| 2025-10-17 21:14:46.114636 | Idx:   80/400    | loss: 0.151 | grad_norm: 0.430 | acc: 99.32% |
| 2025-10-17 21:14:48.926257 | Idx:  120/400    | loss: 0.125 | grad_norm: 0.429 | acc: 99.30% |
| 2025-10-17 21:14:50.100149 | Idx:  160/400    | loss: 0.133 | grad_norm: 0.765 | acc: 98.13% |
| 2025-10-17 21:14:51.330535 | Idx:  200/400    | loss: 0.124 | grad_norm: 0.131 | acc: 99.29% |
| 2025-10-17 21:14:54.097155 | Idx:  240/400    | loss: 0.166 | grad_norm: 0.617 | acc: 99.02% |
| 2025-10-17 21:14:55.285918 | Idx:  280/400    | loss: 0.163 | grad_norm: 0.503 | acc: 99.06% |
| 2025-10-17 21:14:58.260413 | Idx:  320/400    | loss: 0.166 | grad_norm: 2.067 | acc: 99.15% |
| 2025-10-17 21:14:59.434670 | Idx:  360/400    | loss: 0.180 | grad_norm: 0.964 | acc: 98.68% |
| 2025-10-17 21:15:00.585855 | Idx:  400/400    | loss: 0.194 | grad_norm: 1.107 | acc: 98.35% |
Train Loss: 0.1942 Acc: 98.09%
Acc: 85.55%


| val_epoch_acc: 85.55% | epoch: 02 | avg_train_loss: 0.4042 | avg_train_acc: 97.3082 | 


Epoch 4/8
----------
| 2025-10-17 21:15:26.419774 | Idx:   40/400    | loss: 0.135 | grad_norm: 0.150 | acc: 99.43% |
| 2025-10-17 21:15:27.576699 | Idx:   80/400    | loss: 0.115 | grad_norm: 0.402 | acc: 99.70% |
| 2025-10-17 21:15:30.190492 | Idx:  120/400    | loss: 0.208 | grad_norm: 1.564 | acc: 96.71% |
| 2025-10-17 21:15:31.331999 | Idx:  160/400    | loss: 0.182 | grad_norm: 0.195 | acc: 99.41% |
| 2025-10-17 21:15:32.544287 | Idx:  200/400    | loss: 0.172 | grad_norm: 0.577 | acc: 98.84% |
| 2025-10-17 21:15:35.168257 | Idx:  240/400    | loss: 0.259 | grad_norm: 3.702 | acc: 98.17% |
| 2025-10-17 21:15:36.304573 | Idx:  280/400    | loss: 0.267 | grad_norm: 1.050 | acc: 99.17% |
| 2025-10-17 21:15:38.977628 | Idx:  320/400    | loss: 0.246 | grad_norm: 0.630 | acc: 98.74% |
| 2025-10-17 21:15:40.133056 | Idx:  360/400    | loss: 0.279 | grad_norm: 0.295 | acc: 99.47% |
| 2025-10-17 21:15:41.275818 | Idx:  400/400    | loss: 0.262 | grad_norm: 1.359 | acc: 98.66% |
Train Loss: 0.2618 Acc: 98.35%
Acc: 97.60%


| val_epoch_acc: 97.60% | epoch: 03 | avg_train_loss: 0.1112 | avg_train_acc: 98.7610 | 


Epoch 5/8
----------
| 2025-10-17 21:16:07.134043 | Idx:   40/400    | loss: 0.149 | grad_norm: 0.966 | acc: 99.32% |
| 2025-10-17 21:16:08.359732 | Idx:   80/400    | loss: 0.123 | grad_norm: 0.480 | acc: 99.42% |
| 2025-10-17 21:16:10.950970 | Idx:  120/400    | loss: 0.161 | grad_norm: 0.302 | acc: 99.67% |
| 2025-10-17 21:16:12.113750 | Idx:  160/400    | loss: 0.148 | grad_norm: 0.196 | acc: 99.45% |
| 2025-10-17 21:16:13.278395 | Idx:  200/400    | loss: 0.176 | grad_norm: 0.137 | acc: 99.60% |
| 2025-10-17 21:16:15.929419 | Idx:  240/400    | loss: 0.156 | grad_norm: 0.507 | acc: 99.36% |
| 2025-10-17 21:16:17.073223 | Idx:  280/400    | loss: 0.142 | grad_norm: 0.759 | acc: 99.33% |
| 2025-10-17 21:16:19.737753 | Idx:  320/400    | loss: 0.133 | grad_norm: 0.338 | acc: 99.58% |
| 2025-10-17 21:16:20.913699 | Idx:  360/400    | loss: 0.146 | grad_norm: 1.010 | acc: 98.08% |
| 2025-10-17 21:16:22.044062 | Idx:  400/400    | loss: 0.135 | grad_norm: 0.216 | acc: 99.58% |
Train Loss: 0.1352 Acc: 98.85%
Acc: 83.59%


| val_epoch_acc: 83.59% | epoch: 04 | avg_train_loss: 0.0410 | avg_train_acc: 99.5188 | 


Epoch 6/8
----------
| 2025-10-17 21:16:49.150646 | Idx:   40/400    | loss: 0.357 | grad_norm: 0.803 | acc: 98.87% |
| 2025-10-17 21:16:50.326842 | Idx:   80/400    | loss: 0.253 | grad_norm: 0.786 | acc: 98.53% |
| 2025-10-17 21:16:53.023476 | Idx:  120/400    | loss: 0.188 | grad_norm: 0.221 | acc: 99.79% |
| 2025-10-17 21:16:54.249338 | Idx:  160/400    | loss: 0.208 | grad_norm: 1.130 | acc: 97.41% |
| 2025-10-17 21:16:55.418558 | Idx:  200/400    | loss: 0.180 | grad_norm: 0.222 | acc: 99.59% |
| 2025-10-17 21:16:58.102138 | Idx:  240/400    | loss: 0.174 | grad_norm: 0.146 | acc: 99.61% |
| 2025-10-17 21:16:59.384492 | Idx:  280/400    | loss: 0.162 | grad_norm: 0.397 | acc: 99.68% |
| 2025-10-17 21:17:02.035362 | Idx:  320/400    | loss: 0.146 | grad_norm: 0.173 | acc: 99.59% |
| 2025-10-17 21:17:03.198263 | Idx:  360/400    | loss: 0.162 | grad_norm: 0.271 | acc: 99.62% |
| 2025-10-17 21:17:04.328738 | Idx:  400/400    | loss: 0.161 | grad_norm: 6.119 | acc: 98.16% |
Train Loss: 0.1607 Acc: 98.49%
Acc: 97.93%


| val_epoch_acc: 97.93% | epoch: 05 | avg_train_loss: 0.1833 | avg_train_acc: 98.1328 | 


Epoch 7/8
----------
| 2025-10-17 21:17:30.919033 | Idx:   40/400    | loss: 0.106 | grad_norm: 0.093 | acc: 99.73% |
| 2025-10-17 21:17:32.148580 | Idx:   80/400    | loss: 0.253 | grad_norm: 0.081 | acc: 99.68% |
| 2025-10-17 21:17:34.831080 | Idx:  120/400    | loss: 0.203 | grad_norm: 0.530 | acc: 99.07% |
| 2025-10-17 21:17:36.010719 | Idx:  160/400    | loss: 0.204 | grad_norm: 2.095 | acc: 96.68% |
| 2025-10-17 21:17:37.183502 | Idx:  200/400    | loss: 0.265 | grad_norm: 0.593 | acc: 99.59% |
| 2025-10-17 21:17:39.858557 | Idx:  240/400    | loss: 0.246 | grad_norm: 0.177 | acc: 99.35% |
| 2025-10-17 21:17:41.034877 | Idx:  280/400    | loss: 0.218 | grad_norm: 0.122 | acc: 99.71% |
| 2025-10-17 21:17:43.796990 | Idx:  320/400    | loss: 0.235 | grad_norm: 0.313 | acc: 99.58% |
| 2025-10-17 21:17:45.036969 | Idx:  360/400    | loss: 0.227 | grad_norm: 0.997 | acc: 98.52% |
| 2025-10-17 21:17:46.198964 | Idx:  400/400    | loss: 0.221 | grad_norm: 0.616 | acc: 98.96% |
Train Loss: 0.2209 Acc: 98.47%
Acc: 98.76%


| val_epoch_acc: 98.76% | epoch: 06 | avg_train_loss: 0.0954 | avg_train_acc: 98.3811 | 


Epoch 8/8
----------
| 2025-10-17 21:18:12.913131 | Idx:   40/400    | loss: 0.720 | grad_norm: 0.519 | acc: 99.32% |
| 2025-10-17 21:18:14.108093 | Idx:   80/400    | loss: 0.398 | grad_norm: 0.266 | acc: 99.53% |
| 2025-10-17 21:18:16.851921 | Idx:  120/400    | loss: 0.289 | grad_norm: 13.226 | acc: 95.29% |
| 2025-10-17 21:18:18.045271 | Idx:  160/400    | loss: 0.243 | grad_norm: 0.076 | acc: 99.76% |
| 2025-10-17 21:18:19.232900 | Idx:  200/400    | loss: 0.205 | grad_norm: 0.330 | acc: 99.51% |
| 2025-10-17 21:18:22.008773 | Idx:  240/400    | loss: 0.214 | grad_norm: 0.337 | acc: 99.41% |
| 2025-10-17 21:18:23.184265 | Idx:  280/400    | loss: 0.215 | grad_norm: 0.215 | acc: 99.88% |
| 2025-10-17 21:18:25.851264 | Idx:  320/400    | loss: 0.203 | grad_norm: 0.476 | acc: 99.10% |
| 2025-10-17 21:18:27.002169 | Idx:  360/400    | loss: 0.183 | grad_norm: 2.930 | acc: 98.79% |
| 2025-10-17 21:18:28.137414 | Idx:  400/400    | loss: 0.169 | grad_norm: 0.206 | acc: 99.83% |
Train Loss: 0.1689 Acc: 98.97%
Acc: 99.74%


| val_epoch_acc: 99.74% | epoch: 07 | avg_train_loss: 0.0395 | avg_train_acc: 99.5414 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_211309-dae47sz8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_211309-dae47sz8\logs[0m
rule: B345/S5 \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_21-18-53_tiny_2_layer_seq_cnn__200-200-B345_S5


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 1, 200, 200]          51
©À©¤BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-3                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 1, 200, 200]          26
©À©¤BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-6                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:19:08.219852 | Idx:   40/400    | loss: 1.055 | grad_norm: 0.172 | acc: 83.13% |
| 2025-10-17 21:19:09.415749 | Idx:   80/400    | loss: 0.870 | grad_norm: 0.295 | acc: 86.54% |
| 2025-10-17 21:19:12.216600 | Idx:  120/400    | loss: 0.764 | grad_norm: 0.342 | acc: 89.77% |
| 2025-10-17 21:19:13.395766 | Idx:  160/400    | loss: 0.683 | grad_norm: 0.568 | acc: 92.68% |
| 2025-10-17 21:19:14.582147 | Idx:  200/400    | loss: 0.607 | grad_norm: 0.198 | acc: 94.02% |
| 2025-10-17 21:19:17.454197 | Idx:  240/400    | loss: 0.554 | grad_norm: 0.662 | acc: 92.98% |
| 2025-10-17 21:19:18.646081 | Idx:  280/400    | loss: 0.510 | grad_norm: 1.009 | acc: 92.90% |
| 2025-10-17 21:19:21.360641 | Idx:  320/400    | loss: 0.477 | grad_norm: 0.782 | acc: 93.32% |
| 2025-10-17 21:19:22.552150 | Idx:  360/400    | loss: 0.454 | grad_norm: 0.287 | acc: 94.24% |
| 2025-10-17 21:19:23.731748 | Idx:  400/400    | loss: 0.432 | grad_norm: 0.531 | acc: 94.97% |
Train Loss: 0.4320 Acc: 90.31%
Acc: 93.95%


| val_epoch_acc: 93.95% | epoch: 00 | avg_train_loss: 0.2217 | avg_train_acc: 94.7863 | 


Epoch 2/8
----------
| 2025-10-17 21:19:50.079817 | Idx:   40/400    | loss: 0.213 | grad_norm: 1.161 | acc: 94.64% |
| 2025-10-17 21:19:51.298983 | Idx:   80/400    | loss: 0.212 | grad_norm: 0.270 | acc: 95.89% |
| 2025-10-17 21:19:54.075194 | Idx:  120/400    | loss: 0.212 | grad_norm: 0.276 | acc: 95.08% |
| 2025-10-17 21:19:55.241525 | Idx:  160/400    | loss: 0.214 | grad_norm: 1.403 | acc: 92.91% |
| 2025-10-17 21:19:56.423094 | Idx:  200/400    | loss: 0.219 | grad_norm: 0.225 | acc: 95.30% |
| 2025-10-17 21:19:59.149216 | Idx:  240/400    | loss: 0.217 | grad_norm: 0.424 | acc: 96.94% |
| 2025-10-17 21:20:00.311374 | Idx:  280/400    | loss: 0.216 | grad_norm: 0.661 | acc: 94.20% |
| 2025-10-17 21:20:03.294009 | Idx:  320/400    | loss: 0.219 | grad_norm: 0.445 | acc: 94.58% |
| 2025-10-17 21:20:04.459475 | Idx:  360/400    | loss: 0.217 | grad_norm: 0.487 | acc: 94.94% |
| 2025-10-17 21:20:05.587572 | Idx:  400/400    | loss: 0.219 | grad_norm: 0.403 | acc: 94.74% |
Train Loss: 0.2190 Acc: 95.03%
Acc: 95.32%


| val_epoch_acc: 95.32% | epoch: 01 | avg_train_loss: 0.2438 | avg_train_acc: 94.6806 | 


Epoch 3/8
----------
| 2025-10-17 21:20:32.114718 | Idx:   40/400    | loss: 0.212 | grad_norm: 0.449 | acc: 96.39% |
| 2025-10-17 21:20:33.338367 | Idx:   80/400    | loss: 0.230 | grad_norm: 1.839 | acc: 91.04% |
| 2025-10-17 21:20:35.901131 | Idx:  120/400    | loss: 0.229 | grad_norm: 0.310 | acc: 94.83% |
| 2025-10-17 21:20:37.122380 | Idx:  160/400    | loss: 0.223 | grad_norm: 0.180 | acc: 96.37% |
| 2025-10-17 21:20:38.305275 | Idx:  200/400    | loss: 0.223 | grad_norm: 0.473 | acc: 94.52% |
| 2025-10-17 21:20:41.021241 | Idx:  240/400    | loss: 0.227 | grad_norm: 0.492 | acc: 96.10% |
| 2025-10-17 21:20:42.177187 | Idx:  280/400    | loss: 0.226 | grad_norm: 0.644 | acc: 94.18% |
| 2025-10-17 21:20:45.225100 | Idx:  320/400    | loss: 0.226 | grad_norm: 0.206 | acc: 95.86% |
| 2025-10-17 21:20:46.419783 | Idx:  360/400    | loss: 0.225 | grad_norm: 0.308 | acc: 96.86% |
| 2025-10-17 21:20:47.560806 | Idx:  400/400    | loss: 0.224 | grad_norm: 0.903 | acc: 96.19% |
Train Loss: 0.2241 Acc: 95.01%
Acc: 96.40%


| val_epoch_acc: 96.40% | epoch: 02 | avg_train_loss: 0.2257 | avg_train_acc: 95.2423 | 


Epoch 4/8
----------
| 2025-10-17 21:21:14.127205 | Idx:   40/400    | loss: 0.217 | grad_norm: 0.190 | acc: 96.36% |
| 2025-10-17 21:21:15.279719 | Idx:   80/400    | loss: 0.223 | grad_norm: 0.925 | acc: 93.31% |
| 2025-10-17 21:21:17.964716 | Idx:  120/400    | loss: 0.227 | grad_norm: 0.164 | acc: 95.69% |
| 2025-10-17 21:21:19.143005 | Idx:  160/400    | loss: 0.220 | grad_norm: 0.337 | acc: 96.75% |
| 2025-10-17 21:21:20.314528 | Idx:  200/400    | loss: 0.219 | grad_norm: 0.260 | acc: 97.02% |
| 2025-10-17 21:21:23.038873 | Idx:  240/400    | loss: 0.220 | grad_norm: 0.311 | acc: 95.09% |
| 2025-10-17 21:21:24.230511 | Idx:  280/400    | loss: 0.218 | grad_norm: 0.150 | acc: 96.26% |
| 2025-10-17 21:21:26.978673 | Idx:  320/400    | loss: 0.217 | grad_norm: 0.622 | acc: 93.87% |
| 2025-10-17 21:21:28.148624 | Idx:  360/400    | loss: 0.214 | grad_norm: 0.248 | acc: 95.48% |
| 2025-10-17 21:21:29.517660 | Idx:  400/400    | loss: 0.214 | grad_norm: 1.208 | acc: 92.60% |
Train Loss: 0.2144 Acc: 95.14%
Acc: 95.38%


| val_epoch_acc: 95.38% | epoch: 03 | avg_train_loss: 0.2158 | avg_train_acc: 95.3167 | 


Epoch 5/8
----------
| 2025-10-17 21:21:55.888120 | Idx:   40/400    | loss: 0.249 | grad_norm: 0.649 | acc: 93.67% |
| 2025-10-17 21:21:57.040530 | Idx:   80/400    | loss: 0.229 | grad_norm: 0.488 | acc: 95.63% |
| 2025-10-17 21:21:59.818011 | Idx:  120/400    | loss: 0.231 | grad_norm: 0.288 | acc: 95.04% |
| 2025-10-17 21:22:00.982402 | Idx:  160/400    | loss: 0.230 | grad_norm: 0.574 | acc: 97.19% |
| 2025-10-17 21:22:02.149202 | Idx:  200/400    | loss: 0.224 | grad_norm: 0.353 | acc: 94.92% |
| 2025-10-17 21:22:04.951524 | Idx:  240/400    | loss: 0.221 | grad_norm: 0.064 | acc: 96.55% |
| 2025-10-17 21:22:06.109663 | Idx:  280/400    | loss: 0.219 | grad_norm: 0.704 | acc: 93.84% |
| 2025-10-17 21:22:08.755874 | Idx:  320/400    | loss: 0.217 | grad_norm: 0.534 | acc: 97.14% |
| 2025-10-17 21:22:10.019938 | Idx:  360/400    | loss: 0.214 | grad_norm: 0.854 | acc: 92.93% |
| 2025-10-17 21:22:11.156584 | Idx:  400/400    | loss: 0.215 | grad_norm: 0.502 | acc: 97.00% |
Train Loss: 0.2149 Acc: 95.14%
Acc: 95.55%


| val_epoch_acc: 95.55% | epoch: 04 | avg_train_loss: 0.2316 | avg_train_acc: 95.0618 | 


Epoch 6/8
----------
| 2025-10-17 21:22:38.128318 | Idx:   40/400    | loss: 0.238 | grad_norm: 0.396 | acc: 94.62% |
| 2025-10-17 21:22:39.359071 | Idx:   80/400    | loss: 0.218 | grad_norm: 0.308 | acc: 95.02% |
| 2025-10-17 21:22:42.057391 | Idx:  120/400    | loss: 0.215 | grad_norm: 0.431 | acc: 97.02% |
| 2025-10-17 21:22:43.219630 | Idx:  160/400    | loss: 0.213 | grad_norm: 0.472 | acc: 96.54% |
| 2025-10-17 21:22:44.492973 | Idx:  200/400    | loss: 0.212 | grad_norm: 0.348 | acc: 95.13% |
| 2025-10-17 21:22:47.300930 | Idx:  240/400    | loss: 0.212 | grad_norm: 0.565 | acc: 93.77% |
| 2025-10-17 21:22:48.479670 | Idx:  280/400    | loss: 0.213 | grad_norm: 0.589 | acc: 94.19% |
| 2025-10-17 21:22:51.136804 | Idx:  320/400    | loss: 0.210 | grad_norm: 0.158 | acc: 96.26% |
| 2025-10-17 21:22:52.326731 | Idx:  360/400    | loss: 0.212 | grad_norm: 1.170 | acc: 96.35% |
| 2025-10-17 21:22:53.547960 | Idx:  400/400    | loss: 0.211 | grad_norm: 0.238 | acc: 95.31% |
Train Loss: 0.2108 Acc: 95.29%
Acc: 95.53%


| val_epoch_acc: 95.53% | epoch: 05 | avg_train_loss: 0.2008 | avg_train_acc: 95.6363 | 


Epoch 7/8
----------
| 2025-10-17 21:23:19.969309 | Idx:   40/400    | loss: 0.203 | grad_norm: 0.957 | acc: 92.78% |
| 2025-10-17 21:23:21.157633 | Idx:   80/400    | loss: 0.208 | grad_norm: 1.209 | acc: 96.75% |
| 2025-10-17 21:23:24.016391 | Idx:  120/400    | loss: 0.204 | grad_norm: 1.279 | acc: 95.78% |
| 2025-10-17 21:23:25.293300 | Idx:  160/400    | loss: 0.212 | grad_norm: 0.652 | acc: 94.17% |
| 2025-10-17 21:23:26.643810 | Idx:  200/400    | loss: 0.208 | grad_norm: 0.525 | acc: 94.40% |
| 2025-10-17 21:23:29.476317 | Idx:  240/400    | loss: 0.209 | grad_norm: 0.132 | acc: 95.88% |
| 2025-10-17 21:23:30.675677 | Idx:  280/400    | loss: 0.211 | grad_norm: 1.112 | acc: 93.29% |
| 2025-10-17 21:23:33.437223 | Idx:  320/400    | loss: 0.213 | grad_norm: 0.361 | acc: 94.95% |
| 2025-10-17 21:23:34.633100 | Idx:  360/400    | loss: 0.214 | grad_norm: 0.462 | acc: 94.30% |
| 2025-10-17 21:23:35.774897 | Idx:  400/400    | loss: 0.212 | grad_norm: 0.591 | acc: 93.86% |
Train Loss: 0.2117 Acc: 95.22%
Acc: 96.71%


| val_epoch_acc: 96.71% | epoch: 06 | avg_train_loss: 0.1863 | avg_train_acc: 95.6299 | 


Epoch 8/8
----------
| 2025-10-17 21:24:02.029735 | Idx:   40/400    | loss: 0.180 | grad_norm: 1.483 | acc: 95.32% |
| 2025-10-17 21:24:03.206640 | Idx:   80/400    | loss: 0.194 | grad_norm: 0.680 | acc: 95.32% |
| 2025-10-17 21:24:05.886786 | Idx:  120/400    | loss: 0.197 | grad_norm: 0.417 | acc: 94.66% |
| 2025-10-17 21:24:07.139462 | Idx:  160/400    | loss: 0.196 | grad_norm: 0.340 | acc: 95.46% |
| 2025-10-17 21:24:08.383667 | Idx:  200/400    | loss: 0.198 | grad_norm: 0.492 | acc: 96.96% |
| 2025-10-17 21:24:11.011245 | Idx:  240/400    | loss: 0.200 | grad_norm: 0.190 | acc: 95.63% |
| 2025-10-17 21:24:12.153011 | Idx:  280/400    | loss: 0.201 | grad_norm: 0.521 | acc: 94.69% |
| 2025-10-17 21:24:14.946922 | Idx:  320/400    | loss: 0.202 | grad_norm: 0.226 | acc: 96.31% |
| 2025-10-17 21:24:16.114047 | Idx:  360/400    | loss: 0.201 | grad_norm: 0.087 | acc: 96.49% |
| 2025-10-17 21:24:17.251459 | Idx:  400/400    | loss: 0.203 | grad_norm: 0.377 | acc: 96.75% |
Train Loss: 0.2030 Acc: 95.45%
Acc: 96.25%


| val_epoch_acc: 96.25% | epoch: 07 | avg_train_loss: 0.2229 | avg_train_acc: 95.0558 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_211853-3x75vwh8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_211853-3x75vwh8\logs[0m
rule: B13/S012V \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_21-24-42_tiny_2_layer_seq_cnn__200-200-B13_S012V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 1, 200, 200]          51
©À©¤BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-3                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 1, 200, 200]          26
©À©¤BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-6                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:24:56.571183 | Idx:   40/400    | loss: 0.774 | grad_norm: 0.121 | acc: 85.36% |
| 2025-10-17 21:24:57.804395 | Idx:   80/400    | loss: 0.592 | grad_norm: 0.067 | acc: 87.92% |
| 2025-10-17 21:25:00.518323 | Idx:  120/400    | loss: 0.535 | grad_norm: 0.095 | acc: 88.19% |
| 2025-10-17 21:25:01.672230 | Idx:  160/400    | loss: 0.496 | grad_norm: 0.331 | acc: 88.91% |
| 2025-10-17 21:25:02.844325 | Idx:  200/400    | loss: 0.464 | grad_norm: 0.133 | acc: 88.59% |
| 2025-10-17 21:25:05.765197 | Idx:  240/400    | loss: 0.445 | grad_norm: 0.277 | acc: 89.70% |
| 2025-10-17 21:25:06.980425 | Idx:  280/400    | loss: 0.430 | grad_norm: 0.281 | acc: 86.28% |
| 2025-10-17 21:25:09.769903 | Idx:  320/400    | loss: 0.418 | grad_norm: 0.179 | acc: 92.16% |
| 2025-10-17 21:25:10.943542 | Idx:  360/400    | loss: 0.408 | grad_norm: 0.143 | acc: 94.37% |
| 2025-10-17 21:25:12.117378 | Idx:  400/400    | loss: 0.397 | grad_norm: 0.465 | acc: 91.99% |
Train Loss: 0.3966 Acc: 89.27%
Acc: 90.44%


| val_epoch_acc: 90.44% | epoch: 00 | avg_train_loss: 0.3088 | avg_train_acc: 91.7516 | 


Epoch 2/8
----------
| 2025-10-17 21:25:36.548579 | Idx:   40/400    | loss: 0.305 | grad_norm: 0.357 | acc: 93.29% |
| 2025-10-17 21:25:37.714696 | Idx:   80/400    | loss: 0.302 | grad_norm: 0.196 | acc: 93.76% |
| 2025-10-17 21:25:40.328141 | Idx:  120/400    | loss: 0.302 | grad_norm: 0.566 | acc: 92.47% |
| 2025-10-17 21:25:41.490889 | Idx:  160/400    | loss: 0.300 | grad_norm: 0.258 | acc: 95.11% |
| 2025-10-17 21:25:42.660321 | Idx:  200/400    | loss: 0.304 | grad_norm: 0.316 | acc: 93.52% |
| 2025-10-17 21:25:45.268056 | Idx:  240/400    | loss: 0.304 | grad_norm: 0.363 | acc: 88.54% |
| 2025-10-17 21:25:46.425353 | Idx:  280/400    | loss: 0.304 | grad_norm: 0.294 | acc: 93.48% |
| 2025-10-17 21:25:49.052876 | Idx:  320/400    | loss: 0.307 | grad_norm: 0.300 | acc: 93.34% |
| 2025-10-17 21:25:50.207954 | Idx:  360/400    | loss: 0.306 | grad_norm: 0.166 | acc: 95.04% |
| 2025-10-17 21:25:51.353007 | Idx:  400/400    | loss: 0.304 | grad_norm: 0.269 | acc: 94.98% |
Train Loss: 0.3044 Acc: 91.86%
Acc: 92.68%


| val_epoch_acc: 92.68% | epoch: 01 | avg_train_loss: 0.2973 | avg_train_acc: 92.0822 | 


Epoch 3/8
----------
| 2025-10-17 21:26:15.532059 | Idx:   40/400    | loss: 0.317 | grad_norm: 0.480 | acc: 92.02% |
| 2025-10-17 21:26:16.685888 | Idx:   80/400    | loss: 0.302 | grad_norm: 0.445 | acc: 94.06% |
| 2025-10-17 21:26:19.319814 | Idx:  120/400    | loss: 0.302 | grad_norm: 0.228 | acc: 90.44% |
| 2025-10-17 21:26:20.522897 | Idx:  160/400    | loss: 0.302 | grad_norm: 0.367 | acc: 94.98% |
| 2025-10-17 21:26:21.705989 | Idx:  200/400    | loss: 0.298 | grad_norm: 0.579 | acc: 90.18% |
| 2025-10-17 21:26:24.356174 | Idx:  240/400    | loss: 0.300 | grad_norm: 0.240 | acc: 93.40% |
| 2025-10-17 21:26:25.515416 | Idx:  280/400    | loss: 0.299 | grad_norm: 0.212 | acc: 93.97% |
| 2025-10-17 21:26:28.168358 | Idx:  320/400    | loss: 0.297 | grad_norm: 0.095 | acc: 90.20% |
| 2025-10-17 21:26:29.353474 | Idx:  360/400    | loss: 0.295 | grad_norm: 0.326 | acc: 94.68% |
| 2025-10-17 21:26:30.507651 | Idx:  400/400    | loss: 0.295 | grad_norm: 0.445 | acc: 94.19% |
Train Loss: 0.2948 Acc: 92.20%
Acc: 92.11%


| val_epoch_acc: 92.11% | epoch: 02 | avg_train_loss: 0.2836 | avg_train_acc: 92.4863 | 


Epoch 4/8
----------
| 2025-10-17 21:26:55.645384 | Idx:   40/400    | loss: 0.286 | grad_norm: 0.244 | acc: 94.58% |
| 2025-10-17 21:26:56.795769 | Idx:   80/400    | loss: 0.298 | grad_norm: 0.385 | acc: 89.86% |
| 2025-10-17 21:26:59.381825 | Idx:  120/400    | loss: 0.297 | grad_norm: 0.367 | acc: 92.15% |
| 2025-10-17 21:27:00.529575 | Idx:  160/400    | loss: 0.294 | grad_norm: 0.736 | acc: 87.59% |
| 2025-10-17 21:27:01.683225 | Idx:  200/400    | loss: 0.297 | grad_norm: 0.262 | acc: 89.00% |
| 2025-10-17 21:27:04.274301 | Idx:  240/400    | loss: 0.296 | grad_norm: 0.629 | acc: 89.78% |
| 2025-10-17 21:27:05.407781 | Idx:  280/400    | loss: 0.298 | grad_norm: 0.145 | acc: 92.81% |
| 2025-10-17 21:27:07.943831 | Idx:  320/400    | loss: 0.297 | grad_norm: 0.343 | acc: 95.62% |
| 2025-10-17 21:27:09.083293 | Idx:  360/400    | loss: 0.298 | grad_norm: 0.770 | acc: 89.23% |
| 2025-10-17 21:27:10.221165 | Idx:  400/400    | loss: 0.296 | grad_norm: 0.419 | acc: 91.44% |
Train Loss: 0.2957 Acc: 92.16%
Acc: 91.54%


| val_epoch_acc: 91.54% | epoch: 03 | avg_train_loss: 0.2809 | avg_train_acc: 92.5716 | 


Epoch 5/8
----------
| 2025-10-17 21:27:33.289027 | Idx:   40/400    | loss: 0.290 | grad_norm: 0.379 | acc: 95.08% |
| 2025-10-17 21:27:34.454739 | Idx:   80/400    | loss: 0.286 | grad_norm: 0.158 | acc: 93.27% |
| 2025-10-17 21:27:37.064378 | Idx:  120/400    | loss: 0.284 | grad_norm: 0.239 | acc: 95.92% |
| 2025-10-17 21:27:38.195499 | Idx:  160/400    | loss: 0.286 | grad_norm: 0.354 | acc: 92.60% |
| 2025-10-17 21:27:39.367114 | Idx:  200/400    | loss: 0.290 | grad_norm: 0.256 | acc: 90.66% |
| 2025-10-17 21:27:42.221375 | Idx:  240/400    | loss: 0.291 | grad_norm: 0.561 | acc: 93.48% |
| 2025-10-17 21:27:43.358962 | Idx:  280/400    | loss: 0.292 | grad_norm: 0.122 | acc: 92.68% |
| 2025-10-17 21:27:45.934475 | Idx:  320/400    | loss: 0.292 | grad_norm: 0.196 | acc: 93.52% |
| 2025-10-17 21:27:47.069267 | Idx:  360/400    | loss: 0.290 | grad_norm: 0.205 | acc: 96.09% |
| 2025-10-17 21:27:48.183409 | Idx:  400/400    | loss: 0.290 | grad_norm: 0.297 | acc: 92.93% |
Train Loss: 0.2897 Acc: 92.33%
Acc: 92.21%


| val_epoch_acc: 92.21% | epoch: 04 | avg_train_loss: 0.2888 | avg_train_acc: 92.3280 | 


Epoch 6/8
----------
| 2025-10-17 21:28:11.256105 | Idx:   40/400    | loss: 0.303 | grad_norm: 0.109 | acc: 92.74% |
| 2025-10-17 21:28:12.395857 | Idx:   80/400    | loss: 0.289 | grad_norm: 0.559 | acc: 90.84% |
| 2025-10-17 21:28:14.915464 | Idx:  120/400    | loss: 0.280 | grad_norm: 0.278 | acc: 90.87% |
| 2025-10-17 21:28:16.046460 | Idx:  160/400    | loss: 0.283 | grad_norm: 0.314 | acc: 89.24% |
| 2025-10-17 21:28:17.190092 | Idx:  200/400    | loss: 0.285 | grad_norm: 1.191 | acc: 87.85% |
| 2025-10-17 21:28:19.888645 | Idx:  240/400    | loss: 0.288 | grad_norm: 0.474 | acc: 89.18% |
| 2025-10-17 21:28:21.017376 | Idx:  280/400    | loss: 0.290 | grad_norm: 0.176 | acc: 93.01% |
| 2025-10-17 21:28:23.957025 | Idx:  320/400    | loss: 0.289 | grad_norm: 0.507 | acc: 93.52% |
| 2025-10-17 21:28:25.099299 | Idx:  360/400    | loss: 0.289 | grad_norm: 0.166 | acc: 94.09% |
| 2025-10-17 21:28:26.230976 | Idx:  400/400    | loss: 0.289 | grad_norm: 0.225 | acc: 91.78% |
Train Loss: 0.2895 Acc: 92.34%
Acc: 91.85%


| val_epoch_acc: 91.85% | epoch: 05 | avg_train_loss: 0.2997 | avg_train_acc: 91.9579 | 


Epoch 7/8
----------
| 2025-10-17 21:28:49.798464 | Idx:   40/400    | loss: 0.306 | grad_norm: 0.977 | acc: 85.71% |
| 2025-10-17 21:28:50.969514 | Idx:   80/400    | loss: 0.300 | grad_norm: 0.114 | acc: 92.01% |
| 2025-10-17 21:28:53.727803 | Idx:  120/400    | loss: 0.299 | grad_norm: 0.199 | acc: 91.73% |
| 2025-10-17 21:28:54.954405 | Idx:  160/400    | loss: 0.299 | grad_norm: 0.400 | acc: 94.33% |
| 2025-10-17 21:28:56.170349 | Idx:  200/400    | loss: 0.299 | grad_norm: 0.168 | acc: 90.83% |
| 2025-10-17 21:28:58.866896 | Idx:  240/400    | loss: 0.295 | grad_norm: 0.334 | acc: 95.83% |
| 2025-10-17 21:29:00.087317 | Idx:  280/400    | loss: 0.293 | grad_norm: 0.114 | acc: 94.62% |
| 2025-10-17 21:29:02.769020 | Idx:  320/400    | loss: 0.292 | grad_norm: 0.466 | acc: 86.83% |
| 2025-10-17 21:29:03.987309 | Idx:  360/400    | loss: 0.295 | grad_norm: 0.276 | acc: 90.23% |
| 2025-10-17 21:29:05.212937 | Idx:  400/400    | loss: 0.294 | grad_norm: 0.312 | acc: 92.60% |
Train Loss: 0.2942 Acc: 92.20%
Acc: 91.09%


| val_epoch_acc: 91.09% | epoch: 06 | avg_train_loss: 0.2946 | avg_train_acc: 92.1564 | 


Epoch 8/8
----------
| 2025-10-17 21:29:30.248544 | Idx:   40/400    | loss: 0.305 | grad_norm: 2.915 | acc: 76.05% |
| 2025-10-17 21:29:31.460293 | Idx:   80/400    | loss: 0.305 | grad_norm: 0.351 | acc: 94.03% |
| 2025-10-17 21:29:34.284630 | Idx:  120/400    | loss: 0.295 | grad_norm: 0.358 | acc: 93.45% |
| 2025-10-17 21:29:35.507701 | Idx:  160/400    | loss: 0.298 | grad_norm: 0.492 | acc: 90.39% |
| 2025-10-17 21:29:36.757399 | Idx:  200/400    | loss: 0.292 | grad_norm: 0.450 | acc: 94.74% |
| 2025-10-17 21:29:39.492737 | Idx:  240/400    | loss: 0.291 | grad_norm: 0.539 | acc: 91.34% |
| 2025-10-17 21:29:40.661565 | Idx:  280/400    | loss: 0.292 | grad_norm: 0.231 | acc: 91.14% |
| 2025-10-17 21:29:43.302151 | Idx:  320/400    | loss: 0.292 | grad_norm: 0.169 | acc: 93.88% |
| 2025-10-17 21:29:44.487078 | Idx:  360/400    | loss: 0.290 | grad_norm: 0.471 | acc: 87.95% |
| 2025-10-17 21:29:45.653886 | Idx:  400/400    | loss: 0.293 | grad_norm: 0.192 | acc: 93.64% |
Train Loss: 0.2929 Acc: 92.27%
Acc: 92.30%


| val_epoch_acc: 92.30% | epoch: 07 | avg_train_loss: 0.3006 | avg_train_acc: 92.0213 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_212442-y8oyo1r3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_212442-y8oyo1r3\logs[0m
rule: B2/S013V \t, network: tiny_2_layer_seq_cnn \n\n
./predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml

{'info': 'Baseline model with default hyperparameters', 'wandb': {'turn_on': True, 'entity': 'tiny_2_layer_seq_cnn'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 50, 'step_size_down': 10}}, 'model': {'name': 'SimpleCNNTiny'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/tiny_2_layer_seq_cnn.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_21-30-08_tiny_2_layer_seq_cnn__200-200-B2_S013V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNNTiny                            [1, 2, 200, 200]          --
©À©¤Conv2d: 1-1                            [1, 1, 200, 200]          51
©À©¤BatchNorm2d: 1-2                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-3                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-4                            [1, 1, 200, 200]          26
©À©¤BatchNorm2d: 1-5                       [1, 1, 200, 200]          2
©À©¤ReLU: 1-6                              [1, 1, 200, 200]          --
©À©¤Conv2d: 1-7                            [1, 2, 200, 200]          52
==========================================================================================
Total params: 133
Trainable params: 133
Non-trainable params: 0
Total mult-adds (M): 5.16
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 1.92
Params size (MB): 0.00
Estimated Total Size (MB): 2.24
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:30:23.627046 | Idx:   40/400    | loss: 1.207 | grad_norm: 0.232 | acc: 73.74% |
| 2025-10-17 21:30:24.842242 | Idx:   80/400    | loss: 1.040 | grad_norm: 0.221 | acc: 76.99% |
| 2025-10-17 21:30:27.500901 | Idx:  120/400    | loss: 0.981 | grad_norm: 0.714 | acc: 75.90% |
| 2025-10-17 21:30:28.754282 | Idx:  160/400    | loss: 0.942 | grad_norm: 0.225 | acc: 75.50% |
| 2025-10-17 21:30:29.997711 | Idx:  200/400    | loss: 0.917 | grad_norm: 0.181 | acc: 77.97% |
| 2025-10-17 21:30:32.905548 | Idx:  240/400    | loss: 0.899 | grad_norm: 0.240 | acc: 76.92% |
| 2025-10-17 21:30:34.096769 | Idx:  280/400    | loss: 0.877 | grad_norm: 0.179 | acc: 79.86% |
| 2025-10-17 21:30:36.800667 | Idx:  320/400    | loss: 0.855 | grad_norm: 0.833 | acc: 78.09% |
| 2025-10-17 21:30:37.977157 | Idx:  360/400    | loss: 0.829 | grad_norm: 0.155 | acc: 85.34% |
| 2025-10-17 21:30:39.141264 | Idx:  400/400    | loss: 0.808 | grad_norm: 0.367 | acc: 80.85% |
Train Loss: 0.8082 Acc: 77.12%
Acc: 84.65%


| val_epoch_acc: 84.65% | epoch: 00 | avg_train_loss: 0.6347 | avg_train_acc: 81.7160 | 


Epoch 2/8
----------
| 2025-10-17 21:31:04.079201 | Idx:   40/400    | loss: 0.619 | grad_norm: 0.129 | acc: 80.90% |
| 2025-10-17 21:31:05.331071 | Idx:   80/400    | loss: 0.621 | grad_norm: 0.220 | acc: 83.02% |
| 2025-10-17 21:31:08.102819 | Idx:  120/400    | loss: 0.616 | grad_norm: 0.154 | acc: 83.42% |
| 2025-10-17 21:31:09.326739 | Idx:  160/400    | loss: 0.615 | grad_norm: 0.222 | acc: 82.27% |
| 2025-10-17 21:31:10.536492 | Idx:  200/400    | loss: 0.614 | grad_norm: 0.215 | acc: 84.90% |
| 2025-10-17 21:31:13.173151 | Idx:  240/400    | loss: 0.610 | grad_norm: 0.189 | acc: 79.93% |
| 2025-10-17 21:31:14.369504 | Idx:  280/400    | loss: 0.607 | grad_norm: 0.066 | acc: 82.22% |
| 2025-10-17 21:31:17.131878 | Idx:  320/400    | loss: 0.607 | grad_norm: 0.282 | acc: 79.12% |
| 2025-10-17 21:31:18.312087 | Idx:  360/400    | loss: 0.608 | grad_norm: 0.136 | acc: 81.23% |
| 2025-10-17 21:31:19.485265 | Idx:  400/400    | loss: 0.608 | grad_norm: 0.288 | acc: 85.77% |
Train Loss: 0.6084 Acc: 82.10%
Acc: 84.40%


| val_epoch_acc: 84.40% | epoch: 01 | avg_train_loss: 0.6013 | avg_train_acc: 82.3384 | 


Epoch 3/8
----------
| 2025-10-17 21:31:43.345420 | Idx:   40/400    | loss: 0.605 | grad_norm: 0.198 | acc: 83.57% |
| 2025-10-17 21:31:44.547473 | Idx:   80/400    | loss: 0.603 | grad_norm: 0.498 | acc: 79.68% |
| 2025-10-17 21:31:47.167381 | Idx:  120/400    | loss: 0.611 | grad_norm: 0.152 | acc: 82.76% |
| 2025-10-17 21:31:48.340982 | Idx:  160/400    | loss: 0.608 | grad_norm: 0.244 | acc: 79.84% |
| 2025-10-17 21:31:49.529934 | Idx:  200/400    | loss: 0.606 | grad_norm: 0.146 | acc: 83.43% |
| 2025-10-17 21:31:52.284120 | Idx:  240/400    | loss: 0.606 | grad_norm: 0.246 | acc: 81.79% |
| 2025-10-17 21:31:53.460464 | Idx:  280/400    | loss: 0.607 | grad_norm: 0.055 | acc: 81.33% |
| 2025-10-17 21:31:56.205456 | Idx:  320/400    | loss: 0.607 | grad_norm: 0.075 | acc: 82.38% |
| 2025-10-17 21:31:57.419659 | Idx:  360/400    | loss: 0.606 | grad_norm: 0.224 | acc: 83.18% |
| 2025-10-17 21:31:58.599572 | Idx:  400/400    | loss: 0.604 | grad_norm: 0.084 | acc: 82.13% |
Train Loss: 0.6040 Acc: 82.10%
Acc: 81.87%


| val_epoch_acc: 81.87% | epoch: 02 | avg_train_loss: 0.5839 | avg_train_acc: 82.6614 | 


Epoch 4/8
----------
| 2025-10-17 21:32:23.608073 | Idx:   40/400    | loss: 0.586 | grad_norm: 0.313 | acc: 83.49% |
| 2025-10-17 21:32:24.808126 | Idx:   80/400    | loss: 0.589 | grad_norm: 0.172 | acc: 81.22% |
| 2025-10-17 21:32:27.560401 | Idx:  120/400    | loss: 0.594 | grad_norm: 0.277 | acc: 82.28% |
| 2025-10-17 21:32:28.791094 | Idx:  160/400    | loss: 0.598 | grad_norm: 0.238 | acc: 82.66% |
| 2025-10-17 21:32:30.015284 | Idx:  200/400    | loss: 0.603 | grad_norm: 0.066 | acc: 81.98% |
| 2025-10-17 21:32:32.733322 | Idx:  240/400    | loss: 0.602 | grad_norm: 0.390 | acc: 85.04% |
| 2025-10-17 21:32:33.926093 | Idx:  280/400    | loss: 0.601 | grad_norm: 0.355 | acc: 79.64% |
| 2025-10-17 21:32:36.687504 | Idx:  320/400    | loss: 0.602 | grad_norm: 0.096 | acc: 83.01% |
| 2025-10-17 21:32:37.861822 | Idx:  360/400    | loss: 0.604 | grad_norm: 0.563 | acc: 77.37% |
| 2025-10-17 21:32:39.010813 | Idx:  400/400    | loss: 0.607 | grad_norm: 0.341 | acc: 77.86% |
Train Loss: 0.6074 Acc: 81.94%
Acc: 27.31%


| val_epoch_acc: 27.31% | epoch: 03 | avg_train_loss: 0.6398 | avg_train_acc: 80.9260 | 


Epoch 5/8
----------
| 2025-10-17 21:33:03.161618 | Idx:   40/400    | loss: 0.601 | grad_norm: 0.101 | acc: 83.72% |
| 2025-10-17 21:33:04.397234 | Idx:   80/400    | loss: 0.608 | grad_norm: 0.347 | acc: 80.32% |
| 2025-10-17 21:33:07.363407 | Idx:  120/400    | loss: 0.613 | grad_norm: 0.172 | acc: 82.16% |
| 2025-10-17 21:33:08.584044 | Idx:  160/400    | loss: 0.615 | grad_norm: 0.103 | acc: 83.85% |
| 2025-10-17 21:33:09.872898 | Idx:  200/400    | loss: 0.608 | grad_norm: 0.383 | acc: 82.60% |
| 2025-10-17 21:33:12.594328 | Idx:  240/400    | loss: 0.609 | grad_norm: 0.315 | acc: 85.67% |
| 2025-10-17 21:33:13.778671 | Idx:  280/400    | loss: 0.611 | grad_norm: 0.228 | acc: 80.52% |
| 2025-10-17 21:33:16.456793 | Idx:  320/400    | loss: 0.610 | grad_norm: 0.175 | acc: 85.23% |
| 2025-10-17 21:33:17.694560 | Idx:  360/400    | loss: 0.607 | grad_norm: 0.287 | acc: 81.11% |
| 2025-10-17 21:33:18.890667 | Idx:  400/400    | loss: 0.608 | grad_norm: 0.087 | acc: 81.13% |
Train Loss: 0.6081 Acc: 81.89%
Acc: 63.82%


| val_epoch_acc: 63.82% | epoch: 04 | avg_train_loss: 0.6071 | avg_train_acc: 82.0802 | 


Epoch 6/8
----------
| 2025-10-17 21:33:43.483659 | Idx:   40/400    | loss: 0.581 | grad_norm: 0.055 | acc: 83.89% |
| 2025-10-17 21:33:44.682276 | Idx:   80/400    | loss: 0.605 | grad_norm: 0.224 | acc: 84.39% |
| 2025-10-17 21:33:47.408236 | Idx:  120/400    | loss: 0.608 | grad_norm: 0.123 | acc: 83.29% |
| 2025-10-17 21:33:48.591535 | Idx:  160/400    | loss: 0.607 | grad_norm: 0.106 | acc: 80.77% |
| 2025-10-17 21:33:49.840021 | Idx:  200/400    | loss: 0.609 | grad_norm: 0.274 | acc: 84.30% |
| 2025-10-17 21:33:52.570203 | Idx:  240/400    | loss: 0.608 | grad_norm: 0.151 | acc: 81.88% |
| 2025-10-17 21:33:53.763173 | Idx:  280/400    | loss: 0.605 | grad_norm: 0.085 | acc: 81.35% |
| 2025-10-17 21:33:56.918548 | Idx:  320/400    | loss: 0.605 | grad_norm: 0.076 | acc: 80.56% |
| 2025-10-17 21:33:58.104583 | Idx:  360/400    | loss: 0.605 | grad_norm: 0.422 | acc: 78.99% |
| 2025-10-17 21:33:59.274969 | Idx:  400/400    | loss: 0.606 | grad_norm: 0.129 | acc: 80.64% |
Train Loss: 0.6057 Acc: 82.02%
Acc: 84.45%


| val_epoch_acc: 84.45% | epoch: 05 | avg_train_loss: 0.6195 | avg_train_acc: 81.4988 | 


Epoch 7/8
----------
| 2025-10-17 21:34:22.845549 | Idx:   40/400    | loss: 0.620 | grad_norm: 0.576 | acc: 78.97% |
| 2025-10-17 21:34:24.089661 | Idx:   80/400    | loss: 0.608 | grad_norm: 0.156 | acc: 84.67% |
| 2025-10-17 21:34:26.906534 | Idx:  120/400    | loss: 0.616 | grad_norm: 0.369 | acc: 80.75% |
| 2025-10-17 21:34:28.100447 | Idx:  160/400    | loss: 0.616 | grad_norm: 0.213 | acc: 79.09% |
| 2025-10-17 21:34:29.342461 | Idx:  200/400    | loss: 0.613 | grad_norm: 0.171 | acc: 80.91% |
| 2025-10-17 21:34:32.160151 | Idx:  240/400    | loss: 0.608 | grad_norm: 0.174 | acc: 82.21% |
| 2025-10-17 21:34:33.350606 | Idx:  280/400    | loss: 0.607 | grad_norm: 0.214 | acc: 82.01% |
| 2025-10-17 21:34:36.122534 | Idx:  320/400    | loss: 0.606 | grad_norm: 0.449 | acc: 79.94% |
| 2025-10-17 21:34:37.350565 | Idx:  360/400    | loss: 0.607 | grad_norm: 0.081 | acc: 81.06% |
| 2025-10-17 21:34:38.539767 | Idx:  400/400    | loss: 0.605 | grad_norm: 0.353 | acc: 85.65% |
Train Loss: 0.6054 Acc: 81.99%
Acc: 72.73%


| val_epoch_acc: 72.73% | epoch: 06 | avg_train_loss: 0.5955 | avg_train_acc: 82.3493 | 


Epoch 8/8
----------
| 2025-10-17 21:35:04.698356 | Idx:   40/400    | loss: 0.612 | grad_norm: 0.293 | acc: 79.36% |
| 2025-10-17 21:35:05.915850 | Idx:   80/400    | loss: 0.612 | grad_norm: 1.122 | acc: 75.49% |
| 2025-10-17 21:35:08.741221 | Idx:  120/400    | loss: 0.604 | grad_norm: 0.148 | acc: 84.57% |
| 2025-10-17 21:35:09.994909 | Idx:  160/400    | loss: 0.603 | grad_norm: 0.087 | acc: 81.56% |
| 2025-10-17 21:35:11.208065 | Idx:  200/400    | loss: 0.603 | grad_norm: 0.093 | acc: 82.70% |
| 2025-10-17 21:35:14.087891 | Idx:  240/400    | loss: 0.604 | grad_norm: 0.347 | acc: 84.23% |
| 2025-10-17 21:35:15.316884 | Idx:  280/400    | loss: 0.608 | grad_norm: 0.087 | acc: 82.75% |
| 2025-10-17 21:35:18.197697 | Idx:  320/400    | loss: 0.609 | grad_norm: 0.095 | acc: 80.97% |
| 2025-10-17 21:35:19.460177 | Idx:  360/400    | loss: 0.608 | grad_norm: 0.131 | acc: 82.57% |
| 2025-10-17 21:35:20.655340 | Idx:  400/400    | loss: 0.607 | grad_norm: 0.219 | acc: 84.39% |
Train Loss: 0.6072 Acc: 81.95%
Acc: 77.24%


| val_epoch_acc: 77.24% | epoch: 07 | avg_train_loss: 0.6000 | avg_train_acc: 82.0789 | 


[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_213008-bx5ys9h0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_213008-bx5ys9h0\logs[0m
rule: B2/S \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_21-35-42_multiscale_0__200-200-B2_S


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
©À©¤Sequential: 1-1                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-1                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-3                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-2                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-4                       [1, 2, 200, 200]          102
©¦    ©¸©¤BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-6                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-3                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-7                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-9                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-4                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-10                      [1, 4, 200, 200]          604
©¦    ©¸©¤BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
©¦    ©¸©¤ReLU: 2-12                        [1, 4, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:35:59.788034 | Idx:   40/400    | loss: 1.104 | grad_norm: 3.393 | acc: 84.48% |
| 2025-10-17 21:36:02.880706 | Idx:   80/400    | loss: 0.796 | grad_norm: 0.363 | acc: 91.54% |
| 2025-10-17 21:36:07.491136 | Idx:  120/400    | loss: 0.607 | grad_norm: 0.631 | acc: 97.09% |
| 2025-10-17 21:36:10.639041 | Idx:  160/400    | loss: 0.493 | grad_norm: 2.171 | acc: 98.13% |
| 2025-10-17 21:36:13.755510 | Idx:  200/400    | loss: 0.410 | grad_norm: 0.062 | acc: 99.20% |
| 2025-10-17 21:36:18.521289 | Idx:  240/400    | loss: 0.349 | grad_norm: 0.065 | acc: 99.84% |
| 2025-10-17 21:36:21.672722 | Idx:  280/400    | loss: 0.302 | grad_norm: 0.090 | acc: 99.91% |
| 2025-10-17 21:36:26.196626 | Idx:  320/400    | loss: 0.266 | grad_norm: 0.491 | acc: 99.16% |
| 2025-10-17 21:36:29.285550 | Idx:  360/400    | loss: 0.246 | grad_norm: 0.075 | acc: 99.88% |
| 2025-10-17 21:36:32.405944 | Idx:  400/400    | loss: 0.222 | grad_norm: 0.859 | acc: 98.80% |
Train Loss: 0.2225 Acc: 94.34%
Acc: 98.10%


| val_epoch_acc: 98.10% | epoch: 00 | avg_train_loss: 0.0124 | avg_train_acc: 99.8845 | 


Epoch 2/8
----------
| 2025-10-17 21:37:01.232065 | Idx:   40/400    | loss: 0.011 | grad_norm: 0.028 | acc: 100.00% |
| 2025-10-17 21:37:04.376774 | Idx:   80/400    | loss: 0.008 | grad_norm: 0.034 | acc: 99.98% |
| 2025-10-17 21:37:09.035883 | Idx:  120/400    | loss: 0.007 | grad_norm: 0.162 | acc: 99.99% |
| 2025-10-17 21:37:12.160462 | Idx:  160/400    | loss: 0.006 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-17 21:37:15.415008 | Idx:  200/400    | loss: 0.006 | grad_norm: 0.007 | acc: 99.99% |
| 2025-10-17 21:37:20.034397 | Idx:  240/400    | loss: 0.005 | grad_norm: 0.006 | acc: 99.99% |
| 2025-10-17 21:37:23.187576 | Idx:  280/400    | loss: 0.005 | grad_norm: 0.038 | acc: 99.98% |
| 2025-10-17 21:37:28.170467 | Idx:  320/400    | loss: 0.005 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:37:31.249607 | Idx:  360/400    | loss: 0.005 | grad_norm: 0.018 | acc: 99.99% |
| 2025-10-17 21:37:34.348015 | Idx:  400/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0045 Acc: 99.98%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0036 | avg_train_acc: 99.9916 | 


Epoch 3/8
----------
| 2025-10-17 21:38:02.254619 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:38:05.395265 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:38:09.995305 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:38:13.132737 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 21:38:16.246105 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 21:38:20.922245 | Idx:  240/400    | loss: 0.010 | grad_norm: 2.989 | acc: 92.98% |
| 2025-10-17 21:38:24.032098 | Idx:  280/400    | loss: 0.018 | grad_norm: 0.047 | acc: 99.99% |
| 2025-10-17 21:38:28.848304 | Idx:  320/400    | loss: 0.016 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:38:31.986072 | Idx:  360/400    | loss: 0.015 | grad_norm: 0.018 | acc: 100.00% |
| 2025-10-17 21:38:35.119820 | Idx:  400/400    | loss: 0.016 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0161 Acc: 99.82%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0351 | avg_train_acc: 99.7522 | 


Epoch 4/8
----------
| 2025-10-17 21:39:03.821816 | Idx:   40/400    | loss: 0.026 | grad_norm: 0.873 | acc: 98.66% |
| 2025-10-17 21:39:06.955620 | Idx:   80/400    | loss: 0.033 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:39:11.572703 | Idx:  120/400    | loss: 0.023 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:39:14.719362 | Idx:  160/400    | loss: 0.018 | grad_norm: 0.014 | acc: 99.99% |
| 2025-10-17 21:39:17.871223 | Idx:  200/400    | loss: 0.015 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:39:22.496163 | Idx:  240/400    | loss: 0.013 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:39:25.657479 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:39:30.295274 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:39:33.418896 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:39:36.503615 | Idx:  400/400    | loss: 0.009 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0091 Acc: 99.92%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0027 | avg_train_acc: 99.9997 | 


Epoch 5/8
----------
| 2025-10-17 21:40:05.767206 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:40:08.902143 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:40:13.461911 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 21:40:16.563285 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.031 | acc: 99.99% |
| 2025-10-17 21:40:19.662114 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 21:40:24.204132 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.000 | acc: 100.00% |
| 2025-10-17 21:40:27.281071 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:40:31.727811 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:40:34.862523 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:40:37.965647 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.000 | acc: 100.00% |
Train Loss: 0.0028 Acc: 100.00%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_213543-fgn75e3b[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_213543-fgn75e3b\logs[0m
rule: B345/S5 \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_21-41-03_multiscale_0__200-200-B345_S5


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
©À©¤Sequential: 1-1                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-1                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-3                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-2                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-4                       [1, 2, 200, 200]          102
©¦    ©¸©¤BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-6                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-3                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-7                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-9                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-4                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-10                      [1, 4, 200, 200]          604
©¦    ©¸©¤BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
©¦    ©¸©¤ReLU: 2-12                        [1, 4, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:41:20.519871 | Idx:   40/400    | loss: 0.809 | grad_norm: 0.548 | acc: 89.26% |
| 2025-10-17 21:41:23.675606 | Idx:   80/400    | loss: 0.617 | grad_norm: 1.740 | acc: 93.94% |
| 2025-10-17 21:41:28.290054 | Idx:  120/400    | loss: 0.470 | grad_norm: 0.144 | acc: 98.20% |
| 2025-10-17 21:41:31.466907 | Idx:  160/400    | loss: 0.399 | grad_norm: 0.978 | acc: 95.74% |
| 2025-10-17 21:41:34.566583 | Idx:  200/400    | loss: 0.339 | grad_norm: 0.570 | acc: 98.76% |
| 2025-10-17 21:41:39.398924 | Idx:  240/400    | loss: 0.296 | grad_norm: 0.426 | acc: 99.36% |
| 2025-10-17 21:41:42.548968 | Idx:  280/400    | loss: 0.266 | grad_norm: 0.381 | acc: 98.86% |
| 2025-10-17 21:41:47.195123 | Idx:  320/400    | loss: 0.239 | grad_norm: 0.885 | acc: 99.37% |
| 2025-10-17 21:41:50.296050 | Idx:  360/400    | loss: 0.222 | grad_norm: 0.173 | acc: 99.69% |
| 2025-10-17 21:41:53.364589 | Idx:  400/400    | loss: 0.202 | grad_norm: 0.524 | acc: 99.13% |
Train Loss: 0.2022 Acc: 95.74%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 00 | avg_train_loss: 0.0258 | avg_train_acc: 99.5686 | 


Epoch 2/8
----------
| 2025-10-17 21:42:21.634565 | Idx:   40/400    | loss: 0.047 | grad_norm: 0.036 | acc: 99.97% |
| 2025-10-17 21:42:24.789133 | Idx:   80/400    | loss: 0.032 | grad_norm: 0.138 | acc: 99.92% |
| 2025-10-17 21:42:29.307771 | Idx:  120/400    | loss: 0.036 | grad_norm: 1.832 | acc: 94.68% |
| 2025-10-17 21:42:32.414149 | Idx:  160/400    | loss: 0.040 | grad_norm: 0.059 | acc: 99.96% |
| 2025-10-17 21:42:35.487477 | Idx:  200/400    | loss: 0.042 | grad_norm: 0.224 | acc: 99.91% |
| 2025-10-17 21:42:40.097210 | Idx:  240/400    | loss: 0.045 | grad_norm: 0.110 | acc: 99.95% |
| 2025-10-17 21:42:43.248822 | Idx:  280/400    | loss: 0.041 | grad_norm: 0.015 | acc: 99.99% |
| 2025-10-17 21:42:48.116387 | Idx:  320/400    | loss: 0.040 | grad_norm: 0.111 | acc: 99.99% |
| 2025-10-17 21:42:51.255789 | Idx:  360/400    | loss: 0.038 | grad_norm: 0.035 | acc: 99.96% |
| 2025-10-17 21:42:54.386876 | Idx:  400/400    | loss: 0.036 | grad_norm: 0.033 | acc: 99.98% |
Train Loss: 0.0362 Acc: 99.43%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 01 | avg_train_loss: 0.0271 | avg_train_acc: 99.6905 | 


Epoch 3/8
----------
| 2025-10-17 21:43:22.868361 | Idx:   40/400    | loss: 0.018 | grad_norm: 0.601 | acc: 99.79% |
| 2025-10-17 21:43:25.968986 | Idx:   80/400    | loss: 0.014 | grad_norm: 0.138 | acc: 99.91% |
| 2025-10-17 21:43:30.644821 | Idx:  120/400    | loss: 0.013 | grad_norm: 0.063 | acc: 99.97% |
| 2025-10-17 21:43:33.750920 | Idx:  160/400    | loss: 0.018 | grad_norm: 2.477 | acc: 94.75% |
| 2025-10-17 21:43:36.911039 | Idx:  200/400    | loss: 0.025 | grad_norm: 0.110 | acc: 99.96% |
| 2025-10-17 21:43:41.555635 | Idx:  240/400    | loss: 0.027 | grad_norm: 2.208 | acc: 95.63% |
| 2025-10-17 21:43:44.655722 | Idx:  280/400    | loss: 0.028 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:43:49.579206 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.052 | acc: 99.99% |
| 2025-10-17 21:43:52.720459 | Idx:  360/400    | loss: 0.030 | grad_norm: 0.868 | acc: 99.31% |
| 2025-10-17 21:43:55.809387 | Idx:  400/400    | loss: 0.028 | grad_norm: 0.008 | acc: 100.00% |
Train Loss: 0.0284 Acc: 99.57%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0058 | avg_train_acc: 99.9426 | 


Epoch 4/8
----------
| 2025-10-17 21:44:24.958968 | Idx:   40/400    | loss: 0.008 | grad_norm: 0.207 | acc: 99.89% |
| 2025-10-17 21:44:28.118059 | Idx:   80/400    | loss: 0.009 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 21:44:32.761172 | Idx:  120/400    | loss: 0.008 | grad_norm: 0.011 | acc: 99.99% |
| 2025-10-17 21:44:35.895278 | Idx:  160/400    | loss: 0.015 | grad_norm: 0.037 | acc: 99.99% |
| 2025-10-17 21:44:39.035169 | Idx:  200/400    | loss: 0.013 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:44:43.513659 | Idx:  240/400    | loss: 0.016 | grad_norm: 0.564 | acc: 99.72% |
| 2025-10-17 21:44:46.656874 | Idx:  280/400    | loss: 0.017 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 21:44:51.304148 | Idx:  320/400    | loss: 0.015 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 21:44:54.436792 | Idx:  360/400    | loss: 0.015 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 21:44:57.565544 | Idx:  400/400    | loss: 0.016 | grad_norm: 0.061 | acc: 99.95% |
Train Loss: 0.0162 Acc: 99.77%
Acc: 99.65%


| val_epoch_acc: 99.65% | epoch: 03 | avg_train_loss: 0.0299 | avg_train_acc: 99.4559 | 


Epoch 5/8
----------
| 2025-10-17 21:45:26.671034 | Idx:   40/400    | loss: 0.020 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:45:29.757536 | Idx:   80/400    | loss: 0.013 | grad_norm: 0.033 | acc: 99.99% |
| 2025-10-17 21:45:34.348234 | Idx:  120/400    | loss: 0.035 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 21:45:37.456242 | Idx:  160/400    | loss: 0.035 | grad_norm: 0.268 | acc: 99.80% |
| 2025-10-17 21:45:40.573866 | Idx:  200/400    | loss: 0.035 | grad_norm: 0.039 | acc: 99.96% |
| 2025-10-17 21:45:45.197642 | Idx:  240/400    | loss: 0.030 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 21:45:48.294621 | Idx:  280/400    | loss: 0.049 | grad_norm: 0.091 | acc: 99.96% |
| 2025-10-17 21:45:52.989378 | Idx:  320/400    | loss: 0.044 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:45:56.103801 | Idx:  360/400    | loss: 0.044 | grad_norm: 0.009 | acc: 99.99% |
| 2025-10-17 21:45:59.235918 | Idx:  400/400    | loss: 0.040 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0400 Acc: 99.53%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_214103-xgrz7yez[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_214103-xgrz7yez\logs[0m
rule: B13/S012V \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_21-46-23_multiscale_0__200-200-B13_S012V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
©À©¤Sequential: 1-1                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-1                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-3                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-2                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-4                       [1, 2, 200, 200]          102
©¦    ©¸©¤BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-6                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-3                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-7                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-9                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-4                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-10                      [1, 4, 200, 200]          604
©¦    ©¸©¤BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
©¦    ©¸©¤ReLU: 2-12                        [1, 4, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:46:39.862829 | Idx:   40/400    | loss: 0.575 | grad_norm: 0.127 | acc: 93.88% |
| 2025-10-17 21:46:42.975894 | Idx:   80/400    | loss: 0.464 | grad_norm: 0.230 | acc: 91.16% |
| 2025-10-17 21:46:47.497048 | Idx:  120/400    | loss: 0.417 | grad_norm: 0.263 | acc: 88.38% |
| 2025-10-17 21:46:50.643276 | Idx:  160/400    | loss: 0.355 | grad_norm: 0.231 | acc: 97.71% |
| 2025-10-17 21:46:53.754046 | Idx:  200/400    | loss: 0.296 | grad_norm: 0.020 | acc: 99.19% |
| 2025-10-17 21:46:58.524128 | Idx:  240/400    | loss: 0.259 | grad_norm: 2.475 | acc: 92.37% |
| 2025-10-17 21:47:01.639452 | Idx:  280/400    | loss: 0.243 | grad_norm: 0.054 | acc: 98.77% |
| 2025-10-17 21:47:06.347472 | Idx:  320/400    | loss: 0.221 | grad_norm: 0.040 | acc: 98.72% |
| 2025-10-17 21:47:09.466660 | Idx:  360/400    | loss: 0.202 | grad_norm: 0.016 | acc: 98.99% |
| 2025-10-17 21:47:12.559995 | Idx:  400/400    | loss: 0.187 | grad_norm: 0.036 | acc: 98.23% |
Train Loss: 0.1868 Acc: 95.43%
Acc: 98.98%


| val_epoch_acc: 98.98% | epoch: 00 | avg_train_loss: 0.0548 | avg_train_acc: 98.6743 | 


Epoch 2/8
----------
| 2025-10-17 21:47:38.803889 | Idx:   40/400    | loss: 0.042 | grad_norm: 0.046 | acc: 99.57% |
| 2025-10-17 21:47:41.952921 | Idx:   80/400    | loss: 0.040 | grad_norm: 0.027 | acc: 99.28% |
| 2025-10-17 21:47:46.607242 | Idx:  120/400    | loss: 0.063 | grad_norm: 0.147 | acc: 99.10% |
| 2025-10-17 21:47:49.704303 | Idx:  160/400    | loss: 0.058 | grad_norm: 0.011 | acc: 99.11% |
| 2025-10-17 21:47:52.801892 | Idx:  200/400    | loss: 0.054 | grad_norm: 0.025 | acc: 99.68% |
| 2025-10-17 21:47:57.314911 | Idx:  240/400    | loss: 0.050 | grad_norm: 0.023 | acc: 99.36% |
| 2025-10-17 21:48:00.415748 | Idx:  280/400    | loss: 0.047 | grad_norm: 0.061 | acc: 99.39% |
| 2025-10-17 21:48:05.052374 | Idx:  320/400    | loss: 0.044 | grad_norm: 0.023 | acc: 99.77% |
| 2025-10-17 21:48:08.156131 | Idx:  360/400    | loss: 0.041 | grad_norm: 0.026 | acc: 99.41% |
| 2025-10-17 21:48:11.257917 | Idx:  400/400    | loss: 0.054 | grad_norm: 0.204 | acc: 98.82% |
Train Loss: 0.0539 Acc: 99.06%
Acc: 93.30%


| val_epoch_acc: 93.30% | epoch: 01 | avg_train_loss: 0.2171 | avg_train_acc: 97.2580 | 


Epoch 3/8
----------
| 2025-10-17 21:48:39.011359 | Idx:   40/400    | loss: 0.047 | grad_norm: 0.019 | acc: 99.22% |
| 2025-10-17 21:48:42.123920 | Idx:   80/400    | loss: 0.037 | grad_norm: 0.010 | acc: 99.78% |
| 2025-10-17 21:48:46.661045 | Idx:  120/400    | loss: 0.032 | grad_norm: 0.007 | acc: 99.86% |
| 2025-10-17 21:48:49.889906 | Idx:  160/400    | loss: 0.029 | grad_norm: 0.073 | acc: 99.65% |
| 2025-10-17 21:48:53.091038 | Idx:  200/400    | loss: 0.027 | grad_norm: 0.031 | acc: 99.87% |
| 2025-10-17 21:48:57.697472 | Idx:  240/400    | loss: 0.025 | grad_norm: 0.034 | acc: 99.61% |
| 2025-10-17 21:49:00.794198 | Idx:  280/400    | loss: 0.038 | grad_norm: 0.187 | acc: 98.86% |
| 2025-10-17 21:49:05.328018 | Idx:  320/400    | loss: 0.037 | grad_norm: 0.014 | acc: 99.58% |
| 2025-10-17 21:49:08.455359 | Idx:  360/400    | loss: 0.035 | grad_norm: 0.006 | acc: 99.82% |
| 2025-10-17 21:49:11.543913 | Idx:  400/400    | loss: 0.033 | grad_norm: 0.009 | acc: 99.70% |
Train Loss: 0.0330 Acc: 99.49%
Acc: 99.81%


| val_epoch_acc: 99.81% | epoch: 02 | avg_train_loss: 0.0142 | avg_train_acc: 99.8246 | 


Epoch 4/8
----------
| 2025-10-17 21:49:38.201051 | Idx:   40/400    | loss: 0.014 | grad_norm: 0.027 | acc: 99.81% |
| 2025-10-17 21:49:41.605433 | Idx:   80/400    | loss: 0.013 | grad_norm: 0.084 | acc: 99.86% |
| 2025-10-17 21:49:46.753947 | Idx:  120/400    | loss: 0.012 | grad_norm: 0.028 | acc: 99.87% |
| 2025-10-17 21:49:49.981307 | Idx:  160/400    | loss: 0.012 | grad_norm: 0.025 | acc: 99.96% |
| 2025-10-17 21:49:53.134789 | Idx:  200/400    | loss: 0.011 | grad_norm: 0.062 | acc: 99.83% |
| 2025-10-17 21:49:57.774299 | Idx:  240/400    | loss: 0.010 | grad_norm: 0.081 | acc: 99.95% |
| 2025-10-17 21:50:00.892036 | Idx:  280/400    | loss: 0.010 | grad_norm: 0.035 | acc: 99.92% |
| 2025-10-17 21:50:05.474170 | Idx:  320/400    | loss: 0.009 | grad_norm: 0.006 | acc: 99.98% |
| 2025-10-17 21:50:08.609842 | Idx:  360/400    | loss: 0.009 | grad_norm: 0.010 | acc: 99.99% |
| 2025-10-17 21:50:11.737707 | Idx:  400/400    | loss: 0.009 | grad_norm: 0.010 | acc: 99.97% |
Train Loss: 0.0085 Acc: 99.92%
Acc: 99.98%


| val_epoch_acc: 99.98% | epoch: 03 | avg_train_loss: 0.0054 | avg_train_acc: 99.9644 | 


Epoch 5/8
----------
| 2025-10-17 21:50:38.522216 | Idx:   40/400    | loss: 0.014 | grad_norm: 0.011 | acc: 99.99% |
| 2025-10-17 21:50:41.721424 | Idx:   80/400    | loss: 0.010 | grad_norm: 0.024 | acc: 99.99% |
| 2025-10-17 21:50:46.720754 | Idx:  120/400    | loss: 0.008 | grad_norm: 0.013 | acc: 99.97% |
| 2025-10-17 21:50:49.855802 | Idx:  160/400    | loss: 0.007 | grad_norm: 0.006 | acc: 99.98% |
| 2025-10-17 21:50:53.005157 | Idx:  200/400    | loss: 0.006 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:50:57.690426 | Idx:  240/400    | loss: 0.006 | grad_norm: 0.042 | acc: 99.96% |
| 2025-10-17 21:51:00.822161 | Idx:  280/400    | loss: 0.006 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 21:51:05.422455 | Idx:  320/400    | loss: 0.006 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:51:08.558589 | Idx:  360/400    | loss: 0.005 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:51:11.649685 | Idx:  400/400    | loss: 0.005 | grad_norm: 0.011 | acc: 99.98% |
Train Loss: 0.0051 Acc: 99.96%
Acc: 99.99%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_214623-xw52y479[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_214623-xw52y479\logs[0m
rule: B2/S013V \t, network: multiscale_0 \n\n
./predictor_life_simple/hyperparams/multiscale_0.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScale'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_0.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_21-51-35_multiscale_0__200-200-B2_S013V


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MultiScale                               [1, 2, 200, 200]          --
©À©¤Sequential: 1-1                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-1                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-2                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-3                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-2                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-4                       [1, 2, 200, 200]          102
©¦    ©¸©¤BatchNorm2d: 2-5                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-6                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-3                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-7                       [1, 2, 200, 200]          38
©¦    ©¸©¤BatchNorm2d: 2-8                  [1, 2, 200, 200]          4
©¦    ©¸©¤LeakyReLU: 2-9                    [1, 2, 200, 200]          --
©À©¤Sequential: 1-4                        [1, 2, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-10                      [1, 4, 200, 200]          604
©¦    ©¸©¤BatchNorm2d: 2-11                 [1, 4, 200, 200]          8
©¦    ©¸©¤ReLU: 2-12                        [1, 4, 200, 200]          --
©¦    ©¸©¤Conv2d: 2-13                      [1, 2, 200, 200]          202
==========================================================================================
Total params: 1,004
Trainable params: 1,004
Non-trainable params: 0
Total mult-adds (M): 39.36
==========================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 7.04
Params size (MB): 0.00
Estimated Total Size (MB): 7.36
==========================================================================================
Epoch 1/8
----------
| 2025-10-17 21:51:52.560101 | Idx:   40/400    | loss: 0.983 | grad_norm: 0.270 | acc: 77.09% |
| 2025-10-17 21:51:55.748732 | Idx:   80/400    | loss: 0.803 | grad_norm: 0.181 | acc: 88.02% |
| 2025-10-17 21:52:00.371376 | Idx:  120/400    | loss: 0.669 | grad_norm: 0.249 | acc: 85.41% |
| 2025-10-17 21:52:03.558556 | Idx:  160/400    | loss: 0.592 | grad_norm: 0.202 | acc: 93.21% |
| 2025-10-17 21:52:06.698797 | Idx:  200/400    | loss: 0.521 | grad_norm: 0.250 | acc: 97.69% |
| 2025-10-17 21:52:11.676734 | Idx:  240/400    | loss: 0.455 | grad_norm: 0.645 | acc: 98.16% |
| 2025-10-17 21:52:14.834346 | Idx:  280/400    | loss: 0.408 | grad_norm: 0.123 | acc: 99.85% |
| 2025-10-17 21:52:19.383650 | Idx:  320/400    | loss: 0.359 | grad_norm: 0.067 | acc: 99.97% |
| 2025-10-17 21:52:22.495706 | Idx:  360/400    | loss: 0.321 | grad_norm: 0.103 | acc: 99.97% |
| 2025-10-17 21:52:25.597984 | Idx:  400/400    | loss: 0.290 | grad_norm: 0.041 | acc: 100.00% |
Train Loss: 0.2897 Acc: 92.67%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 00 | avg_train_loss: 0.0060 | avg_train_acc: 99.9975 | 


Epoch 2/8
----------
| 2025-10-17 21:52:52.310024 | Idx:   40/400    | loss: 0.079 | grad_norm: 1.016 | acc: 97.63% |
| 2025-10-17 21:52:55.526350 | Idx:   80/400    | loss: 0.047 | grad_norm: 0.085 | acc: 99.92% |
| 2025-10-17 21:53:00.192710 | Idx:  120/400    | loss: 0.033 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 21:53:03.318284 | Idx:  160/400    | loss: 0.026 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-17 21:53:06.457971 | Idx:  200/400    | loss: 0.021 | grad_norm: 0.046 | acc: 99.99% |
| 2025-10-17 21:53:11.006622 | Idx:  240/400    | loss: 0.018 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 21:53:14.122852 | Idx:  280/400    | loss: 0.016 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-17 21:53:18.804975 | Idx:  320/400    | loss: 0.014 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:53:21.916077 | Idx:  360/400    | loss: 0.013 | grad_norm: 0.013 | acc: 100.00% |
| 2025-10-17 21:53:25.050970 | Idx:  400/400    | loss: 0.012 | grad_norm: 0.006 | acc: 100.00% |
Train Loss: 0.0120 Acc: 99.86%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0025 | avg_train_acc: 99.9995 | 


Epoch 3/8
----------
| 2025-10-17 21:53:51.467223 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:53:54.552118 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:53:59.113873 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.017 | acc: 100.00% |
| 2025-10-17 21:54:02.226606 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 21:54:05.348833 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:54:09.852126 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:54:12.949644 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:54:17.535870 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:54:20.642628 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:54:23.743702 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0020 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0017 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 21:54:50.780429 | Idx:   40/400    | loss: 0.144 | grad_norm: 2.431 | acc: 95.70% |
| 2025-10-17 21:54:53.910854 | Idx:   80/400    | loss: 0.092 | grad_norm: 0.050 | acc: 99.98% |
| 2025-10-17 21:54:58.625032 | Idx:  120/400    | loss: 0.064 | grad_norm: 0.098 | acc: 100.00% |
| 2025-10-17 21:55:01.768552 | Idx:  160/400    | loss: 0.049 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:55:04.880171 | Idx:  200/400    | loss: 0.040 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-17 21:55:09.503310 | Idx:  240/400    | loss: 0.033 | grad_norm: 0.024 | acc: 100.00% |
| 2025-10-17 21:55:12.634442 | Idx:  280/400    | loss: 0.029 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:55:17.410836 | Idx:  320/400    | loss: 0.026 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:55:20.541843 | Idx:  360/400    | loss: 0.023 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:55:23.647246 | Idx:  400/400    | loss: 0.021 | grad_norm: 0.005 | acc: 100.00% |
Train Loss: 0.0212 Acc: 99.73%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0040 | avg_train_acc: 99.9826 | 


Epoch 5/8
----------
| 2025-10-17 21:55:51.200034 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:55:54.305821 | Idx:   80/400    | loss: 0.027 | grad_norm: 0.447 | acc: 99.39% |
| 2025-10-17 21:55:59.156167 | Idx:  120/400    | loss: 0.071 | grad_norm: 0.034 | acc: 99.99% |
| 2025-10-17 21:56:02.262925 | Idx:  160/400    | loss: 0.055 | grad_norm: 0.010 | acc: 100.00% |
| 2025-10-17 21:56:05.385379 | Idx:  200/400    | loss: 0.045 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 21:56:09.984566 | Idx:  240/400    | loss: 0.038 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 21:56:13.102411 | Idx:  280/400    | loss: 0.033 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:56:17.656471 | Idx:  320/400    | loss: 0.029 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 21:56:20.781190 | Idx:  360/400    | loss: 0.026 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 21:56:23.910269 | Idx:  400/400    | loss: 0.024 | grad_norm: 0.003 | acc: 100.00% |
Train Loss: 0.0240 Acc: 99.71%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_215135-z6sg8xlq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_215135-z6sg8xlq\logs[0m
rule: B2/S \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B2/S', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S/
Saving Base Directory: 2025-10-17_21-56-46_multiscale_0__200-200-B2_S


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
©À©¤Sequential: 1-1                                       --                        --
©¦    ©¸©¤R2Conv: 2-1                                      --                        26
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-1                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-2                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-2                            --                        4
©¦    ©¸©¤ReLU: 2-3                                        --                        --
©À©¤Sequential: 1-2                                       --                        --
©¦    ©¸©¤R2Conv: 2-4                                      --                        46
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-3                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-5                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-4                            --                        4
©¦    ©¸©¤ReLU: 2-6                                        --                        --
©À©¤Sequential: 1-3                                       --                        --
©¦    ©¸©¤R2Conv: 2-7                                      --                        34
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-5                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-8                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-6                            --                        4
©¦    ©¸©¤ReLU: 2-9                                        --                        --
©À©¤Sequential: 1-4                                       --                        --
©¦    ©¸©¤R2Conv: 2-10                                     --                        1,060
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-7                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-11                             --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-8                            --                        8
©¦    ©¸©¤ReLU: 2-12                                       --                        --
©¦    ©¸©¤R2Conv: 2-13                                     --                        90
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-17 21:57:06.902370 | Idx:   40/400    | loss: 1.003 | grad_norm: 0.403 | acc: 92.55% |
| 2025-10-17 21:57:12.958095 | Idx:   80/400    | loss: 0.584 | grad_norm: 0.921 | acc: 98.13% |
| 2025-10-17 21:57:20.511396 | Idx:  120/400    | loss: 0.402 | grad_norm: 0.724 | acc: 97.97% |
| 2025-10-17 21:57:26.562888 | Idx:  160/400    | loss: 0.308 | grad_norm: 0.277 | acc: 99.79% |
| 2025-10-17 21:57:32.573972 | Idx:  200/400    | loss: 0.249 | grad_norm: 0.081 | acc: 99.96% |
| 2025-10-17 21:57:40.400771 | Idx:  240/400    | loss: 0.209 | grad_norm: 0.058 | acc: 99.97% |
| 2025-10-17 21:57:46.491357 | Idx:  280/400    | loss: 0.180 | grad_norm: 0.035 | acc: 99.98% |
| 2025-10-17 21:57:54.033930 | Idx:  320/400    | loss: 0.159 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-17 21:58:00.058861 | Idx:  360/400    | loss: 0.142 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 21:58:06.067091 | Idx:  400/400    | loss: 0.128 | grad_norm: 0.006 | acc: 100.00% |
Train Loss: 0.1277 Acc: 97.20%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 00 | avg_train_loss: 0.0037 | avg_train_acc: 99.9992 | 


Epoch 2/8
----------
| 2025-10-17 21:58:37.603640 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 21:58:43.517003 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.011 | acc: 100.00% |
| 2025-10-17 21:58:51.022608 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.034 | acc: 100.00% |
| 2025-10-17 21:58:56.985306 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 21:59:02.910521 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 21:59:10.619772 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 21:59:16.640545 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.008 | acc: 100.00% |
| 2025-10-17 21:59:24.147191 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.012 | acc: 100.00% |
| 2025-10-17 21:59:30.154975 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 21:59:36.116190 | Idx:  400/400    | loss: 0.023 | grad_norm: 0.327 | acc: 99.33% |
Train Loss: 0.0227 Acc: 99.75%
Acc: 57.76%


| val_epoch_acc: 57.76% | epoch: 01 | avg_train_loss: 0.2658 | avg_train_acc: 96.6647 | 


Epoch 3/8
----------
| 2025-10-17 22:00:07.928165 | Idx:   40/400    | loss: 0.014 | grad_norm: 0.114 | acc: 99.94% |
| 2025-10-17 22:00:14.057490 | Idx:   80/400    | loss: 0.012 | grad_norm: 0.069 | acc: 99.99% |
| 2025-10-17 22:00:21.620378 | Idx:  120/400    | loss: 0.010 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:00:27.618636 | Idx:  160/400    | loss: 0.009 | grad_norm: 0.083 | acc: 99.97% |
| 2025-10-17 22:00:33.636627 | Idx:  200/400    | loss: 0.011 | grad_norm: 0.014 | acc: 100.00% |
| 2025-10-17 22:00:41.363933 | Idx:  240/400    | loss: 0.010 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:00:47.407252 | Idx:  280/400    | loss: 0.009 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:00:55.259868 | Idx:  320/400    | loss: 0.008 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:01:01.209356 | Idx:  360/400    | loss: 0.008 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 22:01:07.219699 | Idx:  400/400    | loss: 0.007 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0075 Acc: 99.96%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0039 | avg_train_acc: 99.9999 | 


Epoch 4/8
----------
| 2025-10-17 22:01:39.622865 | Idx:   40/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 22:01:45.672754 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:01:53.340176 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:01:59.354867 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 22:02:05.518349 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:02:12.888609 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:02:18.983876 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:02:26.802318 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 22:02:32.792860 | Idx:  360/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 22:02:39.217052 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0035 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0030 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 22:03:12.638211 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:03:18.669488 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.026 | acc: 100.00% |
| 2025-10-17 22:03:26.100702 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.029 | acc: 100.00% |
| 2025-10-17 22:03:32.121103 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:03:38.153086 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:03:45.780238 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:03:51.765538 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 22:03:59.636254 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:04:05.660422 | Idx:  360/400    | loss: 0.005 | grad_norm: 0.153 | acc: 99.79% |
| 2025-10-17 22:04:11.664126 | Idx:  400/400    | loss: 0.005 | grad_norm: 0.010 | acc: 100.00% |
Train Loss: 0.0052 Acc: 99.96%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_215646-icq6tcxc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_215646-icq6tcxc\logs[0m
rule: B345/S5 \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B345/S5', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B345_S5/
Saving Base Directory: 2025-10-17_22-04-36_multiscale_0__200-200-B345_S5


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
©À©¤Sequential: 1-1                                       --                        --
©¦    ©¸©¤R2Conv: 2-1                                      --                        26
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-1                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-2                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-2                            --                        4
©¦    ©¸©¤ReLU: 2-3                                        --                        --
©À©¤Sequential: 1-2                                       --                        --
©¦    ©¸©¤R2Conv: 2-4                                      --                        46
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-3                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-5                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-4                            --                        4
©¦    ©¸©¤ReLU: 2-6                                        --                        --
©À©¤Sequential: 1-3                                       --                        --
©¦    ©¸©¤R2Conv: 2-7                                      --                        34
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-5                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-8                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-6                            --                        4
©¦    ©¸©¤ReLU: 2-9                                        --                        --
©À©¤Sequential: 1-4                                       --                        --
©¦    ©¸©¤R2Conv: 2-10                                     --                        1,060
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-7                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-11                             --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-8                            --                        8
©¦    ©¸©¤ReLU: 2-12                                       --                        --
©¦    ©¸©¤R2Conv: 2-13                                     --                        90
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-17 22:04:57.097187 | Idx:   40/400    | loss: 1.309 | grad_norm: 0.514 | acc: 94.81% |
| 2025-10-17 22:05:03.073667 | Idx:   80/400    | loss: 0.750 | grad_norm: 0.219 | acc: 98.14% |
| 2025-10-17 22:05:10.610134 | Idx:  120/400    | loss: 0.540 | grad_norm: 0.477 | acc: 98.34% |
| 2025-10-17 22:05:16.622969 | Idx:  160/400    | loss: 0.432 | grad_norm: 0.121 | acc: 98.61% |
| 2025-10-17 22:05:22.631860 | Idx:  200/400    | loss: 0.359 | grad_norm: 0.486 | acc: 98.59% |
| 2025-10-17 22:05:30.356457 | Idx:  240/400    | loss: 0.309 | grad_norm: 1.529 | acc: 95.96% |
| 2025-10-17 22:05:36.285009 | Idx:  280/400    | loss: 0.276 | grad_norm: 0.042 | acc: 99.60% |
| 2025-10-17 22:05:43.693521 | Idx:  320/400    | loss: 0.247 | grad_norm: 0.262 | acc: 99.50% |
| 2025-10-17 22:05:49.783609 | Idx:  360/400    | loss: 0.223 | grad_norm: 0.054 | acc: 99.85% |
| 2025-10-17 22:05:55.755637 | Idx:  400/400    | loss: 0.204 | grad_norm: 0.143 | acc: 99.71% |
Train Loss: 0.2040 Acc: 96.12%
Acc: 99.48%


| val_epoch_acc: 99.48% | epoch: 00 | avg_train_loss: 0.0279 | avg_train_acc: 99.5845 | 


Epoch 2/8
----------
| 2025-10-17 22:06:27.807383 | Idx:   40/400    | loss: 0.033 | grad_norm: 0.101 | acc: 99.78% |
| 2025-10-17 22:06:33.873817 | Idx:   80/400    | loss: 0.030 | grad_norm: 0.245 | acc: 99.81% |
| 2025-10-17 22:06:41.327753 | Idx:  120/400    | loss: 0.025 | grad_norm: 0.068 | acc: 99.89% |
| 2025-10-17 22:06:47.356869 | Idx:  160/400    | loss: 0.024 | grad_norm: 0.190 | acc: 99.80% |
| 2025-10-17 22:06:53.336175 | Idx:  200/400    | loss: 0.025 | grad_norm: 0.072 | acc: 99.91% |
| 2025-10-17 22:07:01.132573 | Idx:  240/400    | loss: 0.026 | grad_norm: 0.056 | acc: 99.91% |
| 2025-10-17 22:07:07.206789 | Idx:  280/400    | loss: 0.024 | grad_norm: 0.019 | acc: 99.97% |
| 2025-10-17 22:07:14.857228 | Idx:  320/400    | loss: 0.025 | grad_norm: 0.055 | acc: 99.92% |
| 2025-10-17 22:07:20.871527 | Idx:  360/400    | loss: 0.025 | grad_norm: 0.032 | acc: 99.92% |
| 2025-10-17 22:07:26.932489 | Idx:  400/400    | loss: 0.024 | grad_norm: 0.077 | acc: 99.72% |
Train Loss: 0.0238 Acc: 99.66%
Acc: 99.88%


| val_epoch_acc: 99.88% | epoch: 01 | avg_train_loss: 0.0193 | avg_train_acc: 99.7209 | 


Epoch 3/8
----------
| 2025-10-17 22:07:58.628824 | Idx:   40/400    | loss: 0.011 | grad_norm: 0.076 | acc: 99.91% |
| 2025-10-17 22:08:04.613021 | Idx:   80/400    | loss: 0.010 | grad_norm: 0.065 | acc: 99.94% |
| 2025-10-17 22:08:12.029155 | Idx:  120/400    | loss: 0.010 | grad_norm: 0.005 | acc: 99.97% |
| 2025-10-17 22:08:18.098485 | Idx:  160/400    | loss: 0.010 | grad_norm: 0.087 | acc: 99.90% |
| 2025-10-17 22:08:24.144629 | Idx:  200/400    | loss: 0.012 | grad_norm: 0.015 | acc: 99.94% |
| 2025-10-17 22:08:31.767798 | Idx:  240/400    | loss: 0.011 | grad_norm: 0.010 | acc: 99.97% |
| 2025-10-17 22:08:38.092854 | Idx:  280/400    | loss: 0.011 | grad_norm: 0.032 | acc: 99.97% |
| 2025-10-17 22:08:46.003387 | Idx:  320/400    | loss: 0.011 | grad_norm: 0.041 | acc: 99.95% |
| 2025-10-17 22:08:52.004509 | Idx:  360/400    | loss: 0.010 | grad_norm: 0.014 | acc: 99.99% |
| 2025-10-17 22:08:58.077968 | Idx:  400/400    | loss: 0.010 | grad_norm: 0.014 | acc: 99.96% |
Train Loss: 0.0101 Acc: 99.91%
Acc: 99.97%


| val_epoch_acc: 99.97% | epoch: 02 | avg_train_loss: 0.0087 | avg_train_acc: 99.9351 | 


Epoch 4/8
----------
| 2025-10-17 22:09:30.530796 | Idx:   40/400    | loss: 0.007 | grad_norm: 0.018 | acc: 99.97% |
| 2025-10-17 22:09:36.496516 | Idx:   80/400    | loss: 0.007 | grad_norm: 0.008 | acc: 99.99% |
| 2025-10-17 22:09:43.982944 | Idx:  120/400    | loss: 0.007 | grad_norm: 0.003 | acc: 99.99% |
| 2025-10-17 22:09:49.977212 | Idx:  160/400    | loss: 0.007 | grad_norm: 0.005 | acc: 99.99% |
| 2025-10-17 22:09:55.986492 | Idx:  200/400    | loss: 0.006 | grad_norm: 0.018 | acc: 99.98% |
| 2025-10-17 22:10:03.468484 | Idx:  240/400    | loss: 0.012 | grad_norm: 0.135 | acc: 99.78% |
| 2025-10-17 22:10:09.463354 | Idx:  280/400    | loss: 0.012 | grad_norm: 0.056 | acc: 99.95% |
| 2025-10-17 22:10:17.093940 | Idx:  320/400    | loss: 0.022 | grad_norm: 0.537 | acc: 98.84% |
| 2025-10-17 22:10:23.078678 | Idx:  360/400    | loss: 0.022 | grad_norm: 0.028 | acc: 99.93% |
| 2025-10-17 22:10:29.098228 | Idx:  400/400    | loss: 0.021 | grad_norm: 0.039 | acc: 99.96% |
Train Loss: 0.0207 Acc: 99.75%
Acc: 99.96%


| val_epoch_acc: 99.96% | epoch: 03 | avg_train_loss: 0.0099 | avg_train_acc: 99.9251 | 


Epoch 5/8
----------
| 2025-10-17 22:11:01.102485 | Idx:   40/400    | loss: 0.008 | grad_norm: 0.009 | acc: 99.99% |
| 2025-10-17 22:11:07.159156 | Idx:   80/400    | loss: 0.008 | grad_norm: 0.056 | acc: 99.89% |
| 2025-10-17 22:11:14.429523 | Idx:  120/400    | loss: 0.007 | grad_norm: 0.005 | acc: 99.99% |
| 2025-10-17 22:11:20.514267 | Idx:  160/400    | loss: 0.007 | grad_norm: 0.005 | acc: 99.99% |
| 2025-10-17 22:11:26.596666 | Idx:  200/400    | loss: 0.007 | grad_norm: 0.015 | acc: 99.97% |
| 2025-10-17 22:11:34.005792 | Idx:  240/400    | loss: 0.007 | grad_norm: 0.064 | acc: 99.97% |
| 2025-10-17 22:11:40.030619 | Idx:  280/400    | loss: 0.008 | grad_norm: 2.223 | acc: 98.00% |
| 2025-10-17 22:11:47.633331 | Idx:  320/400    | loss: 0.008 | grad_norm: 0.033 | acc: 99.95% |
| 2025-10-17 22:11:53.632679 | Idx:  360/400    | loss: 0.008 | grad_norm: 2.655 | acc: 98.02% |
| 2025-10-17 22:11:59.692305 | Idx:  400/400    | loss: 0.013 | grad_norm: 0.041 | acc: 99.90% |
Train Loss: 0.0129 Acc: 99.87%
Acc: 99.92%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_220436-xq94yurm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_220436-xq94yurm\logs[0m
rule: B13/S012V \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B13/S012V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B13_S012V/
Saving Base Directory: 2025-10-17_22-12-25_multiscale_0__200-200-B13_S012V


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
©À©¤Sequential: 1-1                                       --                        --
©¦    ©¸©¤R2Conv: 2-1                                      --                        26
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-1                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-2                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-2                            --                        4
©¦    ©¸©¤ReLU: 2-3                                        --                        --
©À©¤Sequential: 1-2                                       --                        --
©¦    ©¸©¤R2Conv: 2-4                                      --                        46
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-3                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-5                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-4                            --                        4
©¦    ©¸©¤ReLU: 2-6                                        --                        --
©À©¤Sequential: 1-3                                       --                        --
©¦    ©¸©¤R2Conv: 2-7                                      --                        34
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-5                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-8                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-6                            --                        4
©¦    ©¸©¤ReLU: 2-9                                        --                        --
©À©¤Sequential: 1-4                                       --                        --
©¦    ©¸©¤R2Conv: 2-10                                     --                        1,060
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-7                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-11                             --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-8                            --                        8
©¦    ©¸©¤ReLU: 2-12                                       --                        --
©¦    ©¸©¤R2Conv: 2-13                                     --                        90
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-17 22:12:45.533260 | Idx:   40/400    | loss: 0.962 | grad_norm: 0.111 | acc: 94.60% |
| 2025-10-17 22:12:51.500023 | Idx:   80/400    | loss: 0.536 | grad_norm: 0.534 | acc: 98.96% |
| 2025-10-17 22:12:59.021710 | Idx:  120/400    | loss: 0.373 | grad_norm: 0.077 | acc: 99.67% |
| 2025-10-17 22:13:04.999136 | Idx:  160/400    | loss: 0.286 | grad_norm: 0.056 | acc: 99.96% |
| 2025-10-17 22:13:10.980336 | Idx:  200/400    | loss: 0.231 | grad_norm: 0.015 | acc: 99.96% |
| 2025-10-17 22:13:18.663992 | Idx:  240/400    | loss: 0.194 | grad_norm: 0.189 | acc: 99.90% |
| 2025-10-17 22:13:24.648624 | Idx:  280/400    | loss: 0.180 | grad_norm: 0.123 | acc: 99.44% |
| 2025-10-17 22:13:32.146077 | Idx:  320/400    | loss: 0.161 | grad_norm: 0.059 | acc: 99.97% |
| 2025-10-17 22:13:38.200862 | Idx:  360/400    | loss: 0.144 | grad_norm: 0.025 | acc: 100.00% |
| 2025-10-17 22:13:44.226284 | Idx:  400/400    | loss: 0.130 | grad_norm: 0.011 | acc: 99.99% |
Train Loss: 0.1302 Acc: 97.12%
Acc: 99.99%


| val_epoch_acc: 99.99% | epoch: 00 | avg_train_loss: 0.0066 | avg_train_acc: 99.9833 | 


Epoch 2/8
----------
| 2025-10-17 22:14:13.073486 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 22:14:19.152406 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.009 | acc: 100.00% |
| 2025-10-17 22:14:26.666401 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 22:14:32.673416 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 22:14:38.708358 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:14:46.218281 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:14:52.230849 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.020 | acc: 100.00% |
| 2025-10-17 22:14:59.967216 | Idx:  320/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:15:05.992092 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:15:11.973329 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0031 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0023 | avg_train_acc: 99.9996 | 


Epoch 3/8
----------
| 2025-10-17 22:15:42.728709 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:15:48.796605 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.005 | acc: 100.00% |
| 2025-10-17 22:15:56.313382 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:16:02.322432 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.015 | acc: 100.00% |
| 2025-10-17 22:16:08.319505 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:16:15.719897 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:16:21.735271 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:16:29.291221 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:16:35.273049 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:16:41.291992 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.000 | acc: 100.00% |
Train Loss: 0.0019 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0016 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 22:17:11.915236 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:17:18.089022 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:17:25.725337 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:17:31.827198 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:17:37.843621 | Idx:  200/400    | loss: 0.001 | grad_norm: 0.000 | acc: 100.00% |
| 2025-10-17 22:17:45.220283 | Idx:  240/400    | loss: 0.001 | grad_norm: 0.000 | acc: 100.00% |
| 2025-10-17 22:17:51.205758 | Idx:  280/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:17:58.697544 | Idx:  320/400    | loss: 0.001 | grad_norm: 0.000 | acc: 100.00% |
| 2025-10-17 22:18:04.620280 | Idx:  360/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:18:10.565974 | Idx:  400/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0014 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0013 | avg_train_acc: 99.9999 | 


Epoch 5/8
----------
| 2025-10-17 22:18:40.259028 | Idx:   40/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:18:46.243493 | Idx:   80/400    | loss: 0.001 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:18:53.967939 | Idx:  120/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:18:59.888600 | Idx:  160/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:19:05.820878 | Idx:  200/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:19:13.157666 | Idx:  240/400    | loss: 0.001 | grad_norm: 0.000 | acc: 100.00% |
| 2025-10-17 22:19:19.199820 | Idx:  280/400    | loss: 0.001 | grad_norm: 0.027 | acc: 99.99% |
| 2025-10-17 22:19:26.830448 | Idx:  320/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:19:32.886622 | Idx:  360/400    | loss: 0.001 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:19:38.964953 | Idx:  400/400    | loss: 0.001 | grad_norm: 0.000 | acc: 100.00% |
Train Loss: 0.0011 Acc: 100.00%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_221225-gi3gmtfb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_221225-gi3gmtfb\logs[0m
rule: B2/S013V \t, network: multiscale_p4 \n\n
./predictor_life_simple/hyperparams/multiscale_p4.toml

{'info': 'basic multiscale model (parallel input head) with weight decay', 'wandb': {'turn_on': True, 'entity': 'multiscale_0'}, 'dataloader': {'train_batch_size': 8, 'train_num_workers': 4, 'train_shuffle': True, 'test_batch_size': 8, 'test_num_workers': 4, 'test_shuffle': False}, 'optimizer': {'name': 'AdamW', 'args': {'lr': 0.001, 'weight_decay': 1e-07}}, 'lr_scheduler': {'name': 'CyclicLR', 'args': {'base_lr': 0.001, 'max_lr': 0.05, 'step_size_up': 70, 'step_size_down': 20}}, 'model': {'name': 'MultiScaleP4'}, 'training': {'epochs': 8, 'r_ratio_start': 1, 'r_ratio_decay': 1e-05, 'r_ratio_min': 0.01}, 'hyperparameters': './predictor_life_simple/hyperparams/multiscale_p4.toml', 'data_rule': 'B2/S013V', 'data_iters': 200, 'sys_size': 200}
Starting training...

Picking Dataset: ./predictor_life_simple/datasets/200-200-B2_S013V/
Saving Base Directory: 2025-10-17_22-20-03_multiscale_0__200-200-B2_S013V


=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiScaleP4                                            --                        --
©À©¤Sequential: 1-1                                       --                        --
©¦    ©¸©¤R2Conv: 2-1                                      --                        26
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-1                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-2                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-2                            --                        4
©¦    ©¸©¤ReLU: 2-3                                        --                        --
©À©¤Sequential: 1-2                                       --                        --
©¦    ©¸©¤R2Conv: 2-4                                      --                        46
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-3                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-5                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-4                            --                        4
©¦    ©¸©¤ReLU: 2-6                                        --                        --
©À©¤Sequential: 1-3                                       --                        --
©¦    ©¸©¤R2Conv: 2-7                                      --                        34
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-5                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-8                              --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-6                            --                        4
©¦    ©¸©¤ReLU: 2-9                                        --                        --
©À©¤Sequential: 1-4                                       --                        --
©¦    ©¸©¤R2Conv: 2-10                                     --                        1,060
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-7                   --                        --
©¦    ©¸©¤InnerBatchNorm: 2-11                             --                        --
©¦    ©¦    ©¸©¤BatchNorm3d: 3-8                            --                        8
©¦    ©¸©¤ReLU: 2-12                                       --                        --
©¦    ©¸©¤R2Conv: 2-13                                     --                        90
©¦    ©¦    ©¸©¤BlocksBasisExpansion: 3-9                   --                        --
=========================================================================================================
Total params: 1,276
Trainable params: 1,276
Non-trainable params: 0
Total mult-adds (M): 0
=========================================================================================================
Input size (MB): 0.32
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.32
=========================================================================================================
Epoch 1/8
----------
| 2025-10-17 22:20:24.335882 | Idx:   40/400    | loss: 1.154 | grad_norm: 0.287 | acc: 79.03% |
| 2025-10-17 22:20:30.289636 | Idx:   80/400    | loss: 0.844 | grad_norm: 0.535 | acc: 90.66% |
| 2025-10-17 22:20:37.794292 | Idx:  120/400    | loss: 0.655 | grad_norm: 0.245 | acc: 95.75% |
| 2025-10-17 22:20:43.850890 | Idx:  160/400    | loss: 0.536 | grad_norm: 0.651 | acc: 97.34% |
| 2025-10-17 22:20:49.847022 | Idx:  200/400    | loss: 0.448 | grad_norm: 0.146 | acc: 98.83% |
| 2025-10-17 22:20:57.512117 | Idx:  240/400    | loss: 0.383 | grad_norm: 0.522 | acc: 99.53% |
| 2025-10-17 22:21:03.495946 | Idx:  280/400    | loss: 0.334 | grad_norm: 0.020 | acc: 99.89% |
| 2025-10-17 22:21:10.921651 | Idx:  320/400    | loss: 0.294 | grad_norm: 0.119 | acc: 99.90% |
| 2025-10-17 22:21:16.900215 | Idx:  360/400    | loss: 0.262 | grad_norm: 0.021 | acc: 99.99% |
| 2025-10-17 22:21:22.838525 | Idx:  400/400    | loss: 0.236 | grad_norm: 0.018 | acc: 100.00% |
Train Loss: 0.2365 Acc: 94.32%
Acc: 99.99%


| val_epoch_acc: 99.99% | epoch: 00 | avg_train_loss: 0.0057 | avg_train_acc: 99.9944 | 


Epoch 2/8
----------
| 2025-10-17 22:21:52.278266 | Idx:   40/400    | loss: 0.005 | grad_norm: 0.028 | acc: 100.00% |
| 2025-10-17 22:21:58.907715 | Idx:   80/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 22:22:06.853833 | Idx:  120/400    | loss: 0.004 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 22:22:13.730385 | Idx:  160/400    | loss: 0.004 | grad_norm: 0.007 | acc: 100.00% |
| 2025-10-17 22:22:20.125945 | Idx:  200/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:22:27.885773 | Idx:  240/400    | loss: 0.004 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 22:22:34.011874 | Idx:  280/400    | loss: 0.004 | grad_norm: 0.006 | acc: 100.00% |
| 2025-10-17 22:22:42.129536 | Idx:  320/400    | loss: 0.004 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:22:48.239729 | Idx:  360/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:22:54.232207 | Idx:  400/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0034 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 01 | avg_train_loss: 0.0028 | avg_train_acc: 100.0000 | 


Epoch 3/8
----------
| 2025-10-17 22:23:24.879565 | Idx:   40/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 22:23:30.852719 | Idx:   80/400    | loss: 0.003 | grad_norm: 0.004 | acc: 100.00% |
| 2025-10-17 22:23:38.345023 | Idx:  120/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:23:44.311964 | Idx:  160/400    | loss: 0.003 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:23:50.238008 | Idx:  200/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:23:57.611689 | Idx:  240/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:24:03.569631 | Idx:  280/400    | loss: 0.003 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:24:10.945913 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:24:16.907607 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:24:22.862024 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
Train Loss: 0.0024 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 02 | avg_train_loss: 0.0022 | avg_train_acc: 100.0000 | 


Epoch 4/8
----------
| 2025-10-17 22:24:52.579599 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:24:58.647462 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:06.226882 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:12.205120 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:18.218625 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:25.668221 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:31.846198 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:39.501326 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:45.602145 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:25:51.689386 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0020 Acc: 100.00%
Acc: 100.00%


| val_epoch_acc: 100.00% | epoch: 03 | avg_train_loss: 0.0018 | avg_train_acc: 100.0000 | 


Epoch 5/8
----------
| 2025-10-17 22:26:21.567001 | Idx:   40/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:26:27.609764 | Idx:   80/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:26:35.529742 | Idx:  120/400    | loss: 0.002 | grad_norm: 0.002 | acc: 100.00% |
| 2025-10-17 22:26:41.487584 | Idx:  160/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:26:47.466185 | Idx:  200/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:26:55.008908 | Idx:  240/400    | loss: 0.002 | grad_norm: 0.003 | acc: 100.00% |
| 2025-10-17 22:27:00.991722 | Idx:  280/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:27:08.547050 | Idx:  320/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:27:14.531914 | Idx:  360/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
| 2025-10-17 22:27:20.518786 | Idx:  400/400    | loss: 0.002 | grad_norm: 0.001 | acc: 100.00% |
Train Loss: 0.0017 Acc: 100.00%
Acc: 100.00%
Early Stopped.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync D:\Internship\bimsa\wandb\offline-run-20251017_222003-wocjxytb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\offline-run-20251017_222003-wocjxytb\logs[0m
